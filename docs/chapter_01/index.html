<!DOCTYPE html>

<html>

<head>
    <meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<title>Chapter I</title>


<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "Gyre Pagella",
        webFont : "STIX-Web",
        imageFont : null
    },
	  TeX: {
		equationNumbers: {
		  autoNumber: "AMS"
		}
	  },
	  tex2jax: {
		inlineMath: [['$','$']], 
		displayMath: [['$$','$$']],
		processEscapes: true,
	  }
	});
  </script>
<style>
    .eqnos {
        display: inline-block;
        position: relative;
        width: 100%;
    }

    .eqnos br {
        display: none;
    }

    .eqnos-number {
        position: absolute;
        right: 0em;
        top: 50%;
        line-height: 0;
    }
</style>

<script>
    window.MathJax = {
        tex: {
            tags: 'all'
        }
    };
</script>
<script>
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
    });
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<link rel="icon" href="https://mrazomej.github.io/phd//favicon.ico"
    type="image/x-icon">
<link href="https://fonts.googleapis.com/css?family=Nanum+Myeongjo&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Alegreya|Lora&display=swap" rel="stylesheet">

<link rel="stylesheet"
    href="https://mrazomej.github.io/phd//doks-theme/assets/css/style.css">
</head>

<body class="grey" data-spy="scroll" data-target=".js-scrollspy">
    


	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="https://mrazomej.github.io/phd//" class="site-header__logo"><i class="icon icon--chevron-left"></i> Home</a>
					
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


    <div class="hero-subheader" style="background-image: url(https://mrazomej.github.io/phd//doks-theme/assets/images/plants.jpg); background-position: center; object-fit: cover; width:100%;">

        <div class="container" style="background-color: rgba(0,0,0, 0.75); border:0.75px solid white;">
            <div class="row">
                <div class="col-md-7">
                    <div class="align-container" data-mh>
                        <div class="align-inner">
                            
                            <h2 class="hero-subheader__title">Chapter I</h2>
                            
                            
                            <h1 class="hero-subheader__title">Introduction
</h1>
                            
                            
                            <!-- 
									
										<a href="https://mrazomej.github.io/phd/abstract" class="btn btn--dark btn--rounded btn--w-icon btn--w-icon-left">
											<i class="icon icon--arrow-left"></i>	
										</a>
									
									
										<a href="https://mrazomej.github.io/phd/chapter_02" class="btn btn--dark btn--rounded btn--w-icon">
											<i class="icon icon--arrow-right"></i>
										</a>
									
								 -->
                        </div><!-- /.align-inner -->
                    </div><!-- /.align-container -->
                </div><!-- /.col -->
                
                <div class="col-md-5 col-md-offset-0 hidden-xs hidden-sm">
                    <div class="align-container" data-mh>
                        <div class="align-inner">
                            <div class="hero-subheader__author">
                                <p class="hero-subheader__author-title">
                                    Summary
                                    <i class="icon icon--chevron-down"></i>
                                </p><!-- /.hero-subheader__author-title -->
                                <p>To fully appreciate the beauty of science, many times, it is necessary not only to superficially understand concepts behind the natural  phenomena being studied, but one needs to dig deeper into the technical details that lead us to a deeper understanding of the way the universe  works. This is the difference between "knowing about something" and  actually "knowing something."
Not a single piece of work is ever complete, and not a single source of information is ever sufficient to understand everything fully. Let alone this thesis. Nevertheless, with a few concepts at hand, it is possible  to appreciate better the work developed here. In this chapter, I  introduce such concepts. I present the conceptual, physical, and  mathematical tools I consider as a prerequisite to work through this text. From the basics of how to model gene expression in bacteria to the ideas from Information Theory, this is an attempt to get the reader on the same page, hopefully making the experience of reading the rest of the text a much less daunting task. 
 
</p>
                            </div><!-- /.hero-subheader__author -->
                        </div><!-- /.align-inner -->
                    </div><!-- /.align-container -->
                </div><!-- /.col -->
                
            </div><!-- /.row -->
        </div><!-- /.container -->
    </div><!-- /.hero-subheader -->
    
    <nav class="page-nav">
        <div class="container">
            <div class="row">
                <div class="col-xs-12">
                    
                    <a href="https://mrazomej.github.io/phd/abstract"
                        class="page-nav__item page-nav__item--prev">
                        <i class="icon icon--arrow-left"></i>
                        Abstract
                    </a><!-- /.page-nav__item -->
                    
                    
                    <a href="https://mrazomej.github.io/phd/chapter_02"
                        class="page-nav__item page-nav__item--next">
                        Chapter 2
                        <i class="icon icon--arrow-right"></i>
                    </a><!-- /.page-nav__item -->
                    
                </div><!-- /.col -->
            </div><!-- /.row -->
        </div><!-- /.container -->
    </nav><!-- /.page-nav -->
    

    <div class="section">
        <div class="container">
            <div class="row">
                <div class="col-md-7">
                    <div class="content">
                        <h1 id="from-bio-to-bit-how-do-cells-sense-the-world-around-them">From Bio to Bit: How do cells sense the world around them?</h1>
<h2 id="introduction">Introduction</h2>
<p>In his classic 1944 book <em>What is Life?</em>, Schrödinger brought to the attention of the scientific community what he thought were two of the biggest challenges we had ahead of us if we were to understand living systems in the same way we understand the electromagnetic field or the universal law of gravitation <span class="citation" data-cites="Schrodinger1992"> [<a href="#ref-Schrodinger1992" role="doc-biblioref">1</a>]</span>. The idea that living organisms could be “accounted for” by physics and chemistry brought with it a new agenda on what needed to be done to transition from a qualitative and descriptive study of the phenomena of life to a quantitative and predictive science in the spirit of the physical sciences. Since the publication of the book, there has been an enormous amount of progress on our understanding of living systems from a first-principles perspective, nevertheless, 75 years later and Schrödinger questions are still as relevant and as vibrant as ever before <span class="citation" data-cites="Phillips2021"> [<a href="#ref-Phillips2021" role="doc-biblioref">2</a>]</span>.</p>
<p>One of the defining features of living organisms at all scales is their capacity of gathering information from the environment, encode an internal representation of the state of the environment, and generate a response based on this information processing capacity. Researchers in the field of origins-of-life had gone as far as declaring that life emerged when chemical systems underwent a radical phase transition after which they were able to process and use information and free energy <span class="citation" data-cites="Cronin2016"> [<a href="#ref-Cronin2016" role="doc-biblioref">3</a>]</span>. So, although speculative, it is highly probable that the physical theory fulfilling Schrödinger’s vision of accounting for the phenomena of life will be the physics of systems capable of processing information <span class="citation" data-cites="Davies2019"> [<a href="#ref-Davies2019" role="doc-biblioref">4</a>]</span>.</p>
<p>In this context, information does not take the generic concept of possessing practical knowledge about something. In this thesis, we use a precise mathematical definition of information <span class="citation" data-cites="Adami2016"> [<a href="#ref-Adami2016" role="doc-biblioref">5</a>]</span>. This formal definition makes information a metric worth quantifying and predicting in various biological context as theoretical studies suggest that natural selection might act on the ability of an organism to process information <span class="citation" data-cites="Taylor2007"> [<a href="#ref-Taylor2007" role="doc-biblioref">6</a>]</span>. Working out the physical details of how it is that organisms sense the environment; this is, gather information about the state of the environment, encode such information in some shape or form within their physical boundaries, and take action based on this information is at the core of the state-of-the-art research in biophysics <span class="citation" data-cites="Bialek2012"> [<a href="#ref-Bialek2012" role="doc-biblioref">7</a>]</span>.</p>
<p>The present thesis is an effort towards this vision of understanding biological systems as information processing machines. Our object of study will be gene regulation in bacteria. This particular system has been the subject of study for microbiologists and molecular biologists for decades, and we have come to learn a lot about the microscopic mechanistic details of how bacteria turn on and off their transcriptional machinery <span class="citation" data-cites="Browning2004"> [<a href="#ref-Browning2004" role="doc-biblioref">8</a>]</span>. In particular, we will focus on what we think of as the “hydrogen atom” of gene regulation–the so-called simple-repression motif (more on that in the next section). In physics, calling something the hydrogen atom of <span class="math inline">\(X\)</span> means that for the area of study <span class="math inline">\(X\)</span>, this “something” represents a system simple enough to be amenable to analytical models that standard mathematical methods can solve but rich enough to capture the general features of the phenomena. This simple genetic circuit will allow us to write tractable mathematical models to guide our experimental efforts with the ultimate goal of testing our understanding of such systems when predicting how much information a bacterium can gather from the environment using this genetic module.</p>
<p>Professional biophysicists might wish to skip the rest of this chapter as we will lay the foundations needed for the rest of our enterprise. We will introduce the basics of gene expression modeling and the mathematical concept of information and work through every single physical and mathematical prerequisite needed for the rest of the thesis. The following chapters are structured as follows: Chapter 2 builds on a decade of understanding this hydrogen atom of gene regulation and expands our models’ predictive ability by including the effect of environmental effectors. This means that we will consider how gene regulation is affected by the presence of an extracellular inducer molecule. Chapter 3 will expand even further our predictive capacities by building a model capable of making predictions about the cell-to-cell variability inherent to all signaling systems working at the molecular scale. Chapter 4 serves as a Supporting Information section for Chapter 2, detailing every calculation and every inference. Likewise, Chapter 5 expands on Chapter 3, explaining every technical detail.</p>
<h2 id="gene-regulation-as-a-physics-101-problem">Gene regulation as a Physics 101 problem</h2>
<p>As organisms navigate the challenges presented by the environment, they must constantly fight against the will of the second law of thermodynamics to bring them back to an equilibrium state. To face such challenges, cells are equipped with a toolkit of genes written in the language of A, T, C, and G of the genome. We can think of a typical bacteria genome with <span class="math inline">\(\approx 5\times 10^3\)</span> genes as the blueprint to produce a repertoire of tools that allow cells to thrive under myriad circumstances that they face throughout their lives. Given the vast number of challenges that organisms face, there is constant pressure on every living system to use the right tools for the right circumstances. Thus all organisms are faced with the task of orchestrating the expression of the correct subset of genes at their disposal when trying to survive. From cells in the fly embryo expressing different genes that will define their identity on the animal’s final body plan to a simple bacteria expressing the correct enzymes to process the available nutrients in the environment.</p>
<p>Our understanding of how organisms regulate their genes’ expression is still not as thorough as one might expect, given the effort that has gone into this question. Take, for example, <em>E. coli</em>–arguably the most well-characterized model organism–for which we know the regulatory scheme of less than 1/3 of its genes <span class="citation" data-cites="Ireland2020"> [<a href="#ref-Ireland2020" role="doc-biblioref">9</a>]</span>. For more complex organisms such as <em>Drosophila</em>, <em>C. elegans</em>, or even humans, we are even more hopeless on getting a holistic view of the regulatory landscape. Nevertheless, we would not be doing justice to the field’s significant advances if we were to pretend we are utterly ignorant about how gene regulation takes place in bacteria. There is a rich mechanistic understanding of how the transcriptional machinery takes the information contained in DNA and transcribes it into RNA <span class="citation" data-cites="Browning2004"> [<a href="#ref-Browning2004" role="doc-biblioref">8</a>]</span>. The relative simplicity of the process has inspired generations of biophysicists to try to write down minimal models that can describe and predict features of the process of gene regulation <span class="citation" data-cites="Ackers1982 Bintu2005 Kuhlman2007"> [<a href="#ref-Ackers1982" role="doc-biblioref">10</a>–<a href="#ref-Kuhlman2007" role="doc-biblioref">12</a>]</span>.</p>
<p>These modeling efforts come into two main flavors: equilibrium statistical mechanical models and kinetic models. In the following sections, we will introduce the necessary background for both approaches relevant to the rest of the thesis.</p>
<h3 id="minimal-model-of-gene-expression">Minimal model of gene expression</h3>
<p>Let us begin our introduction to gene expression modeling with the simplest example. As shown in Fig. 1(A), we imagine a gene promoter (the region of the gene where transcriptional regulation takes place) produces mRNA at a constant rate <span class="math inline">\(r_m\)</span>. Each mRNA can stochastically decay with a rate <span class="math inline">\(\gamma_m\)</span>. Our interest is to understand how the mRNA count <span class="math inline">\(m\)</span> changes over time, given these two competing processes. For that, let us write the mRNA count at time <span class="math inline">\(m(t + \Delta t)\)</span>, where <span class="math inline">\(t\)</span> is the time–which we are thinking of as being “right now”–and <span class="math inline">\(\Delta t\)</span> is a little time step into the future. The mRNA count can then be predicted by computing <span class="math display">\[
m(t + \Delta t) = m(t) + r_m \Delta t - (\gamma_m \Delta t) m(t),
\label{eq:m_t_Delta_t}
\]</span> where we can think of <span class="math inline">\(r_m \Delta t\)</span> as the probability of observing a single mRNA being produced in the time interval <span class="math inline">\([t, t + \Delta t]\)</span> (<span class="math inline">\(\Delta t\)</span> is so small that we neglect the possibility of seeing multiple mRNAs being produced), and <span class="math inline">\(\gamma_m \Delta t\)</span> the probability of seeing a single mRNA being degraded. But since each mRNA has the same probability of being degraded, the total number of mRNAs that we would see decay in this time window would be the probability per mRNA times the total number of mRNAs. This is in contrast with the production of mRNA, which does not depend on the current number of mRNAs. If we send the term <span class="math inline">\(m(t)\)</span> to the left-hand side of the equation and divide both sides by <span class="math inline">\(\Delta t\)</span>, we obtain <span class="math display">\[
\frac{m(t + \Delta t) - m(t)}{\Delta t} = r_m - \gamma_m m(t).
\]</span> Upon taking the limit when <span class="math inline">\(\Delta t \rightarrow 0\)</span>, we see that the left-hand side is the definition of the derivative of the mRNA count with respect to time. We then obtain an ordinary differential equation of the form <span class="math display">\[
\frac{dm}{dt} = r_m - \gamma_m m(t).
\label{eq:dm_dt}
\]</span> Before even attempting to solve <span class="math inline">\(\ref{eq:dm_dt}\)</span>, we can perform a qualitative analysis of the dynamics <span class="citation" data-cites="Strogatz2018"> [<a href="#ref-Strogatz2018" role="doc-biblioref">13</a>]</span>. It is handy to plot the contribution of each of the components (production and degradation) to the derivative <span class="math inline">\(dm/dt\)</span> as a function of <span class="math inline">\(m\)</span>. This is shown in Fig. 1(B), where the blue horizontal line <span class="math inline">\(r_m\)</span> shows the production rate–which does not depend on <span class="math inline">\(m\)</span>, and the red line shows the degradation term <span class="math inline">\(m\gamma_m\)</span> which scales linearly with <span class="math inline">\(m\)</span>. Notice that we do not include the negative sign for the degradation term, i.e., we are not plotting <span class="math inline">\(-m \gamma_m\)</span>. The point <span class="math inline">\(m_{ss}\)</span> where both lines intersect represents the point where the production matches the degradation. For all values less than <span class="math inline">\(m_{ss}\)</span> the production term is larger than the degradation, which means that for any value <span class="math inline">\(m &lt; m_{ss}\)</span> the derivative is positive (<span class="math inline">\(dm/dt &gt; 0\)</span>), so over time the system will accumulate more mRNA. The opposite is true for all values after <span class="math inline">\(m_{ss}\)</span> where the degradation term is larger than the production term, implying that <span class="math inline">\(dm/dt &lt; 0\)</span>. This means that for <span class="math inline">\(m &gt; m_{ss}\)</span>, the system will tend to lose mRNA. These opposite trends point to the idea that <span class="math inline">\(m_{ss}\)</span> must be called a stable fixed point of the dynamical system. This can schematically be seen at the bottom of Fig. 1(B). The arrowheads’ size indicates the system’s trend to move either left or right in <span class="math inline">\(m\)</span>. Since all arrows point at the special value, <span class="math inline">\(m_{ss}\)</span>, we can say that any small perturbation of the system will be dissipated as the system relaxes back to <span class="math inline">\(m_{ss}\)</span>.</p>
<p>This qualitative statement can be confirmed by solving Eq. <span class="math inline">\(\ref{eq:dm_dt}\)</span>. If we define the initial condition <span class="math inline">\(m(t=0) = m_o\)</span> by separation of variables we will obtain a solution of the form <span class="math display">\[
m(t) = m_o e^{-\gamma_m t} + \frac{r_m}{\gamma_m} (1 - e^{-\gamma_m t}).
\]</span> In the limit when <span class="math inline">\(t \rightarrow \infty\)</span> we can see that the steady-state solution is given by <span class="math display">\[
m_{ss} = \frac{r_m}{\gamma_m}.
\]</span> Fig. 1(C) shows the time evolution of <span class="math inline">\(m\)</span> for different initial values <span class="math inline">\(m_o\)</span>. We can see that indeed regardless of the initial mRNA count the system relaxes exponentially to <span class="math inline">\(m_{ss} = r_m / \gamma_m\)</span>.</p>
<figure>
<img src="ch1_fig01.png" id="fig:ch1_fig01" data-short-caption="Minimal model of gene expression" alt="Figure 1: Minimal model of gene expression. (A) Schematic of the kinetics governing gene expression. mRNA is produced at a constant rate r_m independent of the current mRNA copy number. Degradation of each mRNA occurs at a rate \gamma_m. (B) Example of the qualitative analysis of the mRNA dynamics via a 1D phase-portrait. The differential equation governing the dynamics contains two terms: a constant production rate given by r_m, and a degradation rate \gamma_m m, which depends on the current mRNA count. The main plot shows each of the components in the m vs. dm/dt plot. Since r_m does not depend on the current number of mRNA, it gives a straight production rate as a function of m. The total degradation rate depends linearly on the mRNA copy number, giving a line with slope \gamma_m. When the two components are equal (bot lines crossing), we obtain the steady-state mRNA value m_{ss}. The bottom line shows a qualitative schematic of the flow of the system towards this steady state. The further m is from m_{ss}, the faster it moves towards this point as schematized by the arrows’ size. (C) Example of mRNA dynamics for different initial conditions. Over time all curves converge to the steady-state mRNA value m_{ss}=r_m/\gamma_m. For this plot \gamma_m = 1 and r_m/\gamma_m = 10. The Python code (ch1_fig01C.py) used to generate part (C) of this figure can be found on the thesis GitHub repository." /><figcaption aria-hidden="true">Figure 1: <strong>Minimal model of gene expression.</strong> (A) Schematic of the kinetics governing gene expression. mRNA is produced at a constant rate <span class="math inline">\(r_m\)</span> independent of the current mRNA copy number. Degradation of each mRNA occurs at a rate <span class="math inline">\(\gamma_m\)</span>. (B) Example of the qualitative analysis of the mRNA dynamics via a 1D phase-portrait. The differential equation governing the dynamics contains two terms: a constant production rate given by <span class="math inline">\(r_m\)</span>, and a degradation rate <span class="math inline">\(\gamma_m m\)</span>, which depends on the current mRNA count. The main plot shows each of the components in the <span class="math inline">\(m\)</span> vs. <span class="math inline">\(dm/dt\)</span> plot. Since <span class="math inline">\(r_m\)</span> does not depend on the current number of mRNA, it gives a straight production rate as a function of <span class="math inline">\(m\)</span>. The total degradation rate depends linearly on the mRNA copy number, giving a line with slope <span class="math inline">\(\gamma_m\)</span>. When the two components are equal (bot lines crossing), we obtain the steady-state mRNA value <span class="math inline">\(m_{ss}\)</span>. The bottom line shows a qualitative schematic of the flow of the system towards this steady state. The further <span class="math inline">\(m\)</span> is from <span class="math inline">\(m_{ss}\)</span>, the faster it moves towards this point as schematized by the arrows’ size. (C) Example of mRNA dynamics for different initial conditions. Over time all curves converge to the steady-state mRNA value <span class="math inline">\(m_{ss}=r_m/\gamma_m\)</span>. For this plot <span class="math inline">\(\gamma_m = 1\)</span> and <span class="math inline">\(r_m/\gamma_m = 10\)</span>. The <a href="https://github.com/mrazomej/phd/blob/master/src/chapter_01/code/ch1_fig01C.py">Python code (<code>ch1_fig01C.py</code>)</a> used to generate part (C) of this figure can be found on the thesis <a href="https://github.com/mrazomej/phd">GitHub repository</a>.</figcaption>
</figure>
<p>So far, our model assumes a simple constant transcription rate <span class="math inline">\(r_m\)</span>; let us expand this term a little further to include regulation by a transcriptional repressor further down the road. We know that for a transcriptional event to occur, the RNA polymerase (RNAP) must bind to the promoter region and undergo a series of irreversible steps, such as opening the double helix to initiate the DNA sequence’s copying into mRNA <span class="citation" data-cites="Browning2004"> [<a href="#ref-Browning2004" role="doc-biblioref">8</a>]</span>. But before these irreversible steps take place, there is a chance that the RNAP falls off the promoter. If we assume these irreversible steps take place on a much longer timescale compared to the initial binding and unbinding of the RNAP on the promoter, we can separate the time scale and investigate them independently. In particular, we can write that mRNA production happens at a rate <span class="math display">\[
\text{mRNA production} = r_m \cdot p_{\text{bound}},
\label{eq:mRNA_prod}
\]</span> where we split the original production term into two steps: <span class="math inline">\(p_{\text{bound}}\)</span>, the probability of finding an RNAP bound to the promoter, and <span class="math inline">\(r_m\)</span> which captures all of the irreversible downstream steps that take place once the RNAP is engaged in a transcriptional event. A way to think about it–relevant to what I am doing right now as I type my thesis–is to think that the speed at which I type this document has to do with two things: The probability of me being actively working on these notes times the rate at which I type these notes once I engage in the activity. The reason this separation makes sense is that we can include the effect of the regulation by a transcriptional repressor as a reduction of the time (the probability) that the RNAP can be bound to the promoter. Furthermore, since we are assuming that the binding and unbinding of the RNAP happen at a timescale much faster than the downstream events, we can assume this binding reaction is in quasi-equilibrium, for which we can use the powerful theoretical framework of statistical mechanics. Let us now delve into the basics of this physical theory.</p>
<h3 id="the-unreasonable-effectiveness-of-unrealistic-simplifications">The unreasonable effectiveness of unrealistic simplifications</h3>
<p>In the preface of the textbook <em>Molecular Driving Forces</em> Dill and Bromberg introduce the idea of Statistical Mechanics as <em>the unreasonable effectiveness of unrealistic simplifications</em> <span class="citation" data-cites="Dill2010"> [<a href="#ref-Dill2010" role="doc-biblioref">14</a>]</span>. Although one could make the case that all of physics follows this description, it is undoubtedly evident that statistical mechanics is a vivid example of how simple ideas can have profound consequences. Statistical mechanics can be defined as the theory that, upon assuming the atomic nature of matter, explains the phenomenology that classical thermodynamics established from the interactions of the microscopic components of a system <span class="citation" data-cites="Dill2010"> [<a href="#ref-Dill2010" role="doc-biblioref">14</a>]</span>. As with any other physical theory, statistical mechanics is built from a set of <em>empirical</em> facts that define “axioms” that we take to be true. In other words, as Feynman famously described to us: if we want to come up with a new law of nature, there is a simple recipe that we must follow:</p>
<ol type="1">
<li><p>We guess the law. Literally. The most profound understanding of our physical reality we have comes from educated guesses made after a careful observation of nature.</p></li>
<li><p>We compute the consequences of such a guess. That is why mathematical theories allow us to sharpen our statements about how we think nature works.</p></li>
<li><p>We compare with experiments/observations. The scientific revolution came about when, after the dark ages, we finally learned it was okay to say “we don’t know.”</p></li>
</ol>
<p>In such a simple statement, Feynman tells us, lies the key to science <span class="citation" data-cites="Feynman1965"> [<a href="#ref-Feynman1965" role="doc-biblioref">15</a>]</span>. For our purpose of understanding the basis of statistical mechanics, we will argue that Boltzmann’s law gives the main law upon which the field is founded <span class="math display">\[
\frac{P(E_1)}{P(E_2)} = \frac{e^{-E_1 / k_BT}}{e^{-E_2 / k_BT}}.
\label{eq:boltzmann_law}
\]</span> Let us unpack this equation. The main idea behind statistical mechanics is that macroscopic observables (temperature and pressure in classic examples) are emergent properties dictated by the dynamics of the system’s microscopic components. What Boltzmann’s law tells us is that the relative probability of a system in thermal equilibrium to be found in a particular microstate with energy <span class="math inline">\(E_1\)</span> compared to being in a microstate with energy <span class="math inline">\(E_2\)</span> is given by an exponential function of the negative energy of such microstate relative to the thermal energy <span class="math inline">\(k_BT\)</span>. The minus sign in the exponent comes from the fact that states with negative energies are more favorable by convention in physics. Thus, having a large negative energy has a high probability when taking the exponential of minus such negative number. To provide concrete examples of what a microstate can look like, Fig. 2(A) shows three molecular systems relevant to biology. In the first example, we have the classic ligand-receptor binding problem; here, we imagine a solution can be discretized in space into a series of small boxes. In each of these boxes, one and only one ligand molecule can fit in. In principle, we can list all possible spatial arrangements of ligands. We could then calculate the relative likelihood of finding the system in any configurations as long as we can assign an energy value to each of them. The second example focuses on ligand-gated ion channels. In this particular system, we care about the ion channel’s state–either open or closed–and the ligands’ binding configuration. If the channel responds to the ligand’s concentration by changing its probability of gating, we can calculate using equilibrium statistical mechanics. Finally, the third example shows different configurations of a small patch of the cell membrane. All deformations of a membrane have energetic costs associated with them. By listing all possible membrane configurations, we can calculate the most likely shape of a membrane given the forces and stresses acting on it.</p>
<p>The macroscopic states that we observe can then be thought of as a coarse-graining of many microstates into a single macrostate. For example, in the ligand-receptor binding case, we rarely would care about the specific position of all the ligand molecules in the solution. What we would be interested in is whether or not the ligand is bound to the receptor. We can therefore define as our “macrostate” the particular configuration of the receptor as schematically shown in Fig. 2(B).</p>
<figure>
<img src="ch1_fig02.png" id="fig:ch1_fig02" data-short-caption="Boltzmann&#39;s law and the definition of a micro and macrostate" alt="Figure 2: Boltzmann’s law and the definition of a micro and macrostate. (A) Top panel: ligand-receptor binding microstates. Middle panel: ligand-gated ion channel microstates. Bottom panel: membrane patch deformations. (B) Schematic of the definition of a “macrostate.” In the ligand-receptor binding problem, we ignore all ligand molecules’ spatial configuration and focus on the receptor’s binding state." /><figcaption aria-hidden="true">Figure 2: <strong>Boltzmann’s law and the definition of a micro and macrostate.</strong> (A) Top panel: ligand-receptor binding microstates. Middle panel: ligand-gated ion channel microstates. Bottom panel: membrane patch deformations. (B) Schematic of the definition of a “macrostate.” In the ligand-receptor binding problem, we ignore all ligand molecules’ spatial configuration and focus on the receptor’s binding state.</figcaption>
</figure>
<p>If we want to know the likelihood of finding a particular system in any specific configuration, Boltzmann’s law (Eq. <span class="math inline">\(\ref{eq:boltzmann_law}\)</span>) is then telling us a protocol we must follow:</p>
<ol type="1">
<li><p>Enumerate all possible microstates in which the system can be found.</p></li>
<li><p>Compute the energy of each of these microstates.</p></li>
<li><p>Define the “macrostate” we care about by grouping all microstates that belong to the same energy.</p></li>
<li><p>Compute the Boltzmann factor. This factor, sometimes called the Boltzmann weight, is defined as the exponential of the negative energy divided by the thermal energy, as indicated in Eq. <span class="math inline">\(\ref{eq:boltzmann_law}\)</span>.</p></li>
</ol>
<p>To see this protocol in action, let us apply it to the calculation of <span class="math inline">\(p_{\text{bound}}\)</span>, the probability of finding an RNAP bound to the promoter. We will go through each of the protocol steps and build up the “unrealistic simplifications” that will allow us to make this calculation.</p>
<p><strong>1. Enumerate possible microstates.</strong> We begin by making a drastic coarse-graining of the bacterial genome. For us, a genome is simply made out of boxes where the RNAP can bind. We imagine that there is a single site where RNAP can bind specifically–the promoter of interest. There are also <span class="math inline">\(N_{NS} \approx 5\times 10^6\)</span> non-specific binding sites, one per basepair (bp) in the genome. This means that because of the sequence-dependent interactions between the RNAP molecule, and the DNA, the energy associated with specific binding to the gene promoter is more favorable than the rest of the genome. We ignore the fact that the RNAP footprint where it binds to the genome is roughly 30 bp. This assumption is valid if the number of available RNAP molecules is much smaller than the number of non-specific binding sites since it is improbable that two RNAPs would fall next to each other by pure chance. A useful analogy for this point is to think about sitting <span class="math inline">\(\sim \text{few}\times 10\)</span> people on a large stadium with <span class="math inline">\(\sim 10^4\)</span> seats. If the seats are chosen randomly, we do not need to worry about doing the sampling “without replacement” because the chances of two people ending up with the same seat number are negligible. We also ignore the possibility of RNAP not being bound to the genome. This assumption is supported by experimental evidence on a particular type of <em>E. coli</em> mutant that sheds lipid vesicles without segregating DNA into such vesicles. Mass spectrometry analysis on these “min-cells” has shown that there are no RNAP molecules to be found, implying that RNAPs are bound to DNA most if not all of the time <span class="citation" data-cites="Bintu2005"> [<a href="#ref-Bintu2005" role="doc-biblioref">11</a>]</span>. The exercise then consists of randomly choosing one box for each of the <span class="math inline">\(P\)</span> polymerases available to bind. Fig. 3 shows in the first column two possible configurations of our coarse-grained genome.</p>
<p><strong>2. Compute the energy for each microstate.</strong> Let us analyze the case where all <span class="math inline">\(P\)</span> RNAP molecules are bound non-specifically to the genome. For simplicity, we assume that RNAP binds to all <span class="math inline">\(N_{NS}\)</span> non-specific binding sites with the same affinity. We assign this energy to be <span class="math inline">\(\varepsilon_P^{(NS)}\)</span>. This assumption could be relaxed and we could assign instead a distribution of non-specific binding energies, as explored in <span class="citation" data-cites="Phillips2019"> [<a href="#ref-Phillips2019" role="doc-biblioref">16</a>]</span>. But for now, we don’t have to worry about this complication. For the statistical mechanics’ protocol the assignment of binding energies does not come from some quantum first-principled calculation or anything similar. We label the interaction of the RNAP and the rest of the genome with a single value, <span class="math inline">\(\varepsilon_P^{(NS)}\)</span>, that coarse-grains all of the hydrogen bonds and other effects that go into this physical process and gives an average energy. The calculation continues with this “labeled energy,” and, as we will see at the end, a very clean functional form emerges. Since we have <span class="math inline">\(P\)</span> such polymerases bound non specifically, the energy of any state with a similar configuration is then <span class="math inline">\(P \varepsilon_P^{(NS)}\)</span> as shown in Fig. 3 second column, top row.</p>
<p><strong>3. Define the “macrostate” we care about.</strong> In a sense, when we speak about macrostate, it does not necessarily mean something that we can macroscopically observe. What it means is that we group a bunch of states that we take to be functionally equivalent, as shown in Fig. 2(B). In our case, we only care about whether or not the RNAP is bound to our promoter of interest. The configuration of the rest of the background sites is irrelevant to our question. What this means in practice is that we must compute the degeneracy or multiplicity of our state. In other words, for the <em>specific</em> state shown in the first column/top row of Fig. 3 we know its Boltzmann weight. Eq. <span class="math inline">\(\ref{eq:boltzmann_law}\)</span> tells us that the probability of this particular configuration takes the form <span class="math display">\[
P_{\text{state}} \propto e^{-\beta P \varepsilon_P^{(NS)}},
\]</span> where we define <span class="math inline">\(\beta \equiv (k_BT)^{-1}\)</span>. The probability of this binding configuration takes this form since the <span class="math inline">\(P\)</span> RNAP molecules are bound non specifically. But every single arrangement in which all RNAPs are bound non-specifically has the same Boltzmann weight. The question then becomes: in how many of such microstates can the system exist? This is a combinatorics question of the form: in how many different ways can I arrange <span class="math inline">\(P\)</span> molecules into <span class="math inline">\(N_{NS}\)</span> boxes? Which of course, the answer is <span class="math display">\[
\text{\# states with all RNAPs bound non-specifically} = 
\frac{N_{NS}!}{P!(N_{NS} - P)!},
\]</span> as shown in the third column of Fig. 3. This multiplicity can be simplified if we consider that <span class="math inline">\(N_{NS} \gg P\)</span>. To more easily visualize how to simplify this let us for a second assume <span class="math inline">\(N_{NS} = 100\)</span> and <span class="math inline">\(P = 3\)</span>. Given the definition of factorials this means that <span class="math display">\[
\frac{N_{NS}!}{(N_{NS} - P)!} = 
\frac{100\cdot 99\cdot 98\cdots97\cdots 2\cdot 1}{97\cdots2\cdot 1} = 
100\cdot 99\cdot 98.
\]</span> Given this result, we can simply state that <span class="math inline">\(100\cdot 99\cdot 98 \approx 100^3\)</span>, only making a three percent error (<span class="math inline">\(100\cdot 99\cdot 98 / 100^3 \approx 0.97\)</span>). Imagine <span class="math inline">\(N_{NS}\)</span> is in the order of <span class="math inline">\(10^6\)</span>, then the error would become negligible. That is why, as shown in the third column of Fig. 3, we can approximate <span class="math display">\[
\frac{N_{NS}!}{P!(N_{NS} - P)!} \approx \frac{N_{NS}^P}{P!}, \;
\text{for }N_{NS} \gg P.
\]</span> For our other “macrostate” we have the case where only one out of the <span class="math inline">\(P\)</span> RNAPs is bound specifically for the promoter. We define the energy of this single RNAP specifically binding to the promoter as <span class="math inline">\(\varepsilon_{P}^{(S)}\)</span>. We assume that the other <span class="math inline">\(P-1\)</span> RNAPs are bound non-specifically with the usual energy <span class="math inline">\(\varepsilon_{P}^{(NS)}\)</span>. The way to realize this state is then given by <span class="math display">\[
\small
\text{\# states with one RNAP bound specifically} = 
\frac{N_{NS}!}{(P - 1)!(N_{NS} - (P - 1))!} \approx
\frac{N_{NS}^{P-1}}{(P-1)!}.
\]</span> What these Boltzmann weights mean is that for us <em>any</em> state on which a single RNAP is bound to the promoter while the rest are bound non specifically is equivalent. Therefore the probability of finding the promoter occupied by an RNAP would be of the form <span class="math display">\[
p_{\text{bound}} \propto e^{-\beta \epsilon_1} + e^{-\beta \epsilon_2} +
e^{-\beta \epsilon_3} + \cdots
\]</span> where <span class="math inline">\(\epsilon_i\)</span> is the energy of the <span class="math inline">\(i^{\text{th}}\)</span> state that has a single RNAP bound to the promoter. But we established that all of the <span class="math inline">\(\epsilon_i\)</span> energies are the same. So instead of writing this long sum, we multiply the Boltzmann weight of a single state by the number of states with equivalent energy, i.e., we multiply it by the state’s multiplicity or degeneracy. The same logic applies for the states where none of the RNAPs are specifically bound to the promoter.</p>
<p><strong>4. Compute the Boltzmann Factor.</strong> The last step in the protocol is to follow the recipe indicated by Eq. <span class="math inline">\(\ref{eq:boltzmann_law}\)</span>. We exponentiate the energy, with the caveat we mentioned on the last point that this time we multiply by the multiplicity that we just computed. This is because we are lumping together all microstates into a single functional macrostate. So the Boltzmann weight for the unbound <span class="math inline">\(\rho_{\text{unbound}}\)</span> macrostate is given by <span class="math display">\[
\rho_{\text{unbound}} = \frac{N_{NS}^P}{P!}
e^{-\beta P \varepsilon_P^{(NS)}}.
\]</span> For the bound state, we have <span class="math display">\[
\rho_{\text{bound}} = \frac{N_{NS}^{P-1}}{(P-1)!}
e^{-\beta \left(\varepsilon_P^{(S)} +  (P - 1) \varepsilon_P^{(NS)}\right)}.
\]</span> For reasons that will become clear later in this chapter once we work with the entropy and derive the Boltzmann distribution, we know that to compute the probability of a specific microstate (or a macrostate), we simply take the Boltzmann weight of the microstate and divide by the <em>sum</em> of all of the other Boltzmann weights of the states available to the system. This sum of Boltzmann weights place a very special role in statistical mechanics, and it is known as the <em>partition function</em> of the system. Therefore, to calculate <span class="math inline">\(p_{\text{bound}}\)</span> we compute <span class="math display">\[
p_{\text{bound}} = 
\frac{\rho_{\text{bound}}}{\rho_{\text{unbound}} + \rho_{\text{bound}}}.
\]</span> Substituting the Boltzmann weights we derived, we find <span class="math display">\[
p_{\text{bound}} = 
\frac{
\frac{N_{NS}^{P-1}}{(P-1)!}
e^{-\beta \left(\varepsilon_P^{(S)} +  (P - 1) \varepsilon_P^{(NS)}\right)}
}{
\frac{N_{NS}^{P-1}}{(P-1)!}
e^{-\beta \left(\varepsilon_P^{(S)} +  (P - 1) \varepsilon_P^{(NS)}\right)}
+ 
\frac{N_{NS}^P}{P!}
e^{-\beta P \varepsilon_P^{(NS)}}
},
\]</span> an algebraic nightmare. We can simplify this expression enormously by multiplying the numerator and denominator by <span class="math inline">\(\rho_{\text{unbound}}^{-1}\)</span>. Upon simplification, we find the neat expression <span class="math display">\[
p_{\text{bound}} = 
\frac{
    \frac{P}{N_{NS}} e^{-\beta \Delta \varepsilon_P}
}{
    1 + \frac{P}{N_{NS}} e^{-\beta \Delta \varepsilon_P}
},
\label{eq:pbound_unreg}
\]</span> where <span class="math inline">\(\Delta\varepsilon_P \equiv \varepsilon_P^{(S)} - \varepsilon_P^{(NS)}\)</span>. This simple expression, known as the Langmuir isothermal binding curve, tells us that the more RNAPs available (larger <span class="math inline">\(P\)</span>), or the stronger the promoter is (more negative <span class="math inline">\(\Delta\varepsilon_P\)</span>), the more likely it is to find the promoter bound by an RNAP, and according to Eq. <span class="math inline">\(\ref{eq:mRNA_prod}\)</span>, the higher the mRNA production. In the next section, we connect this model to experimental measurements.</p>
<figure>
<img src="ch1_fig03.png" id="fig:ch1_fig03" data-short-caption="Statistical Mechanics protocol for RNAP binding" alt="Figure 3: Statistical Mechanics protocol for RNAP binding. On a discretized genome we follow the statistical mechanics’ protocol to compute the Boltzmann weight of each of the relevant microstates. The P available RNAPs are assumed to have two binding configurations: One specific binding to the promoter of interest (with energy \varepsilon_P^{(S)}) and non-specific to any of the N_{NS} non-specific binding sites (with energy \varepsilon_P^{(NS)})." /><figcaption aria-hidden="true">Figure 3: <strong>Statistical Mechanics protocol for RNAP binding.</strong> On a discretized genome we follow the statistical mechanics’ protocol to compute the Boltzmann weight of each of the relevant microstates. The <span class="math inline">\(P\)</span> available RNAPs are assumed to have two binding configurations: One specific binding to the promoter of interest (with energy <span class="math inline">\(\varepsilon_P^{(S)}\)</span>) and non-specific to any of the <span class="math inline">\(N_{NS}\)</span> non-specific binding sites (with energy <span class="math inline">\(\varepsilon_P^{(NS)}\)</span>).</figcaption>
</figure>
<h3 id="figure-1-theory-in-gene-regulation">Figure 1 theory in gene regulation</h3>
<p>We began this section with a simple model for the dynamics of mRNA production and degradation. We then expanded our model to deconvolve the production term into the rate at which mRNA is produced by RNAP, and the probability of finding such RNAP bound to the promoter. To calculate this probability, we used the statistical mechanics’ protocol, which culminated in Eq. <span class="math inline">\(\ref{eq:pbound_unreg}\)</span>. So far, we are missing two important steps in our logical construction that will lead us to specific quantitative predictions that we can test experimentally:</p>
<ol type="1">
<li><p>The inclusion of a regulatory scheme via a transcriptional repressor.</p></li>
<li><p>The connection of the model with experimentally accessible quantities.</p></li>
</ol>
<p>As hinted at earlier, for a transcriptional repressor, we imagine that the repressor’s effect on the regulation of the gene acts only through changes in <span class="math inline">\(p_{\text{bound}}\)</span>. To include the regulation, we add a series of microstates. Rather than having only <span class="math inline">\(P\)</span> RNAP molecules to bind the genome, we also have <span class="math inline">\(R\)</span> repressors that can bind specifically and non-specifically. Through the same statistical mechanics’ protocol as for the previous case, we can arrive at the Boltzmann weights shown for the three “macrostates” in Fig. 4(A). For the regulated case, we have that the probability of the promoter being bound by an RNAP takes the form <span class="math display">\[
p_{\text{bound}}(R &gt; 0) = 
\frac{
    \frac{P}{N_{NS}} e^{-\beta \Delta \varepsilon_P}
}{
    1 
    + \frac{P}{N_{NS}} e^{-\beta \Delta \varepsilon_P}
    + \frac{R}{N_{NS}} e^{-\beta \Delta \varepsilon_R}
},
\label{eq:pbound_reg}
\]</span> where <span class="math inline">\(\Delta\varepsilon_R\)</span> is the binding energy difference between the repressor binding to a specific binding site and a non-specific one. Although exciting and insightful, the quantities we have derived so far do not have an immediate <strong>quantitative</strong> prediction we can connect with experimental measurements. For example, for the regulated case, the steady-state mRNA count takes the form <span class="math display">\[
m_{ss}(R &gt; 0) = 
\frac{r_m}{\gamma_m} p_{\text{bound}}(R &gt; 0) = 
\frac{r_m}{\gamma_m}
\frac{
    \frac{P}{N_{NS}} e^{-\beta \Delta \varepsilon_P}
}{
    1 
    + \frac{P}{N_{NS}} e^{-\beta \Delta \varepsilon_P}
    + \frac{R}{N_{NS}} e^{-\beta \Delta \varepsilon_R}
}.
\]</span> Determining <span class="math inline">\(r_m\)</span> or <span class="math inline">\(\gamma_m\)</span> directly from experiments, although possible, represents an enormous technical challenge. A convenient metric we can use instead is what we call the fold-change in gene expression. Fig. 4(B) shows a schematic representation of what we mean by the fold-change. This ratiometric quantity normalizes the expression level of a gene with regulation given by a transcriptional repressor by the expression level of the same gene in the absence of the regulation–via a knock-out of the repressor gene, for example. Mathematically this is defined as <span class="math display">\[
\text{fold-change} \equiv \frac{m_{ss}(R &gt; 0)}{m_{ss}(R = 0)}.
\]</span> This expression is convenient because upon taking the ratio of these steady-state mRNA counts, the ratio <span class="math inline">\(r_m / \gamma_m\)</span> drops out of the equation. All we are left with is then the ratio of the <span class="math inline">\(p_{\text{bound}}\)</span>s <span class="math display">\[
\text{fold-change} = \frac{p_{\text{bound}}(R &gt; 0)}{p_{\text{bound}}(R = 0)}.
\]</span> Substitutin Eqs. <span class="math inline">\(\ref{eq:pbound_unreg}\)</span> and <span class="math inline">\(\ref{eq:pbound_reg}\)</span> results in <span class="math display">\[
\text{fold-change} = 
\frac{
    1 
    + \frac{P}{N_{NS}} e^{-\beta \Delta \varepsilon_P}
}{
    1 
    + \frac{P}{N_{NS}} e^{-\beta \Delta \varepsilon_P}
    + \frac{R}{N_{NS}} e^{-\beta \Delta \varepsilon_R}
}.
\]</span> We appeal to some experimental understanding of the bacterial proteome composition <span class="citation" data-cites="Bremer1996 Schmidt2016 Grigorova2006"> [<a href="#ref-Bremer1996" role="doc-biblioref">17</a>–<a href="#ref-Grigorova2006" role="doc-biblioref">19</a>]</span>. RNAP copy number in <em>E. coli</em> is of the order <span class="math inline">\(P \sim 10^3-10^4\)</span> <span class="citation" data-cites="Grigorova2006"> [<a href="#ref-Grigorova2006" role="doc-biblioref">19</a>]</span>. The binding affinity of these promoters is of the order <span class="math inline">\(\Delta\varepsilon_P \sim -2\pm 1\;k_BT\)</span> <span class="citation" data-cites="Bintu2005"> [<a href="#ref-Bintu2005" role="doc-biblioref">11</a>]</span>. Along with the value of <span class="math inline">\(N_{NS}\sim 10^6\)</span> This results in <span class="math display">\[
\frac{P}{N_{NS}} e^{-\beta \Delta \varepsilon_P} \approx
\frac{10^3}{10^6}e^{2.3} \approx
\frac{10^3 \cdot 10}{10^6} \approx 10^{-2} \ll 1,
\]</span> the so-called weak-promoter approximation For the repressor we have that most repressors in <em>E. coli</em> are in the order of <span class="math inline">\(R \sim 10\)</span> <span class="citation" data-cites="Schmidt2016"> [<a href="#ref-Schmidt2016" role="doc-biblioref">18</a>]</span>. Their binding affinities take values between <span class="math inline">\(\Delta\varepsilon_R \sim -15 \pm 5\; k_BT\)</span> <span class="citation" data-cites="Bintu2005"> [<a href="#ref-Bintu2005" role="doc-biblioref">11</a>]</span>. These numerical values then give <span class="math display">\[
\frac{R}{N_{NS}} e^{-\beta \Delta \varepsilon_R} \approx
\frac{10}{10^6}e^{15} \approx
\frac{10 \cdot 10^6}{10^6} \approx 10.
\]</span> If we implement these approximations, we can justify simplifying the fold-change equation to take the form <span class="math display">\[
\text{fold-change} \approx
\left(
    1 + \frac{R}{N_{NS}} e^{-\beta\Delta\varepsilon_R}
\right)^{-1}.
\label{eq:fc}
\]</span> As shown in Fig. 4(C), this expression points directly at two experimental knobs that we can tune using molecular biology. We can modify the number of repressors by changing the ribosomal binding site sequence (RBS) of the repressor gene <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">20</a>]</span>. What that means is that with a sequence-dependent manner, the ribosome translates mRNAs according to a specific region of the gene known as the RBS <span class="citation" data-cites="Chen2013"> [<a href="#ref-Chen2013" role="doc-biblioref">21</a>]</span>. Furthermore, we can change the repressor’s affinity for its binding site by mutating the binding site itself <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">20</a>]</span>. Fig. 4(D) shows predictions of Eq. <span class="math inline">\(\ref{eq:fc}\)</span> for different binding energies.</p>
<p>The model and the predictions presented here were worked out by Garcia and Phillips in a classic publication in 2011 <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">20</a>]</span>. In the next chapter we build upon this theoretical scaffold to expand the predictive power of the model by including the allosteric nature of the transcription factor that allows the cells to change their genetic program upon the presence of an external molecule as a response to the environment.</p>
<figure>
<img src="ch1_fig04.png" id="fig:ch1_fig04" data-short-caption="Figure 1 theory in transcriptional regulation" alt="Figure 4: Figure 1 theory in transcriptional regulation. (A) States and (normalized) weights for the simple repression motif. The promoter can be found in three states: 1) empty, 2) bound by an RNAP, 3) bound by a repressor. The same statistical mechanics’ protocol as in Fig. 3 can be used to derive the weights. (B) Schematic of the experimental determination of the fold-change in gene expression. The expression level of a regulated strain is normalized by the expression level of a strain with a repressor’s knock-out. (C) Experimentally accessible knobs predicted from the theoretical model. The number of transcription factors can be tuned by changing the amount of protein produced per mRNA. The binding energy of the repressor can be tuned by mutating the basepairs in the binding site. (D) Fold-change as a function of the repressor copy number for different binding energies. The Python code (ch1_fig04D.py) used to generate part (C) of this figure can be found on the thesis GitHub repository." /><figcaption aria-hidden="true">Figure 4: <strong>Figure 1 theory in transcriptional regulation.</strong> (A) States and (normalized) weights for the simple repression motif. The promoter can be found in three states: 1) empty, 2) bound by an RNAP, 3) bound by a repressor. The same statistical mechanics’ protocol as in Fig. 3 can be used to derive the weights. (B) Schematic of the experimental determination of the fold-change in gene expression. The expression level of a regulated strain is normalized by the expression level of a strain with a repressor’s knock-out. (C) Experimentally accessible knobs predicted from the theoretical model. The number of transcription factors can be tuned by changing the amount of protein produced per mRNA. The binding energy of the repressor can be tuned by mutating the basepairs in the binding site. (D) Fold-change as a function of the repressor copy number for different binding energies. The <a href="https://github.com/mrazomej/phd/blob/master/src/chapter_01/code/ch1_fig04D.py">Python code (<code>ch1_fig04D.py</code>)</a> used to generate part (C) of this figure can be found on the thesis <a href="https://github.com/mrazomej/phd">GitHub repository</a>.</figcaption>
</figure>
<h3 id="all-cells-are-equal-but-some-are-more-equal-than-others">All cells are equal, but some are more equal than others</h3>
<p>One of the great discoveries that came from the single-cell biology revolution where we began to measure individual cellular behavior rather than bulk observations, was the discovery of the intrinsic cell-to-cell variability in many aspects of biology, gene expression being the canonical example <span class="citation" data-cites="Eldar2010"> [<a href="#ref-Eldar2010" role="doc-biblioref">22</a>]</span>. This means that two cells with the same genome exposed to the same conditions will not express the same number of mRNAs and proteins of any specific gene. From a statistical physics perspective, this is not entirely “surprising” since we know that a system can be found in many different microstates as described in Fig. 2(A). What is different here is that a cell does not have an Avogadro number of mRNA (or, for that matter of anything) in it, making these fluctuations more relevant. If we think of fluctuations scaling as <span class="math inline">\(\sqrt{N}\)</span>, that means that for an <span class="math inline">\(N\)</span> of <span class="math inline">\(\approx\)</span> ten molecules or so, these variations can be significant in terms of the downstream cellular behavior. Cells have to cope with these physical limitations on precision, many times generating systems to actively buffer as much of the “noise” as possible <span class="citation" data-cites="Voliotis2014a"> [<a href="#ref-Voliotis2014a" role="doc-biblioref">23</a>]</span>, other times using this intrinsic variability to their advantage <span class="citation" data-cites="Balaban2004"> [<a href="#ref-Balaban2004" role="doc-biblioref">24</a>]</span>.</p>
<figure>
<img src="ch1_fig05.png" id="fig:ch1_fig05" data-short-caption="Chemical master equation in gene regulation" alt="Figure 5: Chemical master equation in gene regulation. (A-B) Different points of view to understand the chemical master equation. (A) From the “particle” point of view, we imagine following the time trajectory of a single cell. The probability P(m, t) of finding a cell with m mRNAs at time t is then proportional to the time this cell spent with this number of molecules. (B) On the occupation number point of view we imagine observing a large number of isogenic cells (different colors represent the individuality of each cell). The probability P(m,t) is then interpreted as the fraction of the cells representing such copy number exactly at time t. (C) Chemical master equations mathematize the idea of Markov processes. For the case of the unregulated promoter, the Markov process consists of a connection of an infinite number of discrete states that cells can transition between by producing or degrading mRNAs. (D) Spread-the-butter idea. Since probability is conserved, the central bar’s height changes slightly by having in- and outflow of probability mass from the contiguous bins. The Python code (ch1_fig05A.py) used to generate the plot in part (A) of this figure can be found on the thesis GitHub repository." /><figcaption aria-hidden="true">Figure 5: <strong>Chemical master equation in gene regulation.</strong> (A-B) Different points of view to understand the chemical master equation. (A) From the “particle” point of view, we imagine following the time trajectory of <em>a single cell</em>. The probability <span class="math inline">\(P(m, t)\)</span> of finding a cell with <span class="math inline">\(m\)</span> mRNAs at time <span class="math inline">\(t\)</span> is then proportional to the time this cell spent with this number of molecules. (B) On the occupation number point of view we imagine observing a large number of isogenic cells (different colors represent the individuality of each cell). The probability <span class="math inline">\(P(m,t)\)</span> is then interpreted as the fraction of the cells representing such copy number exactly at time <span class="math inline">\(t\)</span>. (C) Chemical master equations mathematize the idea of Markov processes. For the case of the unregulated promoter, the Markov process consists of a connection of an infinite number of discrete states that cells can transition between by producing or degrading mRNAs. (D) Spread-the-butter idea. Since probability is conserved, the central bar’s height changes slightly by having in- and outflow of probability mass from the contiguous bins. The <a href="https://github.com/mrazomej/phd/blob/master/src/chapter_01/code/ch1_fig05A.py">Python code (<code>ch1_fig05A.py</code>)</a> used to generate the plot in part (A) of this figure can be found on the thesis <a href="https://github.com/mrazomej/phd">GitHub repository</a>.</figcaption>
</figure>
<p>The central assumption behind the thermodynamic models of gene regulation that we studied in the last section is that the gene expression is proportional to the probability of finding an RNAP bound to the promoter <span class="citation" data-cites="Gerland2002 Bintu2005"> [<a href="#ref-Bintu2005" role="doc-biblioref">11</a>,<a href="#ref-Gerland2002" role="doc-biblioref">25</a>]</span>. A consequence of this construction is that the probability space–the set of all possible events captured by the distribution–only looks at the state of the promoter itself, not at the state of the mRNA copy number. That is why thermodynamic models of this kind do not speak to the intrinsic cell-to-cell variability. For this, we need to use the so-called chemical master equation framework <span class="citation" data-cites="Sanchez2013"> [<a href="#ref-Sanchez2013" role="doc-biblioref">26</a>]</span>. There are two ways of thinking about the chemical master equation:</p>
<ol type="1">
<li><p>The “particle” point of view.</p></li>
<li><p>The occupation number point of view.</p></li>
</ol>
<p>Depending on the context, we might want to use either of these approaches to write down the master equation for our problem of interest. Let us look into these two different ways of interpreting a master equation using our example of a cell producing mRNA. For the particle point of view, schematized in Fig. 5(A), we imagine following the mRNA copy number <span class="math inline">\(m\)</span> of a single cell. The number of mRNAs in the cell change stochastically from time point to time point. On the one hand, there can be a transcriptional event that increases the number of mRNAs, and on the other hand, an mRNA can be degraded, decreasing the number of mRNAs. Suppose we imagine tracking this cell for a very long time. In that case, we can quantify the fraction of the time that the cell spent with zero mRNAs, one, two, and so on and from that, build the probability distribution <span class="math inline">\(P(m, t)\)</span> of having <span class="math inline">\(m\)</span> mRNA at time <span class="math inline">\(t\)</span> (there is a subtle point here of the process being memoryless, but I do not want to get into it). The occupation number point of view, schematized in Fig. 5(B), takes a different perspective. For this, we imagine tracking not one but many cells simultaneously. Each cell can either produce or degrade an mRNA on a short time window, changing its total individual count. The probability <span class="math inline">\(P(m, t)\)</span> is then built from counting how many cells out of the total have <span class="math inline">\(m\)</span> mRNAs.</p>
<p>Regardless of how we think about the chemical master equation, both of these perspectives describe a Markov process. These are stochastic processes in which a system transitions between different states, but the transitions between such states are only governed by the transition rates between the states and the current state of the system. In other words, a Markov process keeps no track of the states it previously visited; the only factor that determines where is the system going to head is its current state, and the transition rates out of such state–that is why these are considered memoryless processes. Fig. 5(C) shows a schematic of what a Markov process looks like. The schematic of the unregulated promoter indicates that there are two possible reactions: an mRNA production with rate <span class="math inline">\(r_m\)</span> and degradation with rate <span class="math inline">\(\gamma_m\)</span>. The Markov process for this simple model can then be represented as a series of nodes (representing the mRNA counts) connected with bi-directional arrows (representing the transition rates between states) indicating that the transitions can only take place between contiguous states.</p>
<p>In practice, the way we write down a chemical master equation is by a process christened by Professor Jane Kondev as “spread-the-butter.” The idea of spread the butter is that some probability mass (the analogous of the butter) is to be spread over the range of possible values (the equivalent of the toast) where probability mass migrates in and out of a particular bin keeping the total amount of probability to add up to one. The best way to explain this concept is by following the schematic in Fig. 5(D) and going through the math. Let us imagine we are keeping track of a particular mRNA value <span class="math inline">\(m\)</span>–the chemical master equations are in reality, a system of many coupled equations, one for each mRNA count. We want to write down an equation that describes what is the probability of finding a cell with this particular count a small time window into the future <span class="math inline">\(P(m, t + \Delta t)\)</span>, where <span class="math inline">\(t\)</span> represents the time “right now,” and <span class="math inline">\(\Delta t\)</span> is a tiny time increment. The master equation is nothing more than a checks and balances notebook to keep track of all the flow of probability mass in and out of the bin we are interested in, as shown in Fig. 5(D). Informally we would write the equation as <span class="math display">\[
P(m, t + \Delta t) = P(m, t)
+ \sum \left({\text{transitions from} \atop m&#39;\text{ to }m}\right)
- \sum \left({\text{transitions from} \atop m\text{ to }m&#39;}\right),
\label{eq:master_intuition}
\]</span> where we are describing the three main components that go into the equation for <span class="math inline">\(P(m, t + \Delta t)\)</span>:</p>
<ol type="1">
<li><p>The probability of having <span class="math inline">\(m\)</span> mRNA right now,</p></li>
<li><p>the inflow of probability from other copy numbers <span class="math inline">\(m&#39;\)</span> via production and degradation,</p></li>
<li><p>the outflow of probability from <span class="math inline">\(m\)</span> to other copy numbers via production and degradation.</p></li>
</ol>
<p>Taking our time window <span class="math inline">\(\Delta t\)</span> to be sufficiently small, we can focus only on the two contiguous mRNA counts <span class="math inline">\(m-1\)</span> and <span class="math inline">\(m+1\)</span>, and ignore the rest since jumps from further counts become increasingly improbable as the time step gets smaller. Fig. 5(D) shows the four in- and outflows that can happen. Let us rewrite Eq. <span class="math inline">\(\ref{eq:master_intuition}\)</span> following this schematic. If a cell has <span class="math inline">\(m - 1\)</span> mRNA and during the time window <span class="math inline">\(\Delta t\)</span> produces one molecule, then it passes from state <span class="math inline">\(m - 1\)</span> to state <span class="math inline">\(m\)</span>. This transition contributes to the inflow of probability mass by a factor <span class="math inline">\((r_m \Delta t) P(m-1, t)\)</span>, where we can think of <span class="math inline">\(r_m \Delta t\)</span> as the probability of the transcription event taking place during the time window, and this multiplies the probability of having <span class="math inline">\(m - 1\)</span> mRNA to begin with. A similar argument can be made for all transitions in and out of <span class="math inline">\(m\)</span> depicted in Fig. 5(D), with the only difference that as in Eq. <span class="math inline">\(\ref{eq:m_t_Delta_t}\)</span>, the degradation of an mRNA molecule is proportional to the total number of molecules. The resulting equation for <span class="math inline">\(P(m, t + \Delta t)\)</span> then takes the form <span class="math display">\[
\begin{split}
P(m, t + \Delta t) = &amp;P(m, t)
+ \overbrace{(r_m \Delta t) P(m-1, t)}^{m-1 \rightarrow m}
+ \overbrace{(\gamma_m \Delta t) (m + 1) P(m + 1, t)}^{m+1 \rightarrow m}\\
&amp;- \overbrace{(r_m \Delta t) P(m, t)}^{m \rightarrow m+1}
- \overbrace{(\gamma_m \Delta t) m P(m, t)}^{m \rightarrow m-1}
\end{split}.
\label{eq:master_recipe}
\]</span> We send the first term on the right-hand side to the left, divide both sides by <span class="math inline">\(\Delta t\)</span> and take the limit when <span class="math inline">\(\Delta t \rightarrow 0\)</span>. This gives us the master equation we were searching for <span class="math display">\[
\frac{dP(m, t)}{dt} = 
r_m P(m - 1, t) + \gamma_m (m + 1) P(m + 1, t)
- r_m P(m, t) + \gamma_m m P(m, t).
\label{eq:master_simple}
\]</span> Eq. <span class="math inline">\(\ref{eq:master_simple}\)</span> is not isolated. It represents an infinite-dimensional system of coupled ordinary differential equations (one for each mRNA copy number <span class="math inline">\(m\)</span>). It can therefore be tricky to work directly with these types of equations. Instead, let us take Eq. <span class="math inline">\(\ref{eq:master_recipe}\)</span> for a ride. With modern computational power, we can explicitly use this equation as a recipe on how to update an mRNA distribution numerically. Fig. 6 shows such numerical integration for a system with initially no mRNAs present. This could be achieved experimentally by having an inducible system, adding the inducer, and tracking the time evolution of the single-molecule mRNA counts inside cells. Fig. 6(A) presents a heatmap of such time evolution with time running on the vertical axis, while Fig. 6(B) presents specific snapshots. We can see that the distribution begins as a single peak (a delta function in the physics jargon) centered at zero mRNAs. The distribution then relaxes to a broader shape and remains the same after that. This suggests that the distribution converges to a steady-state. Let us compute this steady state distribution.</p>
<figure>
<img src="ch1_fig06.png" id="fig:ch1_fig06" data-short-caption="Chemical master equation in gene regulation" alt="Figure 6: Time evolution of mRNA distribution. . (A) Heat map of the time evolution of the mRNA distribution (Eq. \ref{eq:master_simple}) with P(m=0, t=0) = 1, i.e., a delta function at zero mRNAs at time zero. (B) Snapshots of the same time-evolving distribution at different time points. The Python code (ch1_fig06.py) used to generate the plot in part (A) of this figure can be found on the thesis GitHub repository." /><figcaption aria-hidden="true">Figure 6: <strong>Time evolution of mRNA distribution.</strong> . (A) Heat map of the time evolution of the mRNA distribution (Eq. <span class="math inline">\(\ref{eq:master_simple}\)</span>) with <span class="math inline">\(P(m=0, t=0) = 1\)</span>, i.e., a delta function at zero mRNAs at time zero. (B) Snapshots of the same time-evolving distribution at different time points. The <a href="https://github.com/mrazomej/phd/blob/master/src/chapter_01/code/ch1_fig06.py">Python code (<code>ch1_fig06.py</code>)</a> used to generate the plot in part (A) of this figure can be found on the thesis <a href="https://github.com/mrazomej/phd">GitHub repository</a>.</figcaption>
</figure>
<p>In this system, where we have a series of state transitions as represented in Fig. 5(C), steady-state is reached when the flux of probability from two contiguous states is zero. In other words, when the probability distribution does not change over time anymore, the flow of probability from state <span class="math inline">\(m=0\)</span> to state <span class="math inline">\(m=1\)</span> should be the same as the reverse. The same condition applies to all other pairs of states. Mathematically this is expressed as <span class="math display">\[
\overbrace{r_m P(0)}^{0 \rightarrow 1} = 
\overbrace{\gamma_m \cdot 1 \cdot P(1)}^{1 \rightarrow 0},
\]</span> where we removed the time dependency from <span class="math inline">\(P(m, t)\)</span> since we are at steady-state. Solving for <span class="math inline">\(P(1)\)</span> results in <span class="math display">\[
P(1) = \left(\frac{r_m}{\gamma_m} \right) P(0).
\]</span> The same condition applies between state <span class="math inline">\(m=1\)</span> and <span class="math inline">\(m=2\)</span>, resulting in <span class="math display">\[
\overbrace{r_m P(1)}^{1 \rightarrow 2} = 
\overbrace{\gamma_m \cdot 2 \cdot P(2)}^{2 \rightarrow 1}.
\]</span> Again, we can solve for <span class="math inline">\(P(2)\)</span> and obtain <span class="math display">\[
P(2) = \frac{1}{2}\left(\frac{r_m}{\gamma_m} \right) P(1).
\]</span> Substituting the solution for <span class="math inline">\(P(1)\)</span> gives <span class="math display">\[
P(2) = \frac{1}{2} \left(\frac{r_m}{\gamma_m} \right)^2 P(0).
\]</span> Let’s do one more example to see the general pattern. Between <span class="math inline">\(m=2\)</span> and <span class="math inline">\(m=3\)</span> we have <span class="math display">\[
\overbrace{r_m P(2)}^{2 \rightarrow 3} = 
\overbrace{\gamma_m \cdot 3 \cdot P(3)}^{3 \rightarrow 2}.
\]</span> Following the same procedure and substitutions results in <span class="math display">\[
P(3) = \frac{1}{2\cdot 3} \left(\frac{r_m}{\gamma_m} \right)^3 P(0).
\]</span> Deducing the pattern from these examples, we can see that for any <span class="math inline">\(m\)</span> we have <span class="math display">\[
P(m) = P(0) \frac{\left( \frac{r_m}{\gamma_m}\right)^m}{m!}.
\]</span> All we have left is the unknown value <span class="math inline">\(P(0)\)</span>. To get at it, we use the fact that the distribution must be normalized, giving <span class="math display">\[
\sum_{m=0}^{\infty} P(m)=1 \Rightarrow  P(0)\sum_{m=0}^{\infty} 
\frac{\left(\frac{r_m}{\gamma_m}\right)^{m}}{m !}=1.
\]</span> We recognize the sum as the Taylor series for <span class="math inline">\(e^x\)</span>. This means that our constant <span class="math inline">\(P(0)\)</span> is given by <span class="math display">\[
P(0) = \frac{1}{\sum_{m=0}^{\infty} 
\frac{\left(\frac{r_m}{\gamma_m}\right)^{m}}{m
!}} = e^{-r_m / \gamma_m}.
\]</span> Substituting this result, we find that the mRNA steady-state distribution is a Poission distribution with mean <span class="math inline">\(r_m/\gamma_m\)</span>, i.e., <span class="math display">\[
P(m) = \frac{e^{- r_m / \gamma_m} \left( \frac{r_m}{\gamma_m}\right)^m}{m!}.
\label{eq:mRNA_steady}
\]</span></p>
<h2 id="entropy-information-and-the-math-behind-the-bit">Entropy, information, and the math behind the bit</h2>
<p>Central to the endeavor undertaken in this thesis is the idea that cells can process information from the environment to up or down-regulate their genes to generate an appropriate response to these external signals. Information as a concept is a very plastic term that we commonly use to explain having helpful knowledge to use to our advantage. Phrases such as “<em>that person carries so much information in her brain. She truly knows everything!</em>” point at this somewhat imprecise concept of what we mean by information.</p>
<p>In 1948, while working at Bell Labs, Claude Shannon shocked the world with his seminal work that would go to define the field of information theory <span class="citation" data-cites="Shannon1948"> [<a href="#ref-Shannon1948" role="doc-biblioref">27</a>]</span>. In his paper, Shannon gave us a precise mathematical definition of information. To understand Shannon’s logic better, we need to put it in the context that he was thinking about: communication systems such as the telephone or the telegraph. Although seemingly unrelated to our problem of cells sensing the environment, these systems are incredibly powerful in their conceptual and explanatory reach. For Shannon, the main problem of communication consisted of reproducing a message emitted at one point in space and time with fidelity at a different point. Usually, these messages carry with them <em>meaning</em> (otherwise, why would we even want to send such messages) by which we typically mean that the message “refers to or is correlated according to some system with certain physical or conceptual entities” <span class="citation" data-cites="Shannon1948"> [<a href="#ref-Shannon1948" role="doc-biblioref">27</a>]</span>. But for the task of engineering a reliable communication system, this meaning is irrelevant–in the same way that whatever the cell decides to do with the meaning of the signals obtained from the environment can be thought as irrelevant for the biophysics of how the signal is sensed.</p>
<p>As shown schematically in Fig. 7(A) from Shannon’s original work, a communication system essentially consists of five components:</p>
<ol type="1">
<li><p>An <strong>information source</strong> which produces a message (or sequence of messages) to be communicated to the receiving terminal.</p></li>
<li><p>A <strong>transmitter</strong> which takes the message, converts it into a suitable signal compatible with the communication channel.</p></li>
<li><p>The <strong>channel</strong> that is the medium used to transmit the signal from the transmitter to the receiver.</p></li>
<li><p>The <strong>receiver</strong> in charge of inverting the operation done by the transmitter, reconstructing the original message.</p></li>
<li><p>The <strong>destination</strong> for whom the message is intended.</p></li>
</ol>
<p>Fig. 7(B) shows an analogous schematic to Fig. 7(A) with the relevant components involved in the gene expression context that we focus on in this thesis. In our bacterial gene regulation model, the information source role is played by a small molecule’s environmental concentration. It is this signal that the cells are trying to measure and respond to by up-regulating the expression of a gene. This signal transmitter is the allosteric transcription factor whose conformation depends on the concentration of the small molecule. The receiver of the signal is the DNA promoter that orchestrates the protein expression, which plays the receiver’s role.</p>
<figure>
<img src="ch1_fig07.png" id="fig:ch1_fig07" data-short-caption="Abstract communication system" alt="Figure 7: Abstract communication system. (A) Reproduced from Shannon’s original seminal work  [27]. The schematic shows an abstract communication system with all the components. (B) Adaptation of the Shannon communication system to the context of bacterial gene expression regulated by an allosteric transcription factor." /><figcaption aria-hidden="true">Figure 7: <strong>Abstract communication system.</strong> (A) Reproduced from Shannon’s original seminal work <span class="citation" data-cites="Shannon1948"> [<a href="#ref-Shannon1948" role="doc-biblioref">27</a>]</span>. The schematic shows an abstract communication system with all the components. (B) Adaptation of the Shannon communication system to the context of bacterial gene expression regulated by an allosteric transcription factor.</figcaption>
</figure>
<p>Having this setup in mind, the question becomes: how do we mathematically define what information is? This brings a somewhat subtle difference between two related terms that many time are incorrectly used interchangeably: <em>Entropy</em> and <em>Information</em>. Information allows the entity that possesses it to make predictions with accuracy better than random, while entropy is a quantification of how much we do not know <span class="citation" data-cites="Adami2016"> [<a href="#ref-Adami2016" role="doc-biblioref">5</a>]</span>. From these definitions, we see that having information, therefore, reduces our uncertainty, i.e., reduces the entropy. This means that for Shannon, the amount of information we have from a source is related to that source’s statistical structure and how much we can predict the source’s message given our knowledge of this statistical structure. Let us look at a concrete example: English text. We know that written and spoken language is not completely random. For a message to be meaningful, the choice of words has to come from a statistical structure that obeys the language’s grammar rules. The choice of letters within a word also follows a certain statistical structure. Let us look at the text shown in Fig. 8(A). This is arguably one of the most important and most beautiful pieces of prose ever put together by a human mind as it is the last paragraph of <em>On the Origin of Species</em> by Darwin. If we ignore the paragraph’s message and just quantify how often we find each of the 26 letters in the English alphabet, we obtain a distribution like the one shown in Fig. 8(B). This paragraph shows that the most common vowel is <em>e</em>, exactly as in English writ-large. This distribution <span class="math inline">\(P(x)\)</span> is therefore not maximally random. In other words, if we were to put all letters in the paragraph in a hat and pick one letter at random, we could bet more money on the outcome being a letter <em>e</em> and make money over time given this knowledge of the structure of the distribution. A maximally random distribution would be if all letters appeared equally frequent in the paragraph, such that betting on any letter coming out of the hat would give us equal chances of guessing right. If instead of looking at the distribution of individual letters, we look at pairs of letters, the distribution <span class="math inline">\(P(x, y)\)</span> over the paragraph is shown in Fig. 8(C). Here we can see that just as the letters were not completely random, the pairs of letters are also not random. For example, if we take the first letter of the pair to be <em>t</em>, we see that it is more commonly followed by the letter <em>h</em>. This implies that knowing that the first letter of the pair was <em>t</em> reduced our uncertainty of what character could come next. We would then say that knowing the first letter gave us <em>information</em> about the possible outcomes of the second letter. In the next section, we will follow Shannon’s original derivation to define both entropy and information mathematically.</p>
<figure>
<img src="ch1_fig08.png" id="fig:ch1_fig08" data-short-caption="The statistical structure of the English language" alt="Figure 8: The statistical structure of the English language. (A) Last paragraph of On the Origin of Species by Charles Darwin. This serves as a rather nice not-random text example. (B) Marginal distribution P(x) of all 26 letters and space. The size of the squares is proportional to how often each letter appears in the paragraph. (C) Joint distribution of pairs of characters P(x, y). All pairs of characters in (A) were counted to build this histogram. The x-axis shows the first letter while the y-axis shows the second. For simplicity in (B) and (C) all punctuation was ignored. The Python code (ch1_fig08.py) used to generate this figure can be found on the thesis GitHub repository." /><figcaption aria-hidden="true">Figure 8: <strong>The statistical structure of the English language.</strong> (A) Last paragraph of <em>On the Origin of Species</em> by Charles Darwin. This serves as a rather nice not-random text example. (B) Marginal distribution <span class="math inline">\(P(x)\)</span> of all 26 letters and space. The size of the squares is proportional to how often each letter appears in the paragraph. (C) Joint distribution of pairs of characters <span class="math inline">\(P(x, y)\)</span>. All pairs of characters in (A) were counted to build this histogram. The x-axis shows the first letter while the y-axis shows the second. For simplicity in (B) and (C) all punctuation was ignored. The <a href="https://github.com/mrazomej/phd/blob/master/src/chapter_01/code/ch1_fig08.py">Python code (<code>ch1_fig08.py</code>)</a> used to generate this figure can be found on the thesis <a href="https://github.com/mrazomej/phd">GitHub repository</a>.</figcaption>
</figure>
<h3 id="choice-uncertainty-and-entropy">Choice, Uncertainty, and Entropy</h3>
<p>So far, our discussion about what entropy and information mean has been vague and not rigorous. To derive a formula to quantify these concepts, we need to get more mathematical. Let us assume that an information source (See Fig. 7(A)) produces elements of a message following a distribution <span class="math inline">\(\mathbf{p} = \{p_1, p_2, \ldots, p_n \}\)</span>, where each <span class="math inline">\(p_i\)</span> is the probability of the <span class="math inline">\(i^{\text{th}}\)</span> element. These elements could be letters, words, sentences, basepairs, concentrations of a small molecule, etc., of which we have <span class="math inline">\(n\)</span> possibilities. What we are looking for is a metric <span class="math inline">\(H(\mathbf{p})\)</span> that quantifies how much “choice” is involved in the selection of each element of the message. In other words, how uncertain we are about the message that the information source will produce at random? We demand our desired quantity <span class="math inline">\(H(\mathbf{p})\)</span> to satisfy three reasonable conditions <span class="citation" data-cites="Shannon1948"> [<a href="#ref-Shannon1948" role="doc-biblioref">27</a>]</span>:</p>
<ol type="1">
<li><p><span class="math inline">\(H\)</span> should be continuous in the <span class="math inline">\(p_i\)</span>s. Different information sources might have slightly different distributions <span class="math inline">\(\mathbf{p}\)</span>, nevertheless <span class="math inline">\(H\)</span> should still apply to all possible information sources.</p></li>
<li><p>If all of the elements of the distribution are equally likely, i.e., <span class="math inline">\(p_i = 1/n\)</span>, then <span class="math inline">\(H\)</span> should be a monotonic increasing function of <span class="math inline">\(n\)</span>. This means that the more options to choose from, the more uncertain we are about the possible outcome. For example, we are more uncertain about the outcome of a fair 6-sided die than of a fair coin just because of the number of possible outcomes from each of these “information sources.”</p></li>
<li><p>If the act of choosing one of the possible <span class="math inline">\(n\)</span> elements of our information source can be broken down into two successive choices, the original <span class="math inline">\(H\)</span> should be the weighted sum of the individual <span class="math inline">\(H\)</span>s. What this means is illustrated in Fig. 9(A) where we imagine having an information source with <span class="math inline">\(n=3\)</span> choices, each with probabilities <span class="math inline">\(\mathbf{p} = \{ 1/2, 1/3, 1/6\}\)</span>, which gives <span class="math inline">\(H(1/2, 1/3, 1/6)\)</span> for the left case. For the right case, we imagine first choosing between the upper and the lower path, and then, if the lower path is chosen, a second choice is made. This property then demands that <span class="math display">\[
 \overbrace{H(1/2, 1/3, 1/6)}^{\text{single choice}} = 
 \overbrace{H(1/2, 1/2)}^{\text{first choice}} +
 \overbrace{\frac{1}{2} H(1/3, 1/6)}^{\text{second choice}}.
\]</span> Another way to think about this property is that we want our metric of uncertainty <span class="math inline">\(H\)</span> to be <em>additive</em>.</p></li>
</ol>
<figure>
<img src="ch1_fig09.png" id="fig:ch1_fig09" data-short-caption="Shannon&#39;s theorem" alt="Figure 9: Shannon’s theorem. (A) One of the properties of a reasonable metric for uncertainty is that we can partition choices into multiple steps, and the resulting uncertainty should remain the same. (B) Example of coding functions E. The English alphabet can be converted into Morse code. Amino acids can be encoded in codons. (C) Partitioning of 2^3 equally likely choices into three decision steps, each with two choices. Eight different amino acids can be selected using two schemes: 1) each of the eight codons is chosen at random with equally likely chances, or 2) the codon is built by choosing one basepair at the time. (D) Partitioning of unequal choices. Given the redundancy of the genetic code, for equally likely codons, the resulting amino acid has different probabilities being chosen." /><figcaption aria-hidden="true">Figure 9: <strong>Shannon’s theorem.</strong> (A) One of the properties of a reasonable metric for uncertainty is that we can partition choices into multiple steps, and the resulting uncertainty should remain the same. (B) Example of coding functions <span class="math inline">\(E\)</span>. The English alphabet can be converted into Morse code. Amino acids can be encoded in codons. (C) Partitioning of <span class="math inline">\(2^3\)</span> equally likely choices into three decision steps, each with two choices. Eight different amino acids can be selected using two schemes: 1) each of the eight codons is chosen at random with equally likely chances, or 2) the codon is built by choosing one basepair at the time. (D) Partitioning of unequal choices. Given the redundancy of the genetic code, for equally likely codons, the resulting amino acid has different probabilities being chosen.</figcaption>
</figure>
<p>We will now prove that the only functional form that satisfies all these three properties is given by <span class="math display">\[
    H(\mathbf{p}) = - K \sum_{i=1}^n p_i \log p_i,
\]</span> where <span class="math inline">\(K\)</span> is a constant having to do with the units (choice of the logarithm base). To prove this, we will follow Shannon’s original work. We imagine the problem of encoding a message. For example, imagine encoding a message from the English alphabet into Morse code, or a protein sequence into the corresponding mRNA sequence, as schematically depicted in Fig. 9(B). In there, we take letters in the English alphabet (<em>SOS</em> for the English alphabet, <em>MGF</em> for the protein), run it through an encoding function <span class="math inline">\(E\)</span> and obtain the message (…- - -… for the Morse code, <em>AUGGGCUUC</em> for the mRNA). This process of encoding can be thought of as taking a message <span class="math inline">\(m_x\)</span> written in an alphabet <span class="math inline">\(\mathcal{X} = \{x_1, x_2, \ldots, x_n \}\)</span>, (where <span class="math inline">\(n\)</span> is 26 for the English alphabet, and 20 for the number of amino acids) and converting it into a message <span class="math inline">\(m_y\)</span> written in a different alphabet <span class="math inline">\(\mathcal{Y} = \{y_1, y_2, \ldots, y_m \}\)</span> (where <span class="math inline">\(m=2\)</span> for Morse code since we only have dots and dashes, and <span class="math inline">\(m=4\)</span> for the mRNA with 4 possible nucleotides). The encoding function <span class="math inline">\(E: \mathcal{X}^r \rightarrow \mathcal{Y}^t\)</span> takes a message of length <span class="math inline">\(r\)</span> (for our exmaples <span class="math inline">\(r=3\)</span>) and translates it into a message of size <span class="math inline">\(t\)</span> (in our examples <span class="math inline">\(t=9\)</span>) such that we then have <span class="math display">\[
m_y = E(m_x),
\]</span> Obviously, the larger the message <span class="math inline">\(m_x\)</span> we want to encode, the larger the corresponding message <span class="math inline">\(m_y\)</span> will be. Therefore we have that <span class="math display">\[
L(m_y) \propto L(m_x),
\label{eq:length_proportionality}
\]</span> where <span class="math inline">\(L(\cdot)\)</span> is a function that counts the number of characters in a message. An essential difference between both of the examples in Fig. 9(B) is that for the English to Morse code case, the number of dots and dashes for different letters is different (<em>e</em><span class="math inline">\(\rightarrow\)</span>., <em>x</em><span class="math inline">\(\rightarrow\)</span>-..-). Meanwhile, for the amino acid to codon case, every single codon has the same length. Let us focus for now on this second coding scheme where every character from alphabet <span class="math inline">\(\mathcal{X}\)</span> is encoded with the same number of characters from alphabet <span class="math inline">\(\mathcal{Y}\)</span>. We have then <span class="math inline">\(L(m_x) = r\)</span> and <span class="math inline">\(L(m_y) = t\)</span>. Let us call <span class="math inline">\(k\)</span> the proportionality constant from Eq. <span class="math inline">\(\ref{eq:length_proportionality}\)</span> such that <span class="math display">\[
L(m_y) = k L(m_x).
\label{eq:length_fn}
\]</span> The number of messages of size <span class="math inline">\(r\)</span> that can be encoded with the alphabet <span class="math inline">\(\mathcal{X}\)</span> is given by <span class="math inline">\(n^r\)</span>–because we have <span class="math inline">\(n\)</span> possible options to chose from for each of the <span class="math inline">\(r\)</span> characters, resulting in <span class="math inline">\(n\cdot n\cdot n\cdots = n^r\)</span>. Likewise, the number of messages of size <span class="math inline">\(t\)</span> encoded with alphabet <span class="math inline">\(\mathcal{Y}\)</span> is <span class="math inline">\(m^t\)</span>. We then demand from our coding scheme that the number of messages we can encode is at least the number of messages we could potentially send. In other words, for our coding scheme to be able to take <em>any</em> message of size <span class="math inline">\(r\)</span> it must be true that the number of possible encoded messages is at least as large as the number of possible messages to encode. This demand is expressed as <span class="math display">\[
n^r \leq m^t.
\]</span> If our encoding did not satisfy this, we would have to increase <span class="math inline">\(t\)</span>, i.e., the number of characters we use to encode our message. For example, if codons were made out of only two basepair, the genetic code would not be able to code for all 20 amino acids plus the stop codons. On the other extreme, we could develop a ridiculously long encoding scheme (imagine a version of the genetic code where 1000 basepair represented a single amino acid). To avoid this absurd scheme, we bound the encoded message’s size to be as long as necessary to encode all potential messages, but not any longer. This bound is expressed as <span class="math display">\[
m^{t-1} &lt; n^r \leq m^t.
\label{eq:ineq_messages}
\]</span> Let us now take the logarithm on our previous inequality–this preserves the inequalities since <span class="math inline">\(\log\)</span> is a monotonically increasing function–finding <span class="math display">\[
(t - 1) \log(m) &lt; r \log(n) \leq t \log(m).
\]</span> We are free to choose the logarithm base as we find convenient; therefore, let us use base <span class="math inline">\(m\)</span> for this, obtaining <span class="math display">\[
t - 1 &lt; r \log_m(n) \leq t.
\label{eq:ineq_logm}
\]</span> Dividing Eq. <span class="math inline">\(\ref{eq:ineq_logm}\)</span> by <span class="math inline">\(r\)</span> gives <span class="math display">\[
\frac{t-1}{r} &lt; \log_m(n) \leq \frac{t}{r}.
\label{eq:t_over_r}
\]</span> Let us stare at Eq. <span class="math inline">\(\ref{eq:t_over_r}\)</span>. In Eq. <span class="math inline">\(\ref{eq:ineq_messages}\)</span> We established <span class="math inline">\(t\)</span> as the minimum number of characters from alphabet <span class="math inline">\(\mathcal{Y}\)</span> needed to encode a message of length <span class="math inline">\(r\)</span> written with alphabet <span class="math inline">\(\mathcal{X}\)</span> characters (such as <em>MGF</em> turned into <em>AUGGGCUUC</em> as in Fig. 9(B)). This means that, for the case where all symbols use the same number of characters when encoded, <span class="math inline">\(t/r\)</span> is the number of characters from alphabet <span class="math inline">\(\mathcal{Y}\)</span> per character from alphabet <span class="math inline">\(\mathcal{X}\)</span>, i.e., the proportionality constant <span class="math inline">\(k\)</span> from Eq. <span class="math inline">\(\ref{eq:length_fn}\)</span>. This means that Eq. <span class="math inline">\(\ref{eq:t_over_r}\)</span> implies <span class="math display">\[
\log_m(n) \leq k.
\]</span> In other words, a lower bound for the number of characters from alphabet <span class="math inline">\(\mathcal{Y}\)</span> needed to encode a character from alphabet <span class="math inline">\(\mathcal{X}\)</span> is given by <span class="math inline">\(\log_m(n)\)</span>. For the amino acid to codon case, the minimum number of letters in a codon would be <span class="math inline">\(\log_4(20) \approx 2.16 &gt; 2\)</span>. This shows why we could not encode all 20 amino acids with two basepair long codons. Furthermore, Eq. <span class="math inline">\(\ref{eq:t_over_r}\)</span> implies that <span class="math display">\[
\frac{t}{r} - \log_m(n) &lt; \frac{t}{r} - \frac{(t-1)}{r},
\label{eq:t_over_r_diff}
\]</span> given that <span class="math inline">\((t-1)/r &lt; \log_m(n)\)</span>. Simplifying Eq. <span class="math inline">\(\ref{eq:t_over_r_diff}\)</span> results in <span class="math display">\[
\frac{t}{r} - \log_m(n) &lt; \frac{1}{r}
\Rightarrow k - \log_m(n) &lt; \frac{1}{r}.
\label{eq:logn_1_over_r}
\]</span> Therefore, we can make <span class="math inline">\(k\)</span>, the number of encoding characters, as arbitrarily close to <span class="math inline">\(\log_m(n)\)</span> as we want by increasing the length of the message being encoded, i.e., making <span class="math inline">\(r \rightarrow \infty\)</span>. This would imply a genetic code, not for individual amino acids but entire polypeptides. This scheme would not work biologically; nevertheless, this mathematical limit will help us find the functional form of our desired function <span class="math inline">\(H(\mathbf{p})\)</span>.</p>
<p>Coming back to the function <span class="math inline">\(H\)</span>, let us define <span class="math display">\[
A(n) \equiv H \left(\frac{1}{n}, \frac{1}{n}, \frac{1}{n}, \ldots\right),
\]</span> as the maximum possible value of <span class="math inline">\(H\)</span> when all outcomes are equally likely. Property 2 tells us that <span class="math inline">\(A(n)\)</span> increases monotonically with the length of the message. This means that if we apply the function <span class="math inline">\(A(\cdot)\)</span> to the terms in Eq. <span class="math inline">\(\ref{eq:ineq_messages}\)</span>, we conserve the inequality, i.e., <span class="math display">\[
A(m^{t-1}) &lt; A(n^r) \leq A(m^t).
\label{eq:A_ineq}
\]</span> Using Property 3, we can divide the <span class="math inline">\(n^r\)</span> possible choices into <span class="math inline">\(r\)</span> independent decisions, each with <span class="math inline">\(n\)</span> options to chose from. This property is depicted in Fig. 9(C). On the left, it shows we can choose from eight different codons that code for <span class="math inline">\(2^3 = 8\)</span> different amino acids. On the right, we can choose base by base, building up the codon in three consecutive decisions, each with two equally likely choices, for a total of <span class="math inline">\(2\cdot 2 \cdot 2 = 8\)</span> possible outcomes. This division of choices allows us to rewrite Eq. <span class="math inline">\(\ref{eq:A_ineq}\)</span> as <span class="math display">\[
(t - 1) A(m) &lt; r A(n) \leq t A(m),
\label{eq:A_div}
\]</span> because our requirement of the uncertainty <span class="math inline">\(H\)</span> being an additive property. For the example in Fig. 9(C), at each of the three decision steps, the uncertainty is given by <span class="math inline">\(A(2)\)</span>. Given that the uncertainty is additive, for each of the routes, our total uncertainty is given by <span class="math display">\[
A(2) + A(2) + A(2) = 3 A(2),
\]</span> therefore <span class="math inline">\(A(2^3) = 3 A(2)\)</span>. Dividing Eq. <span class="math inline">\(\ref{eq:A_div}\)</span> by <span class="math inline">\(r\)</span> results in <span class="math display">\[
\frac{(t - 1)}{r} A(m) &lt; A(n) \leq \frac{t}{r} A(m).
\]</span> Since <span class="math inline">\(\frac{(t - 1)}{r} A(m) &lt; A(n)\)</span>, it is also true that <span class="math display">\[
\frac{t}{r} A(m) - A(n) &lt; A(m) \left(\frac{t}{r} - \frac{(t-1)}{r} \right).
\]</span> Simplifying terms we are left with <span class="math display">\[
\frac{t}{r} A(m) - A(n) &lt; \frac{1}{r} A(m).
\]</span> Dividing both sides by <span class="math inline">\(A(m)\)</span> we find <span class="math display">\[
k - \frac{A(n)}{A(m)} &lt; \frac{1}{r}.
\label{eq:A_1_over_r}
\]</span> We can make the ratio <span class="math inline">\(A(n)/A(m)\)</span> as close as <span class="math inline">\(k\)</span> as we want by making <span class="math inline">\(r\)</span> larger. This equation looks shockingly similar to Eq. <span class="math inline">\(\ref{eq:logn_1_over_r}\)</span>, but what is the connection? On the one hand Eq. <span class="math inline">\(\ref{eq:logn_1_over_r}\)</span> is the result of imposing the condition that our coding scheme must be able to encode any possible message from one alphabet <span class="math inline">\(\mathcal{X}\)</span> to another alphabet <span class="math inline">\(\mathcal{Y}\)</span>. This condition leads us to the conclusion that the number of characters from alphabet <span class="math inline">\(\mathcal{Y}\)</span> needed to encode the characters from alphabet <span class="math inline">\(\mathcal{X}\)</span> (the constant <span class="math inline">\(k\)</span>) can be made as arbitrarily close to <span class="math inline">\(\log_m(n)\)</span> as we want by writing a code, not for individual characters (individual amino acids), but for sequences of characters (polypeptides). On the other hand Eq. <span class="math inline">\(\ref{eq:A_1_over_r}\)</span> is a direct consequence of the three logical properties we imposed on our uncertainty metric <span class="math inline">\(H\)</span>. These properties led us to conclude that, whatever our uncertainty function for the equally likely choices <span class="math inline">\(A(\cdot)\)</span> is, the ratio of the uncertainties for each of our two alphabets <span class="math inline">\(A(n)/A(m)\)</span> approaches the same constant <span class="math inline">\(k\)</span> as we make the encoded message longer. Since both <span class="math inline">\(\log_m(n)\)</span> and <span class="math inline">\(A(n)/A(m)\)</span> approach <span class="math inline">\(k\)</span> as <span class="math inline">\(r\)</span> grows, we can conclude that <span class="math display">\[
\frac{A(n)}{A(m)} \rightarrow \frac{\log_m(n)}{\log_m(m)}\; \text{ as }
r \rightarrow \infty.
\]</span> We wrote the ratio <span class="math inline">\(\log_m(n)/\log_m(m)\)</span> because our choice of the logarithm base was arbitrary. Therefore, more generally, we have <span class="math display">\[
\frac{A(n)}{A(m)} \rightarrow \frac{\log(n)}{\log(m)}\; \text{ as }
r \rightarrow \infty,
\]</span> for any base. This convergence only takes place if and only if <span class="math display">\[
A(n) = K \log(n),
\]</span> where <span class="math inline">\(K\)</span> is some constant. This is quite beautiful. What we just demonstrated is that the functional form for the uncertainty metric we are after scales as the logarithm of the number of possible characters in our alphabet. We know that our uncertainty function <span class="math inline">\(H(1/n, 1/n, \ldots)\)</span> is a function of <span class="math inline">\(1/n\)</span> rather than of <span class="math inline">\(n\)</span>. This is easily fixed by using the properties of logarithms, writing <span class="math display">\[
H\left(\frac{1}{n}, \frac{1}{n}, \ldots \right) = 
-K \log\left(\frac{1}{n} \right).
\label{eq:entropy_equally}
\]</span> The general form of Shannon’s entropy is starting to show up. After all, for the case where all choices are equally likely, we have <span class="math inline">\(p_i = 1/n\)</span>. We can therefore write <span class="math display">\[
H\left(\frac{1}{n}, \frac{1}{n}, \ldots \right) = -K 
\sum_{i=1}^n \frac{1}{n} \log\left(\frac{1}{n} \right).
\]</span></p>
<p>Let us generalize the proof for cases where choices are not equally likely. To continue with the amino acid to codon encoding example, we now consider the genetic code’s redundancy. Given that there are <span class="math inline">\(4^3 = 64\)</span> possible codons, multiple codons map to the same amino acid. An example of three amino acids that share the first letter is depicted on Fig. 9(D). The diagram on the left shows a total of nine different codons; two of such codons code for asparagine (<em>N</em>), three for isoleucine (<em>I</em>), and four for threonine (<em>T</em>). A way to express the asymmetry between the choices is to have each codon as an independent and equally likely choice, as depicted on the middle diagram of Fig. 9(D). Let us define the total number of codons <span class="math display">\[
N = \sum_{i=1}^n n_i,
\label{eq:sum_amino}
\]</span> where <span class="math inline">\(n_i\)</span> counts the number of codons for amino acid <span class="math inline">\(i\)</span>, and <span class="math inline">\(n\)</span> is the total number of amino acid choices. Let us call <span class="math inline">\(H_1\)</span> the uncertainty of this set of equal choices. From Eq. <span class="math inline">\(\ref{eq:entropy_equally}\)</span> we know that the resulting uncertainty function <span class="math inline">\(H_1\)</span> is of the form <span class="math display">\[
H_1 = K \log\left(\sum_{i=1}^n n_i \right) = K \log (N),
\]</span> since all codons are equally likely.</p>
<p>Although each codon is equally likely, the resulting amino acid is not. The probability of amino acid <em>I</em> in this case is the number of codons encoding it (two) divided by the total number of codons in the example (nine). In general, we assume that each of the <span class="math inline">\(n\)</span> choices has a probability <span class="math display">\[
p_i = \frac{\text{\# codons for amino acid }i}{\text{total \# of codons}} =
\frac{n_i}{N}.
\label{eq:p_i_amino}
\]</span> By Property 3 of our function <span class="math inline">\(H\)</span>, we can partition the codon’s choice into two consecutive decisions (not three since the first codon is the same for all amino acids in this example). This partitioning is shown on the right diagram of Fig. 9(D). The uncertainty <span class="math inline">\(H_2\)</span> for this case has two contributions, one for each of the decisions <span class="math display">\[
H_2 = 
\overbrace{H(p_1, p_2, \ldots, p_n)}^{\text{first choice}} + 
\overbrace{K \sum_{i=1}^n p_i \log n_i}^{\text{second choice}}.
\]</span> The first decision has an unknown functional form we are trying to figure out. The second choice consists of choosing between <span class="math inline">\(n_i\)</span> equally likely bases for the codon’s last position, each weighted by the probability of going to this particular branch (the one that defines the amino acid) as demanded by Property 3. But whether or not we choose each codon on a single decision or in two steps, the uncertainty of this event is the same. This means that <span class="math inline">\(H_1 = H_2\)</span> as Property 3 requires. This equality results in <span class="math display">\[
K \log (N) = H(p_1, p_2, \ldots, p_n) + K \sum_{i=1}^n p_i \log(n_i).
\]</span> Solving for <span class="math inline">\(H(p_1, p_2, \ldots, p_n)\)</span> results in <span class="math display">\[
H(p_1, p_2, \ldots, p_n) = K \left[ 
    \log N - \sum_{i=1}^n p_i \log(n_i)
\right].
\]</span> Using Eq. <span class="math inline">\(\ref{eq:sum_amino}\)</span> results in <span class="math display">\[
H(p_1, p_2, \ldots, p_n) = - K \left[ 
    \sum_{i=1}^n p_i \log(n_i)
    - \log\left( \sum_{i=1}^n n_i \right)
\right].
\]</span> Since probabilities must be normalized, i.e., <span class="math inline">\(\sum_{i=1}^n p_i = 1\)</span>, we can write <span class="math display">\[
H(p_1, p_2, \ldots, p_n) = - K \left[ 
    \sum_{i=1}^n p_i \log(n_i)
    - \sum_{i=1}^n p_i \log\left( \sum_{i=1}^n n_i \right)
\right].
\]</span> Using the property of logarithms, we can rewrite this as <span class="math display">\[
H(p_1, p_2, \ldots, p_n) = - K \left[ 
    \sum_{i=1}^n p_i 
    \log\left( \frac{n_i}{\sum_{i=1}^n n_i} \right)
\right].
\]</span> Using Eq. <span class="math inline">\(\ref{eq:p_i_amino}\)</span> we find the expected result <span class="math display">\[
H(p_1, p_2, \ldots, p_n) = - K \sum_{i=1}^n p_i \log p_i.
\label{eq:shannon_result}
\]</span></p>
<p>Let us dissect this result. We began this derivation by stating three logical properties that a metric for uncertainty should have. The properties could be summarized simply as 1) the function exists for all possible <span class="math inline">\(p_i\)</span>s, 2) the uncertainty grows as the number of possible outcomes grows, and 3) the uncertainty must be additive. We thought about a coding scheme to encode a message written in an alphabet into a different one. We demanded that our coding scheme should be able to encode <em>any</em> message we want, and this led us to conclude that the average number of characters needed to encode each character on the original message can approach <span class="math inline">\(\log_m(n)\)</span>, where <span class="math inline">\(n\)</span> is the number of characters in the original alphabet and <span class="math inline">\(m\)</span> is the number of characters in the encoding alphabet. We then used the properties of our desired uncertainty function and found a non-obvious connection between the number of characters needed to pass from one alphabet to another and the uncertainty on the message. When we generalized this analysis to cases where not all outcomes are equally likely, we arrived at Eq. <span class="math inline">\(\ref{eq:shannon_result}\)</span>, the so-called Shannon entropy. This is Shannon’s theorem, and what it shows is that Eq. <span class="math inline">\(\ref{eq:shannon_result}\)</span> is the only function that satisfies the three very reasonable conditions we established for an uncertainty measurement.</p>
<p>To gain intuition on what this equation is telling us, let us look at two examples. In our first example, we will think about the simplest random process: a coin toss. To compute how unpredictable the outcome of our simple coin toss is, we can use Eq. <span class="math inline">\(\ref{eq:shannon_result}\)</span>. For this particular case, we only have two possible outcomes–heads with probability <span class="math inline">\(p\)</span> or tails with probability <span class="math inline">\(1 - p\)</span>. The resulting entropy is of the form <span class="math display">\[
H = - p \log(p) - (1 - p) \log(1 - p).
\label{eq:entropy_coin}
\]</span> Fig. 10(A) plots Eq. <span class="math inline">\(\ref{eq:entropy_coin}\)</span> as a function of the probability of heads <span class="math inline">\(p\)</span>. Notice that the curve is concave with a minimum at <span class="math inline">\(p=0\)</span> and <span class="math inline">\(p=1\)</span> and a maximum at <span class="math inline">\(p=1/2\)</span>. This shape should make intuitive sense given that Eq. <span class="math inline">\(\ref{eq:entropy_coin}\)</span> quantifies how unpredictable the outcome of tossing the coin is. If the coin toss’s outcome is always heads (p=1) or always tails (p=0), there is no uncertainty about the resulting face. The more both outcomes become (the closer <span class="math inline">\(p\)</span> gets to <span class="math inline">\(1/2\)</span>), the more unpredictable the random even is. One mathematical subtly here is that for <span class="math inline">\(p=1\)</span> or <span class="math inline">\(p=0\)</span> we have to compute <span class="math inline">\(0 \times \log(0)\)</span>, which is undefined. We take this to be zero because for <span class="math inline">\(x \log(x)\)</span>, the limit where <span class="math inline">\(x \rightarrow 0^+\)</span> converges to zero. Notice that the units on the <span class="math inline">\(y\)</span>-axis are given in bits. These units mean that we used base two for our logarithms. An easy way to think about what a bit means is as the number of <em>yes</em>/<em>no</em> questions one would need to ask on average to infer the random event’s outcome. For a coin, all we need is a single question (therefore one bit) to know what the outcome was.</p>
<p>For our second example, we go back to the mRNA steady-state distribution we derived in Eq. <span class="math inline">\(\ref{eq:mRNA_steady}\)</span>. We found that for our simple one-state DNA promoter, the steady-state distribution resulted Poisson with mean <span class="math inline">\(\langle m \rangle = r_m / \gamma_m\)</span>. Fig. 10(B) shows the entropy of this Poisson distribution as a function of the mean mRNA. WE se a quick initial increase in this entropy up to <span class="math inline">\(\langle m \rangle \approx 20\)</span>, after which there is a much less steep increment. Imagine we sample a random cell from one of these Poisson distributions. Using the interpretation of bits again as the number of <em>yes/no</em> questions, what Fig. 10(B) tells us is that if the promoter produces <span class="math inline">\(\approx 10\)</span> mRNA on average, it will take on average 3.5 of these questions to infer the number of mRNA for random cell. For an average of <span class="math inline">\(\approx 20\)</span> mRNA it would take four questions, and for an average of <span class="math inline">\(\approx 60\)</span> mRNA five questions. These questions would be of the form “<em>is it greater than the average?</em>” or “<em>is it less than or equal to 1/3 of the average?</em>” and so on.</p>
<figure>
<img src="ch1_fig10.png" id="fig:ch1_fig10" data-short-caption="Shannon entropy in action" alt="Figure 10: Shannon entropy in action. (A) The entropy of a coin as a function of the probability of heads p. The entropy is maximum when the coin is fair, i.e., p=0.5, meaning that this is the most unpredictable coin one could have. (B) The entropy of the steady-state mRNA distribution as derived in Eq. \ref{eq:mRNA_steady} as a function of the mean mRNA copy number. The point shows the entropy of the distribution shown in the inset. Bot figures use base 2 for the logarithm, resulting in units of bits for the entropy. The Python code (ch1_fig10.py) used to generate this figure can be found on the thesis GitHub repository." /><figcaption aria-hidden="true">Figure 10: <strong>Shannon entropy in action.</strong> (A) The entropy of a coin as a function of the probability of heads <span class="math inline">\(p\)</span>. The entropy is maximum when the coin is fair, i.e., <span class="math inline">\(p=0.5\)</span>, meaning that this is the most unpredictable coin one could have. (B) The entropy of the steady-state mRNA distribution as derived in Eq. <span class="math inline">\(\ref{eq:mRNA_steady}\)</span> as a function of the mean mRNA copy number. The point shows the entropy of the distribution shown in the inset. Bot figures use base 2 for the logarithm, resulting in units of bits for the entropy. The <a href="https://github.com/mrazomej/phd/blob/master/src/chapter_01/code/ch1_fig10.py">Python code (<code>ch1_fig10.py</code>)</a> used to generate this figure can be found on the thesis <a href="https://github.com/mrazomej/phd">GitHub repository</a>.</figcaption>
</figure>
<h3 id="information-theory-and-statistical-mechanics">Information Theory and Statistical Mechanics</h3>
<p>Our result in Eq. <span class="math inline">\(\ref{eq:shannon_result}\)</span> is of the same functional form as the thermodynamic entropy. The story goes that Shannon was discussing this concept with his friend John von Neumann. It was von Neumann who allegedly convinced Shannon of calling his metric of randomness <em>entropy</em> under the argument that nobody understands the concept. But the fact that the functional forms are the same is too suggestive to dismiss a potential connection between these concepts immediately. It was until much later that E. T. Jaynes formalized ways to link both ideas <span class="citation" data-cites="Jaynes1957"> [<a href="#ref-Jaynes1957" role="doc-biblioref">28</a>]</span>. Nevertheless, Jaynes himself strongly discourages people from trying to map one concept to the other explicitly. In his book “<em>Probability Theory: The Logic of Science</em>” Jaynes warns the reader about failing to distinguish information entropy, which is a property of the mathematical object we call a probability distribution, and the <em>experimental entropy</em> of thermodynamics, which is instead a property of the state of the system as defined by experimentally measurable quantities such as volume, temperature, pressure, magnetization, etc. Jaynes goes on to say: “<em>they should never have been called by the same name; the experimental entropy makes no reference to any probability distribution, and the information entropy makes no reference o thermodynamics</em>” <span class="citation" data-cites="Jaynes2003"> [<a href="#ref-Jaynes2003" role="doc-biblioref">29</a>]</span>.</p>
<p>When Jaynes makes such strong remarks about the disconnection between both entropy concepts, he strictly refers to the classical thermodynamic definition. This classical definition of entropy, due to Clausius, refers to the inability of any thermal engine to convert all of the input energy into useful work. Clausius defined a new quantity <span class="math inline">\(S\)</span> as the amount of energy per unit temperature unavailable to do work. To understand this idea is to realize that from the energy liberated in gasoline combustion on a car engine, we only end up extracting <span class="math inline">\(\approx 20\%\)</span> of the energy to move the car. The other <span class="math inline">\(80\%\)</span> is lost into heating the engine and the environment. But this is not because the engineers are using poor designs. The second law of thermodynamics on its classical definition states that nothing in the universe can convert <span class="math inline">\(100\%\)</span> of the energy into useful work; there will always be residual energy that gets turned into heat.</p>
<p>At the time, the existence of atoms was not widely accepted by the scientific community. But then came Boltzmann and the statistical mechanics’ conceptual revolution. The giant leap in our understanding of why the second law of thermodynamics does not allow the total conversion of energy into useful work came with Boltzmann’s revolutionary entropy idea. Boltzmann hypothesized that matter was made out of atoms. Therefore, everything we can observe and measure macroscopically about any system results from the microscopic configuration of all the atoms that make up the system. Furthermore, many microscopic arrangements are indistinguishable at our macroscopic scale (recall the microstate and macrostate concept in Fig. 2). This line of reasoning led Boltzmann to the law we stated in Eq. <span class="math inline">\(\ref{eq:boltzmann_law}\)</span>. This law and all of the classic results from statistical mechanics are founded on several assumptions about the microscopic scale processes’ reversibility. In other words, for Boltzmann’s law to be “a legit law of nature,” it must be the case that if we play a movie featuring a single atom moving around the system, the same movie played in reverse should be as equally likely to happen.</p>
<p>But it might be the case that the assumptions underlying statistical mechanics laws are not the most fundamental constructs of reality. As we will show next, we can derive a classic result of statistical mechanics from a completely different premise having to do more with statistical inference rather than physical laws of motion governing atoms. This becomes a circular argument where some physicists have the laws of motion as the defining foundation on which to base statistical mechanics laws is better. For others, having an information-theoretic justification for statistical mechanics independent of the underlying physical laws is more appealing. At the end of the day is a matter of taste. Having said all of this, let us delve into the connection between information-theoretic entropy and the Boltzmann distribution.</p>
<p>We already used the Boltzmann distribution when we computed the probability of an RNAP molecule being bound to the promoter <span class="math inline">\(p_{\text{bound}}\)</span>. The Boltzmann distribution applies to systems in thermodynamic equilibrium in contact with a heat bath at a constant temperature. Think of a small Eppendorf tube (<span class="math inline">\(\approx 2\)</span> mL) that we perfectly seal before submerging it into the ocean. The tube’s temperature will equilibrate with that of the ocean, but the ocean’s temperature will not be affected by the tube’s presence. Submerging the tube into the reservoir allows the total energy of the tube not to be fixed. Sometimes the tube can borrow energy from the ocean; sometimes, it can give energy to it. The Boltzmann distribution precisely dictates the likelihood of such energy states. The probability of a state with energy <span class="math inline">\(E_i\)</span> is given by <span class="math display">\[
P(E_i) = \frac{e^{-\beta E_i}}{\mathcal{Z}},
\]</span> where, as before, <span class="math inline">\(\beta \equiv (k_BT)^{-1}\)</span>. <span class="math inline">\(\mathcal{Z}\)</span> is the partition function defined by the sum of the Boltzmann weight for all possible microstates, i.e., <span class="math display">\[
\mathcal{Z} \equiv \sum_{\text{states}} e^{-\beta E_i},
\]</span> where the sum is taken over all microstates available to the system. This equation is equivalent to Eq. <span class="math inline">\(\ref{eq:pbound_unreg}\)</span> and Eq. <span class="math inline">\(\ref{eq:pbound_reg}\)</span>. We can derive this functional form from the so-called maximum entropy principle. This framework is expanded more in Chapter 5 of this thesis. But for our purposes here, the idea is that we are trying to make a “best guess” of what a distribution looks like, given limited information. For our Eppendorf tube inside the ocean, we are thinking about the distribution of all of the molecules’ microstates inside the tube. Experimentally, we never get to observe any of the microstates of the system. But we know that the probability of each microstate depends on its energy, as Boltzmann told us. Let us say we can measure the average energy <span class="math inline">\(\langle E \rangle\)</span> of our little Eppendorf tube. What is then the optimal guess of the functional form of the distribution that does not use any information we do not have at hand? For example, we cannot say that there is only one microstate available to the system with energy <span class="math inline">\(\langle E \rangle\)</span>, because that constrains the possibilities of the system, and measuring the average energy does not lead to such a conclusion. The next best case we can do is to maximize the Shannon entropy, subject to this constraint on the average energy. This makes sense because, as we derived in the previous section, the Shannon entropy is the only functional form that satisfies our properties for a metric of uncertainty. Maximizing the Shannon entropy leads then to a maximally uninformative distribution. Including the constraints when implementing this maximization guarantees that we use all that we know about the distribution and nothing else.</p>
<p>Given Property 1 of our function <span class="math inline">\(H\)</span>, the Shannon entropy is continuous on the individual probabilities’ values <span class="math inline">\(p_i\)</span>. This means that we can maximize the Shannon entropy by taking its derivative with respect to <span class="math inline">\(p_i\)</span> and equating it to zero. This operation does not include the constraints we have on the values of the probabilities of each microstate. Let us say that each microstate available to the system with energy <span class="math inline">\(E_i\)</span> has a probability <span class="math inline">\(p_i\)</span> of happening. The constraint on the average energy is given by <span class="math display">\[
\langle E \rangle = \sum_{\text{states}} E_i p_i,
\]</span> where again, the sum is taken over all possible microstates. Furthermore, we know that the probability distribution must be normalized. This means that <span class="math display">\[
\sum_{\text{states}} p_i = 1.
\]</span> To include these constraints in our optimization, we can use the Lagrange multipliers technique. We refer the reader to any introductory text on multivariate calculus for a quick refresher of this technique. We proceed by defining a Lagrangian <span class="math inline">\(\mathcal{L}\)</span> of the form <span class="math display">\[
\mathcal{L}(p_1, p_2,\ldots, p_N, \beta, \mu) =
\overbrace{- \sum_{i=1}^N p_i \log (p_i)}^{\text{Shannon entropy}} -
\overbrace{\beta \left(\sum_{i=1}^N E_i p_i - \langle E \rangle \right)}^
{\text{average energy constraint}} -
\overbrace{\mu \left(\sum_{i=1}^N  p_i - 1 \right)}^
{\text{normalization constraint}},
\]</span> where <span class="math inline">\(N\)</span> is the total number of microstates available to the system, and <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\mu\)</span> are the Lagrange multipliers associated with each of the constraints. The next step consists on computing the gradient of this Lagrangian which returns a vector of size <span class="math inline">\(N\)</span> where the <span class="math inline">\(k^{\text{th}}\)</span> entry is the derivative of the Lagrangian with respect to <span class="math inline">\(p_k\)</span>. But notice that all of these derivatives will look the same. So taking one of these derivatives is enough. We then take the derivative with respect to a particular <span class="math inline">\(p_k\)</span> and equate it to zero, obtaining <span class="math display">\[
\frac{d\mathcal{L}}{d p_k} = -\log(p_k) - 1 - \lambda - \beta E_k = 0.
\]</span> Notice that all of the terms with <span class="math inline">\(i\neq k\)</span> disappear, leaving a simple expression. Solving for <span class="math inline">\(p_k\)</span> gives <span class="math display">\[
p_k = \exp\left[1 - \lambda - E_k \right] = e^{1 - \lambda} e^{-\beta E_k}.
\]</span> Every single probability <span class="math inline">\(p_k\)</span> takes the same form. We substitute this probability <span class="math inline">\(p_k\)</span> on our normalization constraint, obtaining <span class="math display">\[
\sum_{i=1}^N p_i = e^{1 - \lambda} \sum_{i=1}^N e^{-\beta E_i}=1.
\]</span> This tells us that the term <span class="math inline">\(e^{1 - \lambda}\)</span> is given by <span class="math display">\[
e^{1 - \lambda} = \frac{1}{\sum_{i=1}^Ne^{-\beta E_i}}.
\]</span> Therefore, the probability of microstate <span class="math inline">\(i\)</span> is given by <span class="math display">\[
P(E_i) = p_i = \frac{e^{-\beta E_i}}{\sum_{i=1}^N e^{-\beta E_i}},
\]</span> exactly the Boltzmann distribution. One can show why it is the case that our Lagrange multiplier <span class="math inline">\(\beta\)</span> is exactly <span class="math inline">\(1/k_BT\)</span> as demanded by the thermodynamic version of this distribution, but that is out of the scope for our purposes. This section aims only to show the subtle and deep connection between statistical mechanics and information theory. This connection suggests that part of the unreasonable effectiveness of statistical mechanics might not come from the physical basis of its core theory; but instead from the statistical inference problem on which, given the limited information we have of any thermodynamic system’s microstate, entropy maximization gives us a recipe on what the best guess for the probability distribution over the microstates is.</p>
<h3 id="joint-uncertainty-in-an-uncertain-world">Joint Uncertainty in an Uncertain World</h3>
<p>Part of the complexity in understanding biological systems is that their components form a network of interactions. This connectivity means that one part of the organism’s state depends on many other parts’ states. For example, the wild-type <em>lac</em> operon’s expression depends on the conformation state of two transcription factors: CRP and LacI. The state of these transcription factors depends on the concentration of cyclic-AMP and allolactose, respectively. These concentrations rely on the state of the environment and transporters’ availability to bring them into the cell. This chain of connections continues indefinitely.</p>
<p>The mathematical language to express the dependence between two variables is that of joint and conditional probability. Shannon’s entropy (Eq. <span class="math inline">\(\ref{eq:shannon_result}\)</span>) can also be extended to account for dependence between variables. To make the notation for this extension easier to follow, let us use a different notation from now on. Let us express Shannon’s entropy as <span class="math display">\[
H(m) = -\sum_m P(m) \log P(m),
\label{eq:shannon_x}
\]</span> where instead of giving a vector of probabilities <span class="math inline">\(\mathbf{p}\)</span> to the function <span class="math inline">\(H\)</span>, we now give it a random variable <span class="math inline">\(m\)</span>. This notation is understood as: the entropy is calculated over the distribution of possible values that <span class="math inline">\(m\)</span> can take. If <span class="math inline">\(m\)</span> can take values <span class="math inline">\(\{m_1, m_2, \ldots, m_n\}\)</span>, the probability of obtaining <span class="math inline">\(m = m_k\)</span> is given by the function <span class="math inline">\(P(m=m_k)\)</span>, which for brevity we can write simply as <span class="math inline">\(P(m_k)\)</span>. What Eq. <span class="math inline">\(\ref{eq:shannon_x}\)</span> is saying is: Take the random variable <span class="math inline">\(m\)</span> and all the possible values it can have; compute the Shannon entropy by summing over the probability of all those values. In this way, <span class="math inline">\(H(m)\)</span> is a shorthand for writing <span class="math inline">\(H[P(m)]\)</span>.</p>
<p>With this notation in hand, let’s think about two correlated random variables <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span>. These could be the number of mRNAs and proteins in the cells, as depicted in Fig. 11(A). The <em>joint entropy</em> <span class="math inline">\(H(m, p)\)</span> measures the uncertainty we have about the outcome of a pair of variables rather than a single. All it takes is to sum over both variables on Eq. <span class="math inline">\(\ref{eq:shannon_x}\)</span> as <span class="math display">\[
H(m, p) = -\sum_m \sum_p P(m, p) \log P(m, p).
\label{eq:joint_entropy}
\]</span> Eq. <span class="math inline">\(\ref{eq:joint_entropy}\)</span> then does the same computation as Eq. <span class="math inline">\(\ref{eq:shannon_x}\)</span>, except that the sum is taken over all possible pairs of random variables <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span>. But what if we get to observe the outcome of one of the two variables (observing mRNA via RNA-seq, for example), can that tell us something about the outcome of the other one? For this, we need to understand the concept of conditional entropy.</p>
<figure>
<img src="ch1_fig11.png" id="fig:ch1_fig11" data-short-caption="Shannon&#39;s entropy for more than one random variable" alt="Figure 11: Shannon’s entropy for more than one random variable. (A) Toy model of a random process where mRNA (random variable m) is stochastically produced as a Poisson process with a fixed mean. Proteins (random variable p) are also stochastically produced as a Poisson process, but the mean depends on the number of mRNAs. (B) Samples from the model presented in (A). The center plot shows the joint distribution P(m, p), while the edge histograms show the marginal distributions P(m) and P(p). (C) Venn diagram of the relationship of different information metrics. The Python code (ch1_fig11.py) used to generate this figure can be found on the thesis GitHub repository." /><figcaption aria-hidden="true">Figure 11: <strong>Shannon’s entropy for more than one random variable.</strong> (A) Toy model of a random process where mRNA (random variable <span class="math inline">\(m\)</span>) is stochastically produced as a Poisson process with a fixed mean. Proteins (random variable <span class="math inline">\(p\)</span>) are also stochastically produced as a Poisson process, but the mean depends on the number of mRNAs. (B) Samples from the model presented in (A). The center plot shows the joint distribution <span class="math inline">\(P(m, p)\)</span>, while the edge histograms show the marginal distributions <span class="math inline">\(P(m)\)</span> and <span class="math inline">\(P(p)\)</span>. (C) Venn diagram of the relationship of different information metrics. The <a href="https://github.com/mrazomej/phd/blob/master/src/chapter_01/code/ch1_fig11.py">Python code (<code>ch1_fig11.py</code>)</a> used to generate this figure can be found on the thesis <a href="https://github.com/mrazomej/phd">GitHub repository</a>.</figcaption>
</figure>
<h3 id="thinking-conditionally-a-condition-for-thinking.">Thinking Conditionally, a Condition for Thinking.</h3>
<p>In Joe Blitztein’s excellent <em>Introduction to probability</em> <span class="citation" data-cites="Blitzstein2019"> [<a href="#ref-Blitzstein2019" role="doc-biblioref">30</a>]</span>, he clarifies how conditional probability is one of the most powerful concepts in probability theory. Through the concept of conditional probability, we can learn whether or not two things are somehow correlated, allowing us from there to dissect the nature of such correlation. Given the probabilistic nature of Shannon’s entropy, the power of conditional entropy is extended to the so-called conditional entropy <span class="math inline">\(H(p \mid m)\)</span>. Let us think of our two random variables <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> with a joint probability distribution <span class="math inline">\(P(m, p)\)</span>. We can assume that the outcome of both random variables is correlated for our mRNA-protein pair, meaning that specific pairs of values are more likely to appear. If we observed the outcome of one of the random variables and knew the correlation function between random variables, our guess for the variable’s value that we did not observe would improve over a completely random choice. In our example, if we get to observe that <span class="math inline">\(m\)</span> is a small (or large) number, we would suspect that <span class="math inline">\(p\)</span> is also a small (or large) number, as shown in Fig. 11(B). This means that our uncertainty on the value of <span class="math inline">\(p\)</span> changed–it was reduced–upon observing the value of <span class="math inline">\(m\)</span>. The new uncertainty, i.e., the entropy of <span class="math inline">\(p\)</span> having learned the value of <span class="math inline">\(m\)</span>, averaged over all possible values of <span class="math inline">\(m\)</span>, is computed as <span class="math display">\[
H(p \mid m) = - \sum_m \sum_p P(m) P(p \mid m) \log P(p \mid m),
\label{eq:conditional_entropy}
\]</span> where <span class="math inline">\(P(p \mid m)\)</span> is read as “probability of <span class="math inline">\(p\)</span> given that we observe <span class="math inline">\(m\)</span>.” Finally, with all these concepts in hand, we can discuss the idea of information in the Shannon sense.</p>
<h3 id="one-persons-entropy-is-another-persons-information.">One person’s entropy is another person’s information.</h3>
<p>So far, our discussion has focused on the concept of entropy. We first derived the Shannon entropy from three basic principles that a metric of uncertainty should satisfy. Then, we showed that one of the main statistical mechanics results, i.e., the Boltzmann distribution, could be derived from maximizing this entropy subject to certain constraints, suggesting that statistical mechanics could be nothing more than an optimal statistical inference protocol, given limited information. But no mention of information up to now. This intentional omission is because we first needed to master the idea of entropy to understand the mathematical definition of information.</p>
<p>Recall that <span class="math inline">\(H(p)\)</span> quantifies the uncertainty about the outcome of the random process that generates the value of the variable <span class="math inline">\(p\)</span>. Furthermore, <span class="math inline">\(H(p \mid m)\)</span> quantifies the uncertainty about the outcome of the same variable, but this time observing the outcome of the random variable <span class="math inline">\(m\)</span>. In the worst-case scenario, <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> are uncorrelated, and learning the value of <span class="math inline">\(m\)</span> does not tell us anything about <span class="math inline">\(p\)</span>. In that case, we then have that <span class="math display">\[
 H(p \mid m) = H(p)\;\; \text{ for $m$ and $p$ uncorrelated}.
\]</span> If <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> are correlated, as depicted in Fig. 11(B), then the uncertainty about <span class="math inline">\(p\)</span> is reduced upon learning the value of <span class="math inline">\(m\)</span>, giving us a general relationship between marginal and conditional entropy of the form <span class="math display">\[
H(p) \geq H(p \mid m).
\]</span> In this latter scenario, learning the value of <span class="math inline">\(m\)</span> reduced our uncertainty in the possible value of <span class="math inline">\(p\)</span>. This reduction in uncertainty agrees with an informal definition of what “obtaining information” means. We can then define the mutual information <span class="math inline">\(I(m;p)\)</span> between random variable <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> as the reduction in uncertainty about the value of one of the random variables when we learn the value of the other random variable. For our example in which we get to observe the mRNA copy number, this would mean that the mutual information is computed as <span class="math display">\[
I(m; p) \equiv H(p) - H(p \mid m).
\label{eq:mutual_info_entropy}
\]</span> But the mutual information is symmetric, meaning that the information about the outcome of one of the variables by observing the other variables is the same when the roles of what we get to observe are inverted. This argument means that we can mathematically show that <span class="math display">\[
I(m; p) = H(m) - H(m \mid p).
\]</span> This symmetry is why traditionally, the mutual information is written with a semi-colon rather than a regular comma, indicating that the order of the variables does not matter. To show the above symmetry, let us substitute the definitions of the marginal conditional entropy. This substitution for Eq. <span class="math inline">\(\ref{eq:mutual_info_entropy}\)</span> results in <span class="math display">\[
I(m; p) = 
- \sum_p P(p)\log P(p) - 
\left[ - \sum_m \sum_p P(m) P(p \mid m) \log P(p \mid m)\right].
\label{eq:mutual_info_probs}
\]</span> The trick is now to use the definition of conditional probability in the right way. We know that the conditional probability is defined as <span class="math display">\[
P(p \mid m) \equiv \frac{P(m, p)}{P(m)}.
\label{eq:cond_prob}
\]</span> Furthermore, we know that we can obtain the probability <span class="math inline">\(P(p)\)</span> by marginalizing the joint distribution <span class="math inline">\(P(m, p)\)</span> over all values of <span class="math inline">\(m\)</span>. Mathematically this is written as <span class="math display">\[
P(p) = \sum_m P(m, p).
\label{eq:marginalization}
\]</span> What Eq. <span class="math inline">\(\ref{eq:marginalization}\)</span> is stating is that to compute the probability of observing value <span class="math inline">\(p\)</span> of our random variable, we can add the probability of all pairs <span class="math inline">\(m, p\)</span> with the desired that have the desired value of <span class="math inline">\(p\)</span>. For Eq. <span class="math inline">\(\ref{eq:mutual_info_probs}\)</span>, we substitute Eq. <span class="math inline">\(\ref{eq:marginalization}\)</span> on the first term (outside of the <span class="math inline">\(\log\)</span>) of the right-hand side and Eq.~<span class="math inline">\(\ref{eq:cond_prob}\)</span> on the second term (in and outside of the <span class="math inline">\(\log\)</span>), obtaining <span class="math display">\[
I(m; p) = 
- \sum_p \left[ \sum_m P(m, p) \right]\log P(p)
+ \sum_m \sum_p P(m, p) \log \frac{P(m, p)}{P(m)}.
\]</span> Since the order of the sums do not matter, we can factorize the common terms on the left-hand side and use the properties of logarithms to write <span class="math display">\[
I(m; p) = \sum_m \sum_p P(m, p) \log \frac{P(m, p)}{P(m)P(p)}.
\]</span> It is now easier to see that we would arrive at the same result if we started with the opposite conditional entropy <span class="math inline">\(P(m \mid p)\)</span>. These series of manipulations where we write either joint or conditional entropies will become handy in this thesis as we explore biophysical models of how to compute gene expression input-output functions (more on that in Chapter 3). Fig. 11(C) shows a schematic representation of the relationship of all the entropy-based quantities that we explored in this chapter. Although it is impossible to cover an entire field in a short introduction, I hope this intuitive explanation will suffice to understand the rest of the thesis.</p>
<h2 class="unnumbered" id="references">References</h2>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-Schrodinger1992" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">E. Schrödinger, <em>What Is Life?: With Mind and Matter and Autobiographical Sketches</em> (Cambridge University Press, 1992).</div>
</div>
<div id="ref-Phillips2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">R. Phillips, <em><span class="nocase">Schr<span class="nocase">ö</span>dinger’ <span>“What is Life?”</span> at 75</span></em>, ArXiv 1 (2021).</div>
</div>
<div id="ref-Cronin2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">L. Cronin and S. I. Walker, <em><span class="nocase">Beyond prebiotic chemistry</span></em>, Science (80-. ). <strong>352</strong>, 1174 (2016).</div>
</div>
<div id="ref-Davies2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">P. Davies, <em>The Demon in the Machine: How Hidden Webs of Information Are Solving the Mystery of Life</em> (University of Chicago Press, 2019).</div>
</div>
<div id="ref-Adami2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">C. Adami, <em><span class="nocase">What is information?</span></em>, Philos. Trans. R. Soc. A Math. Phys. Eng. Sci. <strong>374</strong>, 20150230 (2016).</div>
</div>
<div id="ref-Taylor2007" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">S. F. Taylor, N. Tishby, and W. Bialek, <em><span class="nocase">Information and fitness</span></em>, ArXiv (2007).</div>
</div>
<div id="ref-Bialek2012" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">W. Bialek, <em>Biophysics: Searching for Principles</em> (Princeton University Press, 2012).</div>
</div>
<div id="ref-Browning2004" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">D. F. Browning and S. J. W. Busby, <em><span class="nocase">The regulation of bacterial transcription initiation</span></em>, Nature Reviews Microbiology <strong>2</strong>, 57 (2004).</div>
</div>
<div id="ref-Ireland2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">W. T. Ireland, S. M. Beeler, E. Flores-Bautista, N. S. McCarty, T. Röschinger, N. M. Belliveau, M. J. Sweredoski, A. Moradian, J. B. Kinney, and R. Phillips, <em>Deciphering the Regulatory Genome of Escherichia Coli, One Hundred Promoters at a Time</em>, Elife <strong>9</strong>, e55308 (2020).</div>
</div>
<div id="ref-Ackers1982" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">G. K. Ackers, A. D. Johnson, and M. A. Shea, <em><span class="nocase">Quantitative model for gene regulation by lambda phage repressor.</span></em>, Proceedings of the National Academy of Sciences <strong>79</strong>, 1129 (1982).</div>
</div>
<div id="ref-Bintu2005" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">L. Bintu, N. E. Buchler, H. G. Garcia, U. Gerland, T. Hwa, J. Kondev, T. Kuhlman, and R. Phillips, <em><span class="nocase">Transcriptional regulation by the numbers: applications.</span></em>, Curr. Opin. Genet. Dev. <strong>15</strong>, 125 (2005).</div>
</div>
<div id="ref-Kuhlman2007" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">T. Kuhlman, Z. Zhang, M. H. Saier, and T. Hwa, <em><span class="nocase">Combinatorial transcriptional control of the lactose operon of <em>Escherichia coli</em></span>.</em>, Proceedings of the National Academy of Sciences <strong>104</strong>, 6043 (2007).</div>
</div>
<div id="ref-Strogatz2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">S. H. Strogatz, <em>Nonlinear Dynamics and Chaos with Student Solutions Manual: With Applications to Physics, Biology, Chemistry, and Engineering</em> (CRC press, 2018).</div>
</div>
<div id="ref-Dill2010" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">K. Dill and S. Bromberg, <em>Molecular Driving Forces: Statistical Thermodynamics in Biology, Chemistry, Physics, and Nanoscience</em> (Garland Science, 2010).</div>
</div>
<div id="ref-Feynman1965" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">R. P. Feynman, R. B. Leighton, and M. Sands, <em>The Feynman Lectures on Physics; Vol. I</em>, American Journal of Physics <strong>33</strong>, 750 (1965).</div>
</div>
<div id="ref-Phillips2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">R. Phillips, N. M. Belliveau, G. Chure, H. G. Garcia, M. Razo-Mejia, and C. Scholes, <em><span class="nocase">Figure 1 Theory Meets Figure 2 Experiments in the Study of Gene Expression</span></em>, Annual Review of Biophysics <strong>48</strong>, 121 (2019).</div>
</div>
<div id="ref-Bremer1996" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">H. Bremer and P. P. Dennis, <em><span class="nocase">Modulation of Chemical Composition and Other Parameters of the Cell by Growth Rate</span></em>, (n.d.).</div>
</div>
<div id="ref-Schmidt2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">A. Schmidt, K. Kochanowski, S. Vedelaar, E. Ahrne, B. Volkmer, L. Callipo, K. Knoops, M. Bauer, R. Aebersold, and M. Heinemann, <em><span class="nocase">The quantitative and condition-dependent Escherichia coli proteome</span></em>, Nat Biotechnol <strong>34</strong>, 104 (2016).</div>
</div>
<div id="ref-Grigorova2006" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">I. L. Grigorova, N. J. Phleger, V. K. Mutalik, and C. A. Gross, <em><span class="nocase">Insights into transcriptional regulation and sigma competition from an equilibrium model of RNA polymerase binding to DNA.</span></em>, Proceedings of the National Academy of Sciences <strong>103</strong>, 5332 (2006).</div>
</div>
<div id="ref-Garcia2011c" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">H. G. Garcia and R. Phillips, <em><span class="nocase">Quantitative dissection of the simple repression input-output function.</span></em>, Proceedings of the National Academy of Sciences <strong>108</strong>, 12173 (2011a).</div>
</div>
<div id="ref-Chen2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">Y.-J. Chen, P. Liu, A. A. K. Nielsen, J. A. N. Brophy, K. Clancy, T. Peterson, and C. A. Voigt, <em><span class="nocase">Characterization of 582 natural and synthetic terminators and quantification of their design constraints</span></em>, Nat. Methods <strong>10</strong>, 659 (2013).</div>
</div>
<div id="ref-Eldar2010" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">A. Eldar and M. B. Elowitz, <em><span class="nocase">Functional roles for noise in genetic circuits</span></em>, Nature <strong>467</strong>, 167 (2010).</div>
</div>
<div id="ref-Voliotis2014a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">M. Voliotis, R. M. Perrett, C. McWilliams, C. a. McArdle, and C. G. Bowsher, <em><span class="nocase">Information transfer by leaky, heterogeneous, protein kinase signaling systems</span></em>, PNAS <strong>111</strong>, E326 (2014).</div>
</div>
<div id="ref-Balaban2004" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">N. Q. Balaban, <em><span class="nocase">Bacterial Persistence as a Phenotypic Switch</span></em>, Science (80-. ). <strong>305</strong>, 1622 (2004).</div>
</div>
<div id="ref-Gerland2002" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">U. Gerland and T. Hwa, <em><span class="nocase">On the Selection and Evolution of Regulatory DNA Motifs</span></em>, Journal of Molecular Evolution <strong>55</strong>, 386 (2002).</div>
</div>
<div id="ref-Sanchez2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">A. Sanchez, S. Choubey, and J. Kondev, <em><span class="nocase">Stochastic models of transcription: From single molecules to single cells</span></em>, Methods <strong>62</strong>, 13 (2013).</div>
</div>
<div id="ref-Shannon1948" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">C. E. Shannon, <em><span class="nocase">A Mathematical Theory of Communication</span></em>, Bell System Technical Journal <strong>27</strong>, 379 (1948).</div>
</div>
<div id="ref-Jaynes1957" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">E. T. Jaynes, <em><span class="nocase">Information Theory and Statistical Mechanics</span></em>, Physical Review <strong>106</strong>, 620 (1957).</div>
</div>
<div id="ref-Jaynes2003" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">E. T. Jaynes, <em>Probability Theory: The Logic of Science</em> (Cambridge university press, 2003).</div>
</div>
<div id="ref-Blitzstein2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">J. K. Blitzstein and J. Hwang, <em>Introduction to Probability</em> (Chapman; Hall/CRC, 2019).</div>
</div>
</div>
                    </div><!-- /.content -->
                </div><!-- /.col -->
                <div class="col-md-4 col-md-offset-1">
                    <div class="sections-list-wrapper">
                        <div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"><span
                                style="font-size: 14pt">Table of Contents</span>
                            <hr>
                        </div><!-- /.sections-list -->
                    </div>
                </div><!-- /.col -->
            </div><!-- /.row -->
        </div><!-- /.container -->
    </div><!-- /.section -->
    
    <div class="section section--grey">
        <div class="container">
            <div class="row">
                <div class="col-md-7">
                    <div id="disqus_thread"></div>
<script>
	/**
	 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
	 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
	 */
	 /*
	var disqus_config = function () {
	this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
		this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
	};
	*/
	(function() {
		var d = document, s = d.createElement('script');

		s.src = 'https://mrazomej-phd.disqus.com/embed.js';

		s.setAttribute('data-timestamp', +new Date());
		(d.head || d.body).appendChild(s);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

                </div><!-- /.col -->
            </div><!-- /.row -->
        </div><!-- /.container -->
    </div><!-- /.section -->
    
    <div class="js-footer-area">
        
        <nav class="page-nav">
            <div class="container">
                <div class="row">
                    <div class="col-xs-12">
                        
                        <a href="https://mrazomej.github.io/phd/abstract"
                            class="page-nav__item page-nav__item--prev">
                            <i class="icon icon--arrow-left"></i>
                            Abstract
                        </a><!-- /.page-nav__item -->
                        
                        
                        <a href="https://mrazomej.github.io/phd/chapter_02"
                            class="page-nav__item page-nav__item--next">
                            Chapter 2
                            <i class="icon icon--arrow-right"></i>
                        </a><!-- /.page-nav__item -->
                        
                    </div><!-- /.col -->
                </div><!-- /.row -->
            </div><!-- /.container -->
        </nav><!-- /.page-nav -->
        
        
        
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
						<a href="https://mrazomej.github.io/phd//" class="site-footer__logo">Manuel Razo-Mejia | PhD Thesis</a>
					
					
						<hr>
						<p class="site-footer__copyright">Copyright &copy; 2021. - Manuel Razo-Mejia <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
					<div class="col-sm-6 align-right">
						<ul class="social-list">
							
								<li>
									<a href="https://twitter.com/mrazomej" target="_blank" class="social-list__item social-list__item--twitter">
										<i class="icon icon--twitter"></i>
									</a>
								</li>
							
						</ul><!-- /.social-list -->
					</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="https://mrazomej.github.io/phd//doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="https://mrazomej.github.io/phd//doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="https://mrazomej.github.io/phd//doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="https://mrazomej.github.io/phd//doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="https://mrazomej.github.io/phd//doks-theme/assets/js/scripts.min.js"></script>


<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-109526846-4', 'auto');
	ga('send', 'pageview');
</script>



    </div><!-- /.js-footer-area -->
</body>

</html>