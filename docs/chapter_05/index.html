<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<!DOCTYPE html>

<html>

<head>
    <meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<title>Chapter V</title>


<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "Gyre Pagella",
        webFont : "STIX-Web",
        imageFont : null
    },
	  TeX: {
		equationNumbers: {
		  autoNumber: "AMS"
		}
	  },
	  tex2jax: {
		inlineMath: [['$','$']], 
		displayMath: [['$$','$$']],
		processEscapes: true,
	  }
	});
  </script>
  <style>
	.eqnos { display: inline-block; position: relative; width: 100%; }
	.eqnos br { display: none; }
	.eqnos-number { position: absolute; right: 0em; top: 50%; line-height: 0; }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  

<link rel="icon" href="https://mrazomej.github.io/phd//favicon.ico" type="image/x-icon">
<link href="https://fonts.googleapis.com/css?family=Nanum+Myeongjo&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Alegreya|Lora&display=swap" rel="stylesheet">

<link rel="stylesheet" href="https://mrazomej.github.io/phd//doks-theme/assets/css/style.css">

</head>

<body class="grey" data-spy="scroll" data-target=".js-scrollspy">
    


	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="https://mrazomej.github.io/phd//" class="site-header__logo"><i class="icon icon--chevron-left"></i> Home</a>
					
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


    <div class="hero-subheader" style="background-image: url(https://mrazomej.github.io/phd//doks-theme/assets/images/alaska.jpg); background-position: center; object-fit: cover; width:100%;">

        <div class="container" style="background-color: rgba(0,0,0, 0.75); border:0.75px solid white;">
            <div class="row">
                <div class="col-md-7">
                    <div class="align-container" data-mh>
                        <div class="align-inner">
                            
                            <h2 class="hero-subheader__title">Chapter V</h2>
                            
                            
                            <h1 class="hero-subheader__title">Supporting Information for First-principles prediction of the information processing capacity of a simple genetic circuit
</h1>
                            
                            
                            <!-- 
									
										<a href="https://mrazomej.github.io/phd/chapter_04" class="btn btn--dark btn--rounded btn--w-icon btn--w-icon-left">
											<i class="icon icon--arrow-left"></i>	
										</a>
									
									
										<a href="https://mrazomej.github.io/phd/chapter_06" class="btn btn--dark btn--rounded btn--w-icon">
											<i class="icon icon--arrow-right"></i>
										</a>
									
								 -->
                        </div><!-- /.align-inner -->
                    </div><!-- /.align-container -->
                </div><!-- /.col -->
                
                <div class="col-md-5 col-md-offset-0 hidden-xs hidden-sm">
                    <div class="align-container" data-mh>
                        <div class="align-inner">
                            <div class="hero-subheader__author">
                                <p class="hero-subheader__author-title">
                                    Summary
                                    <i class="icon icon--chevron-down"></i>
                                </p><!-- /.hero-subheader__author-title -->
                                <p>Living organisms are constantly sensing intra and extracellular cues and responding accordingly. The quality and precision of such responses can mean the difference between surviving or not certain challenges; therefore, there is a constant selection pressure for cells to gather enough information from any stimulus in order to build an adequate response. In this context, the information that cells can obtain has a precise mathematical definition measured-—just as in computers—-in bits.
In this chapter our goal is to predict how many bits of information can a cell harboring a simple genetic circuit process. To do so, I write down a theoretical model to predict the full distribution of gene expression based on the physics of this molecular process. I calibrate our model with previous information in order to perform parameter-free predictions. To test the model, I compare the predictions with experimental single-cell gene expression data finding great agreement.
</p>
                            </div><!-- /.hero-subheader__author -->
                        </div><!-- /.align-inner -->
                    </div><!-- /.align-container -->
                </div><!-- /.col -->
                
            </div><!-- /.row -->
        </div><!-- /.container -->
    </div><!-- /.hero-subheader -->
    
    <nav class="page-nav">
        <div class="container">
            <div class="row">
                <div class="col-xs-12">
                    
                    <a href="https://mrazomej.github.io/phd/chapter_04"
                        class="page-nav__item page-nav__item--prev">
                        <i class="icon icon--arrow-left"></i>
                        Chapter 4
                    </a><!-- /.page-nav__item -->
                    
                    
                    <a href="https://mrazomej.github.io/phd/chapter_06"
                        class="page-nav__item page-nav__item--next">
                        Chapter 6
                        <i class="icon icon--arrow-right"></i>
                    </a><!-- /.page-nav__item -->
                    
                </div><!-- /.col -->
            </div><!-- /.row -->
        </div><!-- /.container -->
    </nav><!-- /.page-nav -->
    

    <div class="section">
        <div class="container">
            <div class="row">
                <div class="col-md-7">
                    <div class="content">
                        <hr/>
<p>A version of this chapter originally appeared as Razo-Mejia, M., Marzen, S., Chure, G., Taubman, R., Morrison, M., and Phillips, R. (2020). First-principles prediction of the information processing capacity of a simple genetic circuit. Physical Review E 102, 022404. DOI:https://doi:10.1103/PhysRevE.102.022404.</p>
<h2 id="abstract">Abstract</h2>
<p>Given the stochastic nature of gene expression, genetically identical cells exposed to the same environmental inputs will produce different outputs. This heterogeneity has been hypothesized to have consequences for how cells are able to survive in changing environments. Recent work has explored the use of information theory as a framework to understand the accuracy with which cells can ascertain the state of their surroundings. Yet the predictive power of these approaches is limited and has not been rigorously tested using precision measurements. To that end, we generate a minimal model for a simple genetic circuit in which all parameter values for the model come from independently published data sets. We then predict the information processing capacity of the genetic circuit for a suite of biophysical parameters such as protein copy number and protein-DNA affinity. We compare these parameter-free predictions with an experimental determination of protein expression distributions and the resulting information processing capacity of <em>E. coli</em> cells. We find that our minimal model captures the scaling of the cell-to-cell variability in the data and the inferred information processing capacity of our simple genetic circuit up to a systematic deviation.</p>
<h2 id="supp_model">Three-state promoter model for simple repression</h2>
<p>In order to tackle the question of how much information the simple repression motif can process we require the joint probability distribution of mRNA and protein <span class="math inline">\(P(m, p; t)\)</span>. To obtain this distribution we use the chemical master equation formalism as described in . Specifically, we assume a three-state model where the promoter can be found 1) in a transcriptionally active state (<span class="math inline">\(A\)</span> state), 2) in a transcriptionally inactive state without the repressor bound (<span class="math inline">\(I\)</span> state) and 3) with the repressor bound (<span class="math inline">\(R\)</span> state). (See (A)). These three states generate a system of coupled differential equations for each of the three state distributions <span class="math inline">\(P_A(m, p)\)</span>, <span class="math inline">\(P_I(m, p)\)</span> and <span class="math inline">\(P_R(m, p)\)</span>. Given the rates shown in (A) let us define the system of ODEs. For the transcriptionally active state we have <span id="eq:ch5_eq01"><span class="math display">\[
\begin{split}
    \frac{d P_A(m, p)}{dt} &amp;=
    - \overbrace{k^{(p)}_{\text{off}} P_A(m, p)}^{A \rightarrow I} % A -&gt; I
    + \overbrace{k^{(p)}_{\text{on}} P_I(m, p)}^{I \rightarrow A}\\ % I -&gt; A
    &amp;+ \overbrace{r_m P_A(m-1, p)}^{m-1 \rightarrow m} % m-1 -&gt; m
    - \overbrace{r_m P_A(m, p)}^{m \rightarrow m+1}% m -&gt; m+1
    + \overbrace{\gamma _m (m + 1) P_A(m+1 , p)}^{m+1 \rightarrow m} % m+1 -&gt; m
    - \overbrace{\gamma _m m P_A(m , p)}^{m \rightarrow m-1}\\ % m -&gt; m-1
    &amp;+ \overbrace{r_p m P_A(m, p - 1)}^{p-1 \rightarrow p} % p-1 -&gt; p
    - \overbrace{r_p m P_A(m, p)}^{p \rightarrow p+1} % p -&gt; p+1
    + \overbrace{\gamma _p (p + 1) P_A(m, p + 1)}^{p + 1 \rightarrow p} % p+1 -&gt; p
    - \overbrace{\gamma _p p P_A(m, p)}^{p \rightarrow p-1}. % p -&gt; p-1
\end{split}
\qquad(1)\]</span></span> For the inactive promoter state <span class="math inline">\(I\)</span> we have <span id="eq:ch5_eq02"><span class="math display">\[
\begin{split}
    \frac{d P_I(m, p)}{dt} &amp;=
    \overbrace{k^{(p)}_{\text{off}} P_A(m, p)}^{A \rightarrow I} % A -&gt; I
    - \overbrace{k^{(p)}_{\text{on}} P_I(m, p)}^{I \rightarrow A} % I -&gt; A
    + \overbrace{k^{(r)}_{\text{off}} P_R(m, p)}^{R \rightarrow I} % R -&gt; I
    - \overbrace{k^{(r)}_{\text{on}} P_I(m, p)}^{I \rightarrow R}\\ % I -&gt; R
    &amp;+ \overbrace{\gamma _m (m + 1) P_I(m+1 , p)}^{m+1 \rightarrow m} % m+1 -&gt; m
    - \overbrace{\gamma _m m P_I(m , p)}^{m \rightarrow m-1}\\ % m -&gt; m-1
    &amp;+ \overbrace{r_p m P_I(m, p - 1)}^{p-1 \rightarrow p} % p-1 -&gt; p
    - \overbrace{r_p m P_I(m, p)}^{p \rightarrow p+1} % p -&gt; p+1
    + \overbrace{\gamma _p (p + 1) P_I(m, p + 1)}^{p + 1 \rightarrow p} % p+1 -&gt; p
    - \overbrace{\gamma _p p P_I(m, p)}^{p \rightarrow p-1}. % p -&gt; p-1
  \end{split}
\qquad(2)\]</span></span> And finally for the repressor bound state <span class="math inline">\(R\)</span> we have <span id="eq:ch5_eq03"><span class="math display">\[
\begin{split}
    \frac{d P_R(m, p)}{dt} &amp;=
    - \overbrace{k^{(r)}_{\text{off}} P_R(m, p)}^{R \rightarrow I} % R -&gt; I
    + \overbrace{k^{(r)}_{\text{on}} P_I(m, p)}^{I \rightarrow R}\\ % I -&gt; R
    &amp;+ \overbrace{\gamma _m (m + 1) P_R(m+1 , p)}^{m+1 \rightarrow m} % m+1 -&gt; m
    - \overbrace{\gamma _m m P_R(m , p)}^{m \rightarrow m-1}\\ % m -&gt; m-1
    &amp;+ \overbrace{r_p m P_R(m, p - 1)}^{p-1 \rightarrow p} % p-1 -&gt; p
    - \overbrace{r_p m P_R(m, p)}^{p \rightarrow p+1} % p -&gt; p+1
    + \overbrace{\gamma _p (p + 1) P_R(m, p + 1)}^{p + 1 \rightarrow p} % p+1 -&gt; p
    - \overbrace{\gamma _p p P_R(m, p)}^{p \rightarrow p-1}. % p -&gt; p-1
\end{split}
\qquad(3)\]</span></span></p>
<p>For an unregulated promoter, i.e. a promoter in a cell that has no repressors present, and therefore constitutively expresses the gene, we use a two-state model in which the state <span class="math inline">\(R\)</span> is not allowed. All the terms in the system of ODEs containing <span class="math inline">\(k^{(r)}_{\text{on}}\)</span> or <span class="math inline">\(k^{(r)}_{\text{off}}\)</span> are then set to zero.</p>
<p>As detailed in it is convenient to express this system using matrix notation <span class="citation" data-cites="Sanchez2013"> [<a href="#ref-Sanchez2013" role="doc-biblioref">20</a>]</span>. For this we define <span class="math inline">\(\mathbf{P}(m, p) = (P_A(m, p), P_I(m, p), P_R(m, p))^T\)</span>. Then the system of ODEs can be expressed as <span id="eq:ch5_eq04"><span class="math display">\[
\begin{split}
    \frac{d \mathbf{P}(m, p)}{dt} &amp;= \mathbf{K} \mathbf{P}(m, p)
    - \mathbf{R}_m \mathbf{P}(m, p) + \mathbf{R}_m \mathbf{P}(m-1, p)
    - m \mathbf{\Gamma}_m \mathbf{P}(m, p) + (m + 1) \mathbf{\Gamma}_m \mathbf{P}(m + 1, p)\\
    &amp;- m \mathbf{R}_p \mathbf{P}(m, p) + m \mathbf{R}_p \mathbf{P}(m, p - 1)
    - p \mathbf{\Gamma}_p \mathbf{P}(m, p) + (p + 1) \mathbf{\Gamma}_p \mathbf{P}(m, p + 1),
\end{split}
\qquad(4)\]</span></span> where we defined matrices representing the promoter state transition <span class="math inline">\(\mathbf{K}\)</span>, <span id="eq:ch5_eq05"><span class="math display">\[
 \mathbf{K} \equiv
 \begin{bmatrix}
    -k^{(p)}_{\text{off}}   &amp; k^{(p)}_{\text{on}}         &amp; 0\\
    k^{(p)}_{\text{off}}    &amp; -k^{(p)}_{\text{on}} -k^{(r)}_{\text{on}}  &amp; k^{(r)}_{\text{off}}\\
    0         &amp; k^{(r)}_{\text{on}}         &amp; -k^{(r)}_{\text{off}}
 \end{bmatrix},
\qquad(5)\]</span></span> mRNA production, <span class="math inline">\(\mathbf{R}_m\)</span>, and degradation, <span class="math inline">\(\mathbf{\Gamma}_m\)</span>, as <span id="eq:ch5_eq06"><span class="math display">\[
\mathbf{R}_m \equiv
\begin{bmatrix}
    r_m   &amp; 0 &amp; 0\\
    0     &amp; 0 &amp; 0\\
    0     &amp; 0 &amp; 0\\
\end{bmatrix},
\qquad(6)\]</span></span> and <span id="eq:ch5_eq07"><span class="math display">\[
\mathbf{\Gamma}_m \equiv
  \begin{bmatrix}
    \gamma _m   &amp; 0   &amp; 0\\
    0     &amp; \gamma _m &amp; 0\\
    0     &amp; 0   &amp; \gamma _m\\
  \end{bmatrix}.
\qquad(7)\]</span></span> For the protein we also define production <span class="math inline">\(\mathbf{R}_p\)</span> and degradation <span class="math inline">\(\mathbf{\Gamma}_p\)</span> matrices as <span id="eq:ch5_eq08"><span class="math display">\[
\mathbf{R}_p \equiv
\begin{bmatrix}
    r_p   &amp; 0   &amp; 0\\
    0     &amp; r_p &amp; 0\\
    0     &amp; 0   &amp; r_p\\
\end{bmatrix}
\qquad(8)\]</span></span> and <span id="eq:ch5_eq09"><span class="math display">\[
\mathbf{\Gamma}_p \equiv
  \begin{bmatrix}
    \gamma _p   &amp; 0   &amp; 0\\
    0     &amp; \gamma _p &amp; 0\\
    0     &amp; 0   &amp; \gamma _p\\
\end{bmatrix}.
\qquad(9)\]</span></span></p>
<p>The corresponding equation for the unregulated two-state promoter takes the exact same form with the definition of the matrices following the same scheme without including the third row and third column, and setting <span class="math inline">\(k^{(r)}_{\text{on}}\)</span> and <span class="math inline">\(k^{(r)}_{\text{off}}\)</span> to zero.</p>
<p>A closed-form solution for this master equation might not even exist. The approximate solution of chemical master equations of this kind is an active area of research. As we will see in the two-state promoter master equation has been analytically solved for the mRNA <span class="citation" data-cites="Peccoud1995"> [<a href="#ref-Peccoud1995" role="doc-biblioref">10</a>]</span> and protein distributions <span class="citation" data-cites="Shahrezaei2008"> [<a href="#ref-Shahrezaei2008" role="doc-biblioref">31</a>]</span>. For our purposes, in we will detail how to use the Maximum Entropy principle to approximate the full distribution for the two- and three-state promoter.</p>
<h2 id="parameter-inference">Parameter inference</h2>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/chemical_master_mRNA_FISH_mcmc.html">following link</a> as an annotated Jupyter notebook)</p>
<p>With the objective of generating falsifiable predictions with meaningful parameters, we infer the kinetic rates for this three-state promoter model using different data sets generated in our lab over the last decade concerning different aspects of the regulation of the simple repression motif. For example, for the unregulated promoter transition rates <span class="math inline">\(k^{(p)}_{\text{on}}\)</span> and <span class="math inline">\(k^{(p)}_{\text{off}}\)</span> and the mRNA production rate <span class="math inline">\(r_m\)</span>, we use single-molecule mRNA FISH counts from an unregulated promoter <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">29</a>]</span>. Once these parameters are fixed, we use the values to constrain the repressor rates <span class="math inline">\(k^{(r)}_{\text{on}}\)</span> and <span class="math inline">\(k^{(r)}_{\text{off}}\)</span>. These repressor rates are obtained using information from mean gene expression measurements from bulk LacZ colorimetric assays <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">35</a>]</span>. We also expand our model to include the allosteric nature of the repressor protein, taking advantage of video microscopy measurements done in the context of multiple promoter copies <span class="citation" data-cites="Brewster2014"> [<a href="#ref-Brewster2014" role="doc-biblioref">36</a>]</span> and flow-cytometry measurements of the mean response of the system to different levels of induction <span class="citation" data-cites="Razo-Mejia2018"> [<a href="#ref-Razo-Mejia2018" role="doc-biblioref">18</a>]</span>. In what follows of this section we detail the steps taken to infer the parameter values. At each step the values of the parameters inferred in previous steps constrain the values of the parameters that are not yet determined, building in this way a self-consistent model informed by work that spans several experimental techniques.</p>
<h3 id="unregulated-promoter-rates">Unregulated promoter rates</h3>
<p>We begin our parameter inference problem with the promoter on and off rates <span class="math inline">\(k^{(p)}_{\text{on}}\)</span> and <span class="math inline">\(k^{(p)}_{\text{off}}\)</span>, as well as the mRNA production rate <span class="math inline">\(r_m\)</span>. In this case there are only two states available to the promoter – the inactive state <span class="math inline">\(I\)</span> and the transcriptionally active state <span class="math inline">\(A\)</span>. That means that the third ODE for <span class="math inline">\(P_R(m, p)\)</span> is removed from the system. The mRNA steady state distribution for this particular two-state promoter model was solved analytically by Peccoud and Ycart <span class="citation" data-cites="Peccoud1995"> [<a href="#ref-Peccoud1995" role="doc-biblioref">10</a>]</span>. This distribution <span class="math inline">\(P(m) \equiv P_I(m) + P_A(m)\)</span> is of the form <span id="eq:ch5_eq10"><span class="math display">\[
\small
  P(m \mid k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m, \gamma _m) =
  {\Gamma \left( \frac{k^{(p)}_{\text{on}}}{\gamma _m} + m \right) \over
  \Gamma(m + 1) \Gamma\left( \frac{k^{(p)}_{\text{off}}+k^{(p)}_{\text{on}}}{\gamma _m} + m \right)}
  {\Gamma\left( \frac{k^{(p)}_{\text{off}}+k^{(p)}_{\text{on}}}{\gamma _m} \right) \over
  \Gamma\left( \frac{k^{(p)}_{\text{on}}}{\gamma _m} \right)}
  \left( \frac{r_m}{\gamma _m} \right)^m
  F_1^1 \left( \frac{k^{(p)}_{\text{on}}}{\gamma _m} + m,
  \frac{k^{(p)}_{\text{off}} + k^{(p)}_{\text{on}}}{\gamma _m} + m,
  -\frac{r_m}{\gamma _m} \right),
\qquad(10)\]</span></span> where <span class="math inline">\(\Gamma(\cdot)\)</span> is the gamma function, and <span class="math inline">\(F_1^1\)</span> is the confluent hypergeometric function of the first kind. This rather complicated expression will aid us to find parameter values for the rates. The inferred rates <span class="math inline">\(k^{(p)}_{\text{on}}\)</span>, <span class="math inline">\(k^{(p)}_{\text{off}}\)</span> and <span class="math inline">\(r_m\)</span> will be expressed in units of the mRNA degradation rate <span class="math inline">\(\gamma _m\)</span>. This is because the model in is homogeneous in time, meaning that if we divide all rates by a constant it would be equivalent to multiplying the characteristic time scale by the same constant. As we will discuss in the next section, has degeneracy in the parameter values. What this means is that a change in one of the parameters, specifically <span class="math inline">\(r_m\)</span>, can be compensated by a change in another parameter, specifically <span class="math inline">\(k^{(p)}_{\text{off}}\)</span>, to obtain the exact same distribution. To work around this intrinsic limitation of the model we will include in our inference prior information from what we know from equilibrium-based models.</p>
<h3 id="bayesian-parameter-inference-of-rnap-rates">Bayesian parameter inference of RNAP rates</h3>
<p>In order to make progress at inferring the unregulated promoter state transition rates, we make use of the single-molecule mRNA FISH data from Jones et al. <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">29</a>]</span>. shows the distribution of mRNA per cell for the <em>lacUV5</em> promoter used for our inference. This promoter, being very strong, has a mean copy number of <span class="math inline">\(\langle m \rangle \approx 18\)</span> mRNA/cell.</p>
<figure>
<img src="ch5_fig01.png" id="fig:ch5_fig01" data-short-caption="lacUV5* mRNA per cell distribution" alt="Figure 1: lacUV5 mRNA per cell distribution. Data from  [29] of the unregulated lacUV5 promoter as inferred from single molecule mRNA FISH." /><figcaption aria-hidden="true">Figure 1: <strong><em>lacUV5</em> mRNA per cell distribution.</strong> Data from <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">29</a>]</span> of the unregulated <em>lacUV5</em> promoter as inferred from single molecule mRNA FISH.</figcaption>
</figure>
<p>Having this data in hand we now turn to Bayesian parameter inference. Writing Bayes theorem we have <span id="eq:ch5_eq11"><span class="math display">\[
P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m \mid D) = 
\frac{P(D \mid k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m)
P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m)}{P(D)},
\qquad(11)\]</span></span> where <span class="math inline">\(D\)</span> represents the data. For this case the data consists of single-cell mRNA counts <span class="math inline">\(D = \{ m_1, m_2, \ldots, m_N \}\)</span>, where <span class="math inline">\(N\)</span> is the number of cells. We assume that each cell’s measurement is independent of the others such that we can rewrite as <span id="eq:ch5_eq12"><span class="math display">\[
P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m \mid \{m_i\}) \propto
  \left[\prod_{i=1}^N P(m_i \mid k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m) \right]
  P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m),
\qquad(12)\]</span></span> where we ignore the normalization constant <span class="math inline">\(P(D)\)</span>. The likelihood term <span class="math inline">\(P(m_i \mid k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m)\)</span> is exactly given by with <span class="math inline">\(\gamma _m = 1\)</span>. Given that we have this functional form for the distribution, we can use Markov Chain Monte Carlo (MCMC) sampling to explore the 3D parameter space in order to fit to the mRNA-FISH data.</p>
<h4 id="constraining-the-rates-given-prior-thermodynamic-knowledge.">Constraining the rates given prior thermodynamic knowledge.</h4>
<p>One of the strengths of the Bayesian approach is that we can include all the prior knowledge on the parameters when performing an inference <span class="citation" data-cites="MacKay2003"> [<a href="#ref-MacKay2003" role="doc-biblioref">7</a>]</span>. Basic features such as the fact that the rates have to be strictly positive constrain the values that these parameters can take. For the specific rates analyzed in this section we know more than the simple constraint of non-negative values. The expression of an unregulated promoter has been studied from a thermodynamic perspective <span class="citation" data-cites="Brewster2012"> [<a href="#ref-Brewster2012" role="doc-biblioref">34</a>]</span>. Given the underlying assumptions of these equilibrium models, in which the probability of finding the RNAP bound to the promoter is proportional to the transcription rate <span class="citation" data-cites="Bintu2005a"> [<a href="#ref-Bintu2005a" role="doc-biblioref">30</a>]</span>, they can only make statements about the mean expression level. Nevertheless if both the thermodynamic and the kinetic model describe the same process, the predictions for the mean gene expression level must agree. That means that we can use what we know about the mean gene expression, and how this is related to parameters such as molecule copy numbers and binding affinities, to constrain the values that the rates in question can take.</p>
<p>In the case of this two-state promoter it can be shown that the mean number of mRNA is given by <span class="citation" data-cites="Sanchez2013"> [<a href="#ref-Sanchez2013" role="doc-biblioref">20</a>]</span> (See XXX for moment computation) <span id="eq:ch5_eq13"><span class="math display">\[
\langle m \rangle = \frac{r_m}{\gamma _m}
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}.
\qquad(13)\]</span></span> Another way of expressing this is as <span class="math inline">\(\frac{r_m}{\gamma _m} \times p_{\text{active}}^{(p)}\)</span>, where <span class="math inline">\(p_{\text{active}}^{(p)}\)</span> is the probability of the promoter being in the transcriptionally active state. The thermodynamic picture has an equivalent result where the mean number of mRNA is given by <span class="citation" data-cites="Brewster2012 Bintu2005a"> [<a href="#ref-Bintu2005a" role="doc-biblioref">30</a>,<a href="#ref-Brewster2012" role="doc-biblioref">34</a>]</span> <span id="eq:ch5_eq14"><span class="math display">\[
\left\langle m \right\rangle = \frac{r_m}{\gamma _m}
\frac{\frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p}
}{1 + \frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p}},
\qquad(14)\]</span></span> where <span class="math inline">\(P\)</span> is the number of RNAP per cell, <span class="math inline">\(N_{NS}\)</span> is the number of non-specific binding sites, <span class="math inline">\(\Delta\varepsilon_p\)</span> is the RNAP binding energy in <span class="math inline">\(k_BT\)</span> units and <span class="math inline">\(\beta\equiv {(k_BT)}^{-1}\)</span>. Using and we can easily see that if these frameworks are to be equivalent, then it must be true that <span id="eq:ch5_eq15"><span class="math display">\[
\frac{k^{(p)}_{\text{on}}}{ k^{(p)}_{\text{off}}} =
\frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p},
\qquad(15)\]</span></span> or equivalently <span id="eq:ch5_eq16"><span class="math display">\[
\ln \left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) =
  -\beta\Delta\varepsilon_p + \ln P - \ln N_{NS}.
\qquad(16)\]</span></span> To put numerical values into these variables we can use information from the literature. The RNAP copy number is order <span class="math inline">\(P \approx 1000-3000\)</span> RNAP/cell for a 1 hour doubling time <span class="citation" data-cites="Klumpp2008"> [<a href="#ref-Klumpp2008" role="doc-biblioref">32</a>]</span>. As for the number of non-specific binding sites and the binding energy, we have that <span class="math inline">\(N_{NS} = 4.6\times 10^6\)</span> <span class="citation" data-cites="Bintu2005a"> [<a href="#ref-Bintu2005a" role="doc-biblioref">30</a>]</span> and <span class="math inline">\(-\beta\Delta\varepsilon_p \approx 5 - 7\)</span> <span class="citation" data-cites="Brewster2012"> [<a href="#ref-Brewster2012" role="doc-biblioref">34</a>]</span>. Given these values we define a Gaussian prior for the log ratio of these two quantities of the form <span id="eq:ch5_eq17"><span class="math display">\[
P\left(\ln \left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) \right) \propto
\exp \left\{ - \frac{\left(\ln \left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) -
\left(-\beta\Delta\varepsilon_p + \ln P - \ln N_{NS} \right) \right)^2
}{2 \sigma^2} \right\},
\qquad(17)\]</span></span> where <span class="math inline">\(\sigma\)</span> is the variance that accounts for the uncertainty in these parameters. We include this prior as part of the prior term <span class="math inline">\(P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m)\)</span> of . We then use MCMC to sample out of the posterior distribution given by . shows the MCMC samples of the posterior distribution. For the case of the <span class="math inline">\(k^{(p)}_{\text{on}}\)</span> parameter there is a single symmetric peak. <span class="math inline">\(k^{(p)}_{\text{off}}\)</span> and <span class="math inline">\(r_m\)</span> have a rather long tail towards large values. In fact, the 2D projection of <span class="math inline">\(k^{(p)}_{\text{off}}\)</span> vs <span class="math inline">\(r_m\)</span> shows that the model is sloppy, meaning that the two parameters are highly correlated. This feature is a common problem for many non-linear systems used in biophysics and systems biology <span class="citation" data-cites="Transtrum2015"> [<a href="#ref-Transtrum2015" role="doc-biblioref">37</a>]</span>. What this implies is that we can change the value of <span class="math inline">\(k^{(p)}_{\text{off}}\)</span>, and then compensate by a change in <span class="math inline">\(r_m\)</span> in order to maintain the shape of the mRNA distribution. Therefore it is impossible from the data and the model themselves to narrow down a single value for the parameters. Nevertheless since we included the prior information on the rates as given by the analogous form between the equilibrium and non-equilibrium expressions for the mean mRNA level, we obtained a more constrained parameter value for the RNAP rates and the transcription rate that we will take as the peak of this long-tailed distribution.</p>
<figure>
<img src="ch5_fig02.png" id="fig:ch5_fig02" data-short-caption="MCMC posterior distribution." alt="Figure 2: MCMC posterior distribution. Sampling out of the plot shows 2D and 1D projections of the 3D parameter space. The parameter values are (in units of the mRNA degradation rate \gamma _m) k^{(p)}_{\text{on}} = 4.3^{+1}_{-0.3}, k^{(p)}_{\text{off}} = 18.8^{+120}_{-10} and r_m = 103.8^{+423}_{-37} which are the modes of their respective distributions, where the superscripts and subscripts represent the upper and lower bounds of the 95^\text{th} percentile of the parameter value distributions" /><figcaption aria-hidden="true">Figure 2: <strong>MCMC posterior distribution.</strong> Sampling out of the plot shows 2D and 1D projections of the 3D parameter space. The parameter values are (in units of the mRNA degradation rate <span class="math inline">\(\gamma _m\)</span>) <span class="math inline">\(k^{(p)}_{\text{on}} = 4.3^{+1}_{-0.3}\)</span>, <span class="math inline">\(k^{(p)}_{\text{off}} = 18.8^{+120}_{-10}\)</span> and <span class="math inline">\(r_m = 103.8^{+423}_{-37}\)</span> which are the modes of their respective distributions, where the superscripts and subscripts represent the upper and lower bounds of the 95<span class="math inline">\(^\text{th}\)</span> percentile of the parameter value distributions</figcaption>
</figure>
<p>The inferred values <span class="math inline">\(k^{(p)}_{\text{on}} = 4.3^{+1}_{-0.3}\)</span>, <span class="math inline">\(k^{(p)}_{\text{off}} = 18.8^{+120}_{-10}\)</span> and <span class="math inline">\(r_m = 103.8^{+423}_{-37}\)</span> are given in units of the mRNA degradation rate <span class="math inline">\(\gamma _m\)</span>. Given the asymmetry of the parameter distributions we report the upper and lower bound of the 95<span class="math inline">\(^\text{th}\)</span> percentile of the posterior distributions. Assuming a mean life-time for mRNA of <span class="math inline">\(\approx\)</span> 3 min (from this <a href="http://bionumbers.hms.harvard.edu/bionumber.aspx?&amp;id=107514&amp;ver=1&amp;trm=mRNA%20mean%20lifetime">link</a>) we have an mRNA degradation rate of <span class="math inline">\(\gamma _m \approx 2.84 \times 10^{-3} s^{-1}\)</span>. Using this value gives the following values for the inferred rates: <span class="math inline">\(k^{(p)}_{\text{on}} = 0.024_{-0.002}^{+0.005} s^{-1}\)</span>, <span class="math inline">\(k^{(p)}_{\text{off}} = {0.11}_ {-0.05}^{+0.66} s^{-1}\)</span>, and <span class="math inline">\(r_m = 0.3_{-0.2}^{+2.3} s^{-1}\)</span>.</p>
<p>Fig. 3 compares the experimental data from with the resulting distribution obtained by substituting the most likely parameter values into . As we can see this two-state model fits the data adequately.</p>
<figure>
<img src="ch5_fig03.png" id="fig:ch5_fig03" data-short-caption="Experimental vs. theoretical distribution of mRNA per cell using parameters from Bayesian inference" alt="Figure 3: Experimental vs. theoretical distribution of mRNA per cell using parameters from Bayesian inference. Dotted line shows the result of using along with the parameters inferred for the rates. Blue bars are the same data as obtained from  [29]." /><figcaption aria-hidden="true">Figure 3: <strong>Experimental vs. theoretical distribution of mRNA per cell using parameters from Bayesian inference.</strong> Dotted line shows the result of using along with the parameters inferred for the rates. Blue bars are the same data as obtained from <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">29</a>]</span>.</figcaption>
</figure>
<h3 id="accounting-for-variability-in-the-number-of-promoters">Accounting for variability in the number of promoters</h3>
<p>As discussed in ref. <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">29</a>]</span> and further expanded in <span class="citation" data-cites="Peterson2015"> [<a href="#ref-Peterson2015" role="doc-biblioref">6</a>]</span> an important source of cell-to-cell variability in gene expression in bacteria is the fact that, depending on the growth rate and the position relative to the chromosome replication origin, cells can have multiple copies of any given gene. Genes closer to the replication origin have on average higher gene copy number compared to genes at the opposite end. For the locus in which our reporter construct is located (<em>galK</em>) and the doubling time of the mRNA FISH experiments we expect to have <span class="math inline">\(\approx\)</span> 1.66 copies of the gene <span class="citation" data-cites="Jones2014a Bremer1996"> [<a href="#ref-Bremer1996" role="doc-biblioref">17</a>,<a href="#ref-Jones2014a" role="doc-biblioref">29</a>]</span>. This implies that the cells spend 2/3 of the cell cycle with two copies of the promoter and the rest with a single copy.</p>
<p>To account for this variability in gene copy we extend the model assuming that when cells have two copies of the promoter the mRNA production rate is <span class="math inline">\(2 r_m\)</span> compared to the rate <span class="math inline">\(r_m\)</span> for a single promoter copy. The probability of observing a certain mRNA copy <span class="math inline">\(m\)</span> is therefore given by <span id="eq:ch5_eq18"><span class="math display">\[
P(m) = P(m \mid \text{one promoter}) \cdot P(\text{one promoter}) +
P(m \mid \text{two promoters}) \cdot P(\text{two promoters}).
\qquad(18)\]</span></span> Both terms <span class="math inline">\(P(m \mid \text{promoter copy})\)</span> are given by with the only difference being the rate <span class="math inline">\(r_m\)</span>. It is important to acknowledge that assumes that once the gene is replicated the time scale in which the mRNA count relaxes to the new steady state is much shorter than the time that the cells spend in this two promoter copies state. This approximation should be valid for a short lived mRNA molecule, but the assumption is not applicable for proteins whose degradation rate is comparable to the cell cycle length as explored in XXX.</p>
<p>In order to repeat the Bayesian inference including this variability in gene copy number we must split the mRNA count data into two sets – cells with a single copy of the promoter and cells with two copies of the promoter. For the single molecule mRNA FISH data there is no labeling of the locus, making it impossible to determine the number of copies of the promoter for any given cell. We therefore follow Jones et al. <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">29</a>]</span> in using the cell area as a proxy for stage in the cell cycle. In their approach they sorted cells by area, considering cells below the 33<span class="math inline">\(^\text{th}\)</span> percentile having a single promoter copy and the rest as having two copies. This approach ignores that cells are not uniformly distributed along the cell cycle. As first derived in <span class="citation" data-cites="Powell1956"> [<a href="#ref-Powell1956" role="doc-biblioref">16</a>]</span> populations of cells in a log-phase are exponentially distributed along the cell cycle. This distribution is of the form <span id="eq:ch5_eq19"><span class="math display">\[
P(a) = (\ln 2) \cdot 2^{1 - a},
\qquad(19)\]</span></span> where <span class="math inline">\(a \in [0, 1]\)</span> is the stage of the cell cycle, with <span class="math inline">\(a = 0\)</span> being the start of the cycle and <span class="math inline">\(a = 1\)</span> being the cell division (See for a derivation of ). shows the separation of the two groups based on area where was used to weight the distribution along the cell cycle.</p>
<figure>
<img src="ch5_fig04.png" id="fig:ch5_fig04" data-short-caption="Separation of cells based on cell size" alt="Figure 4: Separation of cells based on cell size. Using the area as a proxy for position in the cell cycle, cells can be sorted into two groups – small cells (with one promoter copy) and large cells (with two promoter copies). The vertical black line delimits the threshold that divides both groups as weighted by ." /><figcaption aria-hidden="true">Figure 4: <strong>Separation of cells based on cell size.</strong> Using the area as a proxy for position in the cell cycle, cells can be sorted into two groups – small cells (with one promoter copy) and large cells (with two promoter copies). The vertical black line delimits the threshold that divides both groups as weighted by .</figcaption>
</figure>
<p>A subtle, but important consequence of is that computing any quantity for a single cell is not equivalent to computing the same quantity for a population of cells. For example, let us assume that we want to compute the mean mRNA copy number <span class="math inline">\(\langle m \rangle\)</span>. For a single cell this would be of the form <span id="eq:ch5_eq20"><span class="math display">\[
\langle m \rangle_{\text{cell}} =
\langle m \rangle_1 \cdot f + \langle m \rangle_2 \cdot (1 - f),
\qquad(20)\]</span></span> where <span class="math inline">\(\langle m \rangle_i\)</span> is the mean mRNA copy number with <span class="math inline">\(i\)</span> promoter copies in the cell, and <span class="math inline">\(f\)</span> is the fraction of the cell cycle that cells spend with a single copy of the promoter. For a single cell the probability of having a single promoter copy is equivalent to this fraction <span class="math inline">\(f\)</span>. But tells us that if we sample unsynchronized cells we are not sampling uniformly across the cell cycle. Therefore for a population of cells the mean mRNA is given by <span id="eq:ch5_eq21"><span class="math display">\[
\langle m \rangle_{\text{population}} =
\langle m \rangle_1 \cdot \phi + \langle m \rangle_2 \cdot (1 - \phi)
\qquad(21)\]</span></span> where the probability of sampling a cell with one promoter <span class="math inline">\(\phi\)</span> is given by <span id="eq:ch5_eq22"><span class="math display">\[
\phi = \int_0^f P(a) da,
\qquad(22)\]</span></span> where <span class="math inline">\(P(a)\)</span> is given by XXX. What this equation computes is the probability of sampling a cell during a stage of the cell cycle <span class="math inline">\(&lt; f\)</span> where the reporter gene hasn’t been replicated yet. shows the distribution of both groups. As expected larger cells have a higher mRNA copy number on average.</p>
<figure>
<img src="ch5_fig05.png" id="fig:ch5_fig05" data-short-caption="mRNA distribution for small and large cells." alt="Figure 5: mRNA distribution for small and large cells. (A) histogram and (B) cumulative distribution function of the small and large cells as determined in . The triangles above histograms in (A) indicate the mean mRNA copy number for each group." /><figcaption aria-hidden="true">Figure 5: <strong>mRNA distribution for small and large cells.</strong> (A) histogram and (B) cumulative distribution function of the small and large cells as determined in . The triangles above histograms in (A) indicate the mean mRNA copy number for each group.</figcaption>
</figure>
<p>We modify to account for the two separate groups of cells. Let <span class="math inline">\(N_s\)</span> be the number of cells in the small size group and <span class="math inline">\(N_l\)</span> the number of cells in the large size group. Then the posterior distribution for the parameters is of the form <span id="eq:ch5_eq23"><span class="math display">\[
\small
P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m \mid \{m_i\}) \propto
\left[\prod_{i=1}^{N_s} P(m_i \mid k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m)\right]
\left[\prod_{j=1}^{N_l} P(m_j \mid k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, 2 r_m)\right]
P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m),
\qquad(23)\]</span></span> where we split the product of small and large cells.</p>
<p>For the two-promoter model the prior shown in requires a small modification. gives the mean mRNA copy number of a population of asynchronous cells growing at steady state. Given that we assume that the only difference between having one vs. two promoter copies is the change in transcription rate from <span class="math inline">\(r_m\)</span> in the single promoter case to <span class="math inline">\(2 r_m\)</span> in the two-promoter case we can write as <span id="eq:ch5_eq24"><span class="math display">\[
\langle m \rangle = \phi \cdot \frac{r_m}{\gamma _m}
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}} +
(1 -\phi) \cdot \frac{2 r_m}{\gamma _m}
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}.
\qquad(24)\]</span></span> This can be simplified to <span id="eq:ch5_eq25"><span class="math display">\[
\langle m \rangle = (2 - \phi) \frac{r_m}{\gamma _m} 
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}.
\qquad(25)\]</span></span></p>
<p>Equating and to again require self-consistent predictions of the mean mRNA from the equilibrium and kinetic models gives <span id="eq:ch5_eq26"><span class="math display">\[
(2 - \phi) \frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}} =
\frac{\frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p}
}{1 + \frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p}}.
\qquad(26)\]</span></span> Solving for <span class="math inline">\(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\)</span> results in <span id="eq:ch5_eq27"><span class="math display">\[
\left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) =
\frac{\rho}{\left[ (1 + \rho)(2 - \phi) - \rho \right]},
\qquad(27)\]</span></span> where we define <span class="math inline">\(\rho \equiv \frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p}\)</span>. To simplify things further we notice that for the specified values of <span class="math inline">\(P = 1000 - 3000\)</span> per cell, <span class="math inline">\(N_{NS} = 4.6 \times 10^6\)</span> bp, and <span class="math inline">\(-\beta\Delta\varepsilon_p = 5 - 7\)</span>, we can safely assume that <span class="math inline">\(\rho \ll 1\)</span>. This simplifying assumption has been previously called the weak promoter approximation <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">35</a>]</span>. Given this we can simplify as <span id="eq:ch5_eq28"><span class="math display">\[
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}} =
\frac{1}{2 - \phi} \frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p}.
\qquad(28)\]</span></span> Taking the log of both sides gives <span id="eq:ch5_eq29"><span class="math display">\[
\ln\left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) =
-\ln (2 - \phi) + \ln P - \ln N_{NS}
  - \beta\Delta\varepsilon_p.
\qquad(29)\]</span></span> With this we can set as before a Gaussian prior to constrain the ratio of the RNAP rates as <span id="eq:ch5_eq30"><span class="math display">\[
P\left(\ln \left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) \right)  \propto
\exp \left\{ - \frac{\left(\ln \left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) -
\left[-\ln(2 - \phi) -\beta\Delta\varepsilon_p + \ln P - \ln N_{NS} \right) \right]^2
}{2 \sigma^2} \right\}.
\qquad(30)\]</span></span></p>
<p>Fig. 6 shows the result of sampling out of XXX. Again we see that the model is highly sloppy with large credible regions obtained for <span class="math inline">\(k^{(p)}_{\text{off}}\)</span> and <span class="math inline">\(r_m\)</span>. Nevertheless, again the use of the prior information allows us to get a parameter values consistent with the equilibrium picture.</p>
<figure>
<img src="ch5_fig06.png" id="fig:ch5_fig06" data-short-caption="MCMC posterior distribution for a multi-promoter model" alt="Figure 6: MCMC posterior distribution for a multi-promoter model. Sampling out of the plot shows 2D and 1D projections of the 3D parameter space. The parameter values are (in units of the mRNA degradation rate \gamma _m) k^{(p)}_{\text{on}} = 6.4^{+0.8}_{-0.4}, k^{(p)}_{\text{off}} = 132^{+737}_{-75} and r_m = 257^{+1307}_{-132} which are the modes of their respective distributions, where the superscripts and subscripts represent the upper and lower bounds of the 95^\text{th} percentile of the parameter value distributions. The sampling was bounded to values &lt; 1000 for numerical stability when computing the confluent hypergeometric function." /><figcaption aria-hidden="true">Figure 6: <strong>MCMC posterior distribution for a multi-promoter model.</strong> Sampling out of the plot shows 2D and 1D projections of the 3D parameter space. The parameter values are (in units of the mRNA degradation rate <span class="math inline">\(\gamma _m\)</span>) <span class="math inline">\(k^{(p)}_{\text{on}} = 6.4^{+0.8}_{-0.4}\)</span>, <span class="math inline">\(k^{(p)}_{\text{off}} = 132^{+737}_{-75}\)</span> and <span class="math inline">\(r_m = 257^{+1307}_{-132}\)</span> which are the modes of their respective distributions, where the superscripts and subscripts represent the upper and lower bounds of the 95<span class="math inline">\(^\text{th}\)</span> percentile of the parameter value distributions. The sampling was bounded to values <span class="math inline">\(&lt;\)</span> 1000 for numerical stability when computing the confluent hypergeometric function.</figcaption>
</figure>
<p>Using again a mRNA mean lifetime of <span class="math inline">\(\approx 3\)</span> min gives the following values for the parameters: <span class="math inline">\(k^{(p)}_{\text{on}} = {0.03}_{-0.002}^{+0.004} s^{-1}\)</span>, <span class="math inline">\(k^{(p)}_{\text{off}} = {0.7} _{-0.4}^{+4.1} s^{-1}\)</span>, and <span class="math inline">\(r_m = {1.4}_{-0.7}^{+7.3} s^{-1}\)</span>. shows the result of applying using these parameter values. Specifically (A) shows the global distribution including cells with one and two promoters and (B) splits the distributions within the two populations. Given that the model adequately describes both populations independently and pooled together we confirm that using the cell area as a proxy for stage in the cell cycle and the doubling of the transcription rate once cells have two promoters are reasonable approximations.</p>
<figure>
<img src="ch5_fig07.png" id="fig:ch5_fig07" data-short-caption="Experimental vs. theoretical distribution of mRNA per cell using parameters for multi-promoter model" alt="Figure 7: Experimental vs. theoretical distribution of mRNA per cell using parameters for multi-promoter model. (A) Solid line shows the result of using with the parameters inferred by sampling . Blue bars are the same data as from  [29]. (B) Split distributions of small cells (light blue bars) and large cells (dark blue) with the corresponding theoretical predictions with transcription rate r_m (light blue line) and transcription rate 2 r_m (dark blue line)" /><figcaption aria-hidden="true">Figure 7: <strong>Experimental vs. theoretical distribution of mRNA per cell using parameters for multi-promoter model.</strong> (A) Solid line shows the result of using with the parameters inferred by sampling . Blue bars are the same data as from <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">29</a>]</span>. (B) Split distributions of small cells (light blue bars) and large cells (dark blue) with the corresponding theoretical predictions with transcription rate <span class="math inline">\(r_m\)</span> (light blue line) and transcription rate <span class="math inline">\(2 r_m\)</span> (dark blue line)</figcaption>
</figure>
<p>It is hard to make comparisons with literature reported values because these kinetic rates are effective parameters hiding a lot of the complexity of transcription initiation <span class="citation" data-cites="Browning2004"> [<a href="#ref-Browning2004" role="doc-biblioref">23</a>]</span>. Also the non-identifiability of the parameters restricts our explicit comparison of the actual numerical values of the inferred rates. Nevertheless from the model we can see that the mean burst size for each transcription event is given by <span class="math inline">\(r_m / k^{(p)}_{\text{off}}\)</span>. From our inferred values we obtain then a mean burst size of <span class="math inline">\(\approx 1.9\)</span> transcripts per cell. This is similar to the reported burst size of 1.15 on a similar system on <em>E. coli</em> <span class="citation" data-cites="Yu2006"> [<a href="#ref-Yu2006" role="doc-biblioref">22</a>]</span>.</p>
<h3 id="repressor-rates-from-three-state-regulated-promoter.">Repressor rates from three-state regulated promoter.</h3>
<p>Having determined the unregulated promoter transition rates we now proceed to determine the repressor rates <span class="math inline">\(k^{(r)}_{\text{on}}\)</span> and <span class="math inline">\(k^{(r)}_{\text{off}}\)</span>. The values of these rates are constrained again by the correspondence between our kinetic picture and what we know from equilibrium models <span class="citation" data-cites="Phillips2015a"> [<a href="#ref-Phillips2015a" role="doc-biblioref">33</a>]</span>. For this analysis we again exploit the feature that, at the mean, both the kinetic language and the thermodynamic language should have equivalent predictions. Over the last decade there has been great effort in developing equilibrium models for gene expression regulation <span class="citation" data-cites="Buchler2003 Vilar2011 Bintu2005a"> [<a href="#ref-Vilar2011" role="doc-biblioref">21</a>,<a href="#ref-Buchler2003" role="doc-biblioref">28</a>,<a href="#ref-Bintu2005a" role="doc-biblioref">30</a>]</span>. In particular our group has extensively characterized the simple repression motif using this formalism <span class="citation" data-cites="Garcia2011c Brewster2014 Razo-Mejia2018"> [<a href="#ref-Razo-Mejia2018" role="doc-biblioref">18</a>,<a href="#ref-Garcia2011c" role="doc-biblioref">35</a>,<a href="#ref-Brewster2014" role="doc-biblioref">36</a>]</span>.</p>
<p>The dialogue between theory and experiments has led to simplified expressions that capture the phenomenology of the gene expression response as a function of natural variables such as molecule count and affinities between molecular players. A particularly interesting quantity for the simple repression motif used by Garcia &amp; Phillips <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">35</a>]</span> is the fold-change in gene expression, defined as <span id="eq:ch5_eq31"><span class="math display">\[
\text{fold-change} =
\frac{\left\langle{\text{gene expression}(R \neq 0)}\right\rangle}{
\left\langle{\text{gene expression}(R = 0)}\right\rangle},
\qquad(31)\]</span></span> where <span class="math inline">\(R\)</span> is the number of repressors per cell and <span class="math inline">\(\left\langle{\cdot}\right\rangle\)</span> is the population average. The fold-change is simply the mean expression level in the presence of the repressor relative to the mean expression level in the absence of regulation. In the language of statistical mechanics this quantity takes the form <span id="eq:ch5_eq32"><span class="math display">\[
\text{fold-change} = \left( 1 + \frac{R}{N_{NS}}
e^{-\beta\Delta\varepsilon_r} \right)^{-1},
\qquad(32)\]</span></span> where <span class="math inline">\(\Delta\varepsilon_r\)</span> is the repressor-DNA binding energy, and as before <span class="math inline">\(N_{NS}\)</span> is the number of non-specific binding sites where the repressor can bind <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">35</a>]</span>.</p>
<p>To compute the fold-change in the chemical master equation language we compute the first moment of the steady sate mRNA distribution <span class="math inline">\(\langle m \rangle\)</span> for both the three-state promoter (<span class="math inline">\(R \neq 0\)</span>) and the two-state promoter case (<span class="math inline">\(R=0\)</span>) (See for moment derivation). The unregulated (two-state) promoter mean mRNA copy number is given by . For the regulated (three-state) promoter we have an equivalent expression of the form <span id="eq:ch5_eq33"><span class="math display">\[
\left\langle{m (R \neq 0)}\right\rangle = 
(2 - \phi)\frac{r_m}{\gamma _m}
\frac{k^{(r)}_{\text{off}}k^{(p)}_{\text{on}}
}{k^{(p)}_{\text{off}}k^{(r)}_{\text{off}} +
k^{(p)}_{\text{off}}k^{(r)}_{\text{on}} + 
k^{(r)}_{\text{off}}k^{(p)}_{\text{on}}}.
\qquad(33)\]</span></span> Computing the fold-change then gives <span id="eq:ch5_eq34"><span class="math display">\[
\text{fold-change} =
\frac{\left\langle{m (R \neq 0)}\right\rangle}
{\left\langle{m (R = 0)}\right\rangle} =
\frac{k^{(r)}_{\text{off}} \left( k^{(p)}_{\text{off}} + 
k^{(p)}_{\text{on}} \right)}{
k^{(p)}_{\text{off}}k^{(r)}_{\text{on}} +
k^{(r)}_{\text{off}} \left( k^{(p)}_{\text{off}} + k^{(p)}_{\text{on}} \right)},
\qquad(34)\]</span></span> where the factor <span class="math inline">\((2 - \phi)\)</span> due to the multiple promoter copies, the transcription rate <span class="math inline">\(r_m\)</span> and the mRNA degradation rate <span class="math inline">\(\gamma _m\)</span> cancel out.</p>
<p>Given that the number of repressors per cell <span class="math inline">\(R\)</span> is an experimental variable that we can control, we assume that the rate at which the promoter transitions from the transcriptionally inactive state to the repressor bound state, <span class="math inline">\(k^{(r)}_{\text{on}}\)</span>, is given by the concentration of repressors <span class="math inline">\([R]\)</span> times a diffusion limited on rate <span class="math inline">\(k_o\)</span>. For the diffusion limited constant <span class="math inline">\(k_o\)</span> we use the value used by Jones et al. <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">29</a>]</span>. With this in hand we can rewrite as <span id="eq:ch5_eq35"><span class="math display">\[
\text{fold-change} = \left( 1 + \frac{k_0 [R]}{k^{(r)}_{\text{off}}}
\frac{k^{(p)}_{\text{off}}}{k^{(p)}_{\text{on}} +
k^{(p)}_{\text{off}}} \right)^{-1}.
\qquad(35)\]</span></span></p>
<p>We note that both and have the same functional form. Therefore if both languages predict the same output for the mean gene expression level, it must be true that <span id="eq:ch5_eq36"><span class="math display">\[
\frac{k_o [R]}{k^{(r)}_{\text{off}}}
\frac{k^{(p)}_{\text{off}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}} =
\frac{R}{N_{NS}} e^{-\beta\Delta\varepsilon_r}.
\qquad(36)\]</span></span> Solving for <span class="math inline">\(k^{(r)}_{\text{off}}\)</span> gives <span id="eq:ch5_eq37"><span class="math display">\[
k^{(r)}_{\text{off}} = \frac{k_o [R] N_{NS}
e^{\beta\Delta\varepsilon_r}}{R}
\frac{k^{(p)}_{\text{off}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}.
\qquad(37)\]</span></span></p>
<p>Since the reported value of <span class="math inline">\(k_o\)</span> is given in units of nM<span class="math inline">\(^{-1}\)</span>s<span class="math inline">\(^{-1}\)</span> in order for the units to cancel properly the repressor concentration has to be given in nM rather than absolute count. If we consider that the repressor concentration is equal to <span id="eq:ch5_eq38"><span class="math display">\[ 
[R] = \frac{R}{V_{cell}}\cdot \frac{1}{N_A},
\qquad(38)\]</span></span> where <span class="math inline">\(R\)</span> is the absolute repressor copy number per cell, <span class="math inline">\(V_{cell}\)</span> is the cell volume and <span class="math inline">\(N_A\)</span> is Avogadro’s number. The <em>E. coli</em> cell volume is 2.1 fL <span class="citation" data-cites="Radzikowski2016"> [<a href="#ref-Radzikowski2016" role="doc-biblioref">25</a>]</span>, and Avogadro’s number is <span class="math inline">\(6.022 \times 10^{23}\)</span>. If we further include the conversion factor to turn M into nM we find that <span id="eq:ch5_eq39"><span class="math display">\[ 
[R] = \frac{R}{2.1 \times 10^{-15} L} \cdot \frac{1}{6.022 \times 10^{23}}
\cdot \frac{10^9 \text{ nmol}}{1 \text{ mol}} \approx 0.8 \times R.
\qquad(39)\]</span></span> Using this we simplify as <span id="eq:ch5_eq40"><span class="math display">\[
k^{(r)}_{\text{off}} \approx 0.8 \cdot k_o \cdot
N_{NS} e^{\beta\Delta\varepsilon_r}
\cdot \frac{k^{(p)}_{\text{off}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}.
\qquad(40)\]</span></span> What shows is the direct relationship that must be satisfied if the equilibrium model is set to be consistent with the non-equilibrium kinetic picture. summarizes the values obtained for the three operator sequences used throughout this work. To compute these numbers the number of non-specific binding sites <span class="math inline">\(N_{NS}\)</span> was taken to be <span class="math inline">\(4.6 \times 10^6\)</span> bp, i.e. the size of the <em>E. coli</em> K12 genome.</p>
<div id="tbl:ch5_tbl01">
<table>
<caption>Table 1: <strong>Binding sites and corresponding parameters.</strong></caption>
<thead>
<tr class="header">
<th>Operator</th>
<th><span class="math inline">\(\Delta\varepsilon_r\; (k_BT)\)</span></th>
<th><span class="math inline">\(k^{(r)}_{\text{off}}\; (s^{-1})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>O1</td>
<td>-15.3</td>
<td>0.002</td>
</tr>
<tr class="even">
<td>O2</td>
<td>-13.9</td>
<td>0.008</td>
</tr>
<tr class="odd">
<td>O3</td>
<td>-9.7</td>
<td>0.55</td>
</tr>
</tbody>
</table>
</div>
<p><em>In-vivo</em> measurements of the Lac repressor off rate have been done with single-molecule resolution <span class="citation" data-cites="Hammar2014"> [<a href="#ref-Hammar2014" role="doc-biblioref">14</a>]</span>. The authors report a mean residence time of <span class="math inline">\(5.3 \pm 0.2\)</span> minutes for the repressor on an O1 operator. The corresponding rate is <span class="math inline">\(k^{(r)}_{\text{off}} \approx 0.003\)</span> <span class="math inline">\((s^{-1})\)</span>, very similar value to what we inferred from our model. In this same reference the authors determined that on average the repressor takes <span class="math inline">\(30.9 \pm 0.5\)</span> seconds to bind to the operator <span class="citation" data-cites="Hammar2014"> [<a href="#ref-Hammar2014" role="doc-biblioref">14</a>]</span>. Given the kinetic model presented in (A) this time can be converted to the probability of not being on the repressor bound state <span class="math inline">\(P_{\text{not }R}\)</span>. This is computed as <span id="eq:ch5_eq41"><span class="math display">\[
P_{\text{not }R} = {\tau_{\text{not }R} \over
\tau_{\text{not }R} + \tau_{R}},
\qquad(41)\]</span></span> where <span class="math inline">\(\tau_{\text{not }R}\)</span> is the average time that the operator is not occupied by the repressor and <span class="math inline">\(\tau_{R}\)</span> is the average time that the repressor spends bound to the operator. Substituting the numbers from <span class="citation" data-cites="Hammar2014"> [<a href="#ref-Hammar2014" role="doc-biblioref">14</a>]</span> gives <span class="math inline">\(P_{\text{not }R} \approx 0.088\)</span>. From our model we can compute the zeroth moment <span class="math inline">\(\left\langle{m^0 p^0}\right\rangle\)</span> for each of the three promoter states. This moment is equivalent to the probability of being on each of the promoter states. Upon substitution of our inferred rate parameters we can compute <span class="math inline">\(P_{\text{not }R}\)</span> as <span id="eq:ch5_eq42"><span class="math display">\[
P_{\text{not }R} = 1 - P_R \approx 0.046,
\qquad(42)\]</span></span> where <span class="math inline">\(P_R\)</span> is the probability of the promoter being bound by the repressor. The value we obtained is within a factor of two from the one reported in <span class="citation" data-cites="Hammar2014"> [<a href="#ref-Hammar2014" role="doc-biblioref">14</a>]</span>.</p>
<h2 id="computing-moments-from-the-master-equation">Computing moments from the master equation</h2>
<p>In this section we will compute the moment equations for the distribution <span class="math inline">\(P(m, p)\)</span>. Without lost of generality here we will focus on the three-state regulated promoter. The computation of the moments for the two-state promoter follows the exact same procedure, changing only the definition of the matrices in the master equation.</p>
<h3 id="computing-moments-of-a-distribution">Computing moments of a distribution</h3>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/moment_dynamics_system.html">following link</a> as an annotated Jupyter notebook)</p>
<p>To compute any moment of our chemical master equation () let us define a vector <span id="eq:ch5_eq43"><span class="math display">\[
\left\langle \mathbf{m^x p^y} \right\rangle \equiv
(\left\langle m^x p^y \right\rangle_A, 
\left\langle m^x p^y \right\rangle_I, \left\langle m^x p^y \right\rangle_R)^T,
\qquad(43)\]</span></span> where <span class="math inline">\(\left\langle m^x p^y \right\rangle_S\)</span> is the expected value of <span class="math inline">\(m^x p^y\)</span> in state <span class="math inline">\(S \in \{A, I, R\}\)</span> with <span class="math inline">\(x, y \in \mathbb{N}\)</span>. In other words, just as we defined the vector <span class="math inline">\(\mathbf{P}(m, p)\)</span>, here we define a vector to collect the expected value of each of the promoter states. By definition, these moments <span class="math inline">\(\left\langle m^x p^y \right\rangle_S\)</span> are computed as <span id="eq:ch5_eq44"><span class="math display">\[
\left\langle m^x p^y \right\rangle_S \equiv 
\sum_{m=0}^\infty \sum_{p=0}^\infty m^x p^y P_S(m, p).
\qquad(44)\]</span></span> To simplify the notation, let <span class="math inline">\(\sum_x \equiv \sum_{x=0}^\infty\)</span>. Since we are working with a system of three ODEs, one for each state, let us define the following operation: <span id="eq:ch5_eq45"><span class="math display">\[
\left\langle \mathbf{m^x p^y} \right\rangle =
\sum_m \sum_p m^x p^y \mathbf{P}(m, p) \equiv
  \begin{bmatrix}
    \sum_m \sum_p m^x p^y P_A(m, p)\\
    \sum_m \sum_p m^x p^y P_I(m, p)\\
    \sum_m \sum_p m^x p^y P_R(m, p)\\
  \end{bmatrix}.
\qquad(45)\]</span></span></p>
<p>With this in hand we can then apply this sum over <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> to . For the left-hand side we have <span id="eq:ch5_eq46"><span class="math display">\[
\sum_m \sum_p m^x p^y \frac{d \mathbf{P}(m, p)}{dt} = 
\frac{d}{dt}\left[ \sum_m \sum_p m^x p^y \mathbf{P}(m, p) \right],
\qquad(46)\]</span></span> where we made use of the linearity property of the derivative to switch the order between the sum and the derivative. Notice that the right-hand side of contains the definition of a moment from XXX. That means that we can rewrite it as <span id="eq:ch5_eq47"><span class="math display">\[
\frac{d}{dt}\left[ \sum_m \sum_p m^x p^y \mathbf{P}(m, p) \right] = 
\frac{d \mathbf{\left\langle m^x p^y \right\rangle}}{dt}.
\qquad(47)\]</span></span></p>
<p>Distributing the sum on the right-hand side of gives <span id="eq:ch5_eq48"><span class="math display">\[
\begin{split}
    \frac{d \mathbf{\left\langle m^x p^y \right\rangle}}{dt} &amp;=
    \mathbf{K} \sum_m \sum_p m^x p^y \mathbf{P}(m, p)\\
    &amp;- \mathbf{R}_m \sum_m \sum_p m^x p^y \mathbf{P}(m, p) + 
    \mathbf{R}_m \sum_m \sum_p m^x p^y \mathbf{P}(m-1, p)\\
    &amp;- \mathbf{\Gamma}_m \sum_m \sum_p (m) m^x p^y \mathbf{P}(m, p) + 
    \mathbf{\Gamma}_m \sum_m \sum_p (m + 1) m^x p^y \mathbf{P}(m + 1, p)\\
    &amp;- \mathbf{R}_p \sum_m \sum_p (m) m^x p^y \mathbf{P}(m, p) + 
    \mathbf{R}_p \sum_m \sum_p (m) m^x p^y \mathbf{P}(m, p - 1)\\
    &amp;- \mathbf{\Gamma}_p \sum_m \sum_p (p) m^x p^y \mathbf{P}(m, p) + 
    \mathbf{\Gamma}_p \sum_m \sum_p (p + 1) m^x p^y \mathbf{P}(m, p + 1).
  \end{split}
\qquad(48)\]</span></span></p>
<p>Let’s look at each term on the right-hand side individually. For the terms in involving <span class="math inline">\(\mathbf{P}(m, p)\)</span> we can again use to rewrite them in a more compact form. This means that we can rewrite the state transition term as <span id="eq:ch5_eq49"><span class="math display">\[
\mathbf{K} \sum_m \sum_p m^x p^y \mathbf{P}(m, p) = 
\mathbf{K} \mathbf{\left\langle m^x p^y \right\rangle}.
\qquad(49)\]</span></span> The mRNA production term involving <span class="math inline">\(\mathbf{P}(m, p)\)</span> can be rewritten as <span id="eq:ch5_eq50"><span class="math display">\[
\mathbf{R}_m \sum_m \sum_p m^x p^y \mathbf{P}(m, p) = 
\mathbf{R}_m \mathbf{\left\langle m^x p^y \right\rangle}.
\qquad(50)\]</span></span> In the same way the mRNA degradation term gives <span id="eq:ch5_eq51"><span class="math display">\[
\mathbf{\Gamma}_m \sum_m \sum_p (m) m^x p^y \mathbf{P}(m, p) = 
\mathbf{\Gamma}_m \mathbf{\left\langle{m^{(x + 1)} p^y}\right\rangle}.
\qquad(51)\]</span></span> For the protein production and degradation terms involving <span class="math inline">\(\mathbf{P}(m, p)\)</span> we have <span id="eq:ch5_eq52"><span class="math display">\[
\mathbf{R}_p \sum_m \sum_p (m) m^x p^y \mathbf{P}(m, p) = 
\mathbf{R}_p \mathbf{\left\langle{m^{(x + 1)} p^y}\right\rangle},
\qquad(52)\]</span></span> and <span id="eq:ch5_eq53"><span class="math display">\[
\mathbf{\Gamma}_p \sum_m \sum_p (p) m^x p^y \mathbf{P}(m, p) = 
\mathbf{\Gamma}_p \mathbf{\left\langle{m^x p^{(y + 1)}}\right\rangle},
\qquad(53)\]</span></span> respectively.</p>
<p>For the sums terms in involving <span class="math inline">\(\mathbf{P}(m \pm 1, p)\)</span> or <span class="math inline">\(\mathbf{P}(m, p \pm 1)\)</span> we can reindex the sum to work around this mismatch. To be more specific let’s again look at each term case by case. For the mRNA production term involving <span class="math inline">\(\mathbf{P}(m-1, p)\)</span> we define <span class="math inline">\(m&#39; \equiv m - 1\)</span>. Using this we write <span id="eq:ch5_eq54"><span class="math display">\[
\mathbf{R}_m \sum_m \sum_p m^x p^y \mathbf{P}(m-1, p) =
\mathbf{R}_m \sum_{m&#39; = -1}^\infty \sum_p (m&#39; + 1)^x p^y \mathbf{P}(m&#39;, p).
\qquad(54)\]</span></span> Since having negative numbers of mRNA or protein doesn’t make physical sense we have that <span class="math inline">\(\mathbf{P}(-1, p) = 0\)</span>. Therefore we can rewrite the sum starting from 0 rather than from -1, obtaining <span id="eq:ch5_eq55"><span class="math display">\[
\mathbf{R}_m \sum_{m&#39; = -1}^\infty \sum_p (m&#39; + 1)^x p^y \mathbf{P}(m&#39;, p) =
\mathbf{R}_m \sum_{m&#39;=0}^\infty \sum_p (m&#39; + 1)^x p^y \mathbf{P}(m&#39;, p).
\qquad(55)\]</span></span> Recall that our distribution <span class="math inline">\(\mathbf{P}(m, p)\)</span> takes <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> as numerical inputs and returns a probability associated with such a molecule count. Nevertheless, <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> themselves are dimensionless quantities that serve as indices of how many molecules are in the cell. The distribution is the same whether the variable is called <span class="math inline">\(m\)</span> or <span class="math inline">\(m&#39;\)</span>; for a specific number, let’s say <span class="math inline">\(m = 5\)</span>, or <span class="math inline">\(m&#39; = 5\)</span>, <span class="math inline">\(\mathbf{P}(5, p)\)</span> will return the same result. This means that the variable name is arbitrary, and the right-hand side of can be written as <span id="eq:ch5_eq56"><span class="math display">\[
\mathbf{R}_m \sum_{m&#39;=0}^\infty \sum_p (m&#39; + 1)^x p^y \mathbf{P}(m&#39;, p) =
\mathbf{R}_m \mathbf{\left\langle{(m+1)^x p^y}\right\rangle},
\qquad(56)\]</span></span> since the left-hand side corresponds to the definition of a moment.</p>
<p>For the mRNA degradation term involving <span class="math inline">\(\mathbf{P}(m + 1, p)\)</span> we follow a similar procedure in which we define <span class="math inline">\(m&#39; = m + 1\)</span> to obtain <span id="eq:ch5_eq57"><span class="math display">\[
\mathbf{\Gamma}_m \sum_m \sum_p (m + 1) m^x p^y \mathbf{P}(m + 1, p) =
\mathbf{\Gamma}_m \sum_{m&#39; = 1}^\infty \sum_p m&#39; 
(m&#39; - 1)^x p^y \mathbf{P}(m&#39;, p).
\qquad(57)\]</span></span> In this case since the term on the right-hand side of the equation is multiplied by <span class="math inline">\(m&#39;\)</span>, starting the sum over <span class="math inline">\(m&#39;\)</span> from 0 rather than from 1 will not affect the result since this factor will not contribute to the total sum. Nevertheless this is useful since our definition of a moment from requires the sum to start at zero. This means that we can rewrite this term as <span id="eq:ch5_eq58"><span class="math display">\[
\mathbf{\Gamma}_m \sum_{m&#39; = 1}^\infty m&#39; \sum_p 
(m&#39; - 1)^x p^y \mathbf{P}(m&#39;, p) =
\mathbf{\Gamma}_m \sum_{m&#39; = 0}^\infty m&#39; \sum_p 
(m&#39; - 1)^x p^y \mathbf{P}(m&#39;, p).
\qquad(58)\]</span></span> Here again we can change the arbitrary label <span class="math inline">\(m&#39;\)</span> back to <span class="math inline">\(m\)</span> obtaining <span id="eq:ch5_eq59"><span class="math display">\[
\mathbf{\Gamma}_m \sum_{m&#39; = 0}^\infty m&#39; \sum_p 
(m&#39; - 1)^x p^y \mathbf{P}(m&#39;, p) =
\mathbf{\Gamma}_m \mathbf{\left\langle{m (m - 1)^x p^y}\right\rangle}.
\qquad(59)\]</span></span></p>
<p>The protein production term involving <span class="math inline">\(\mathbf{P}(m, p - 1)\)</span> can be reindexed by defining <span class="math inline">\(p&#39; \equiv p - 1\)</span>. This gives <span id="eq:ch5_eq60"><span class="math display">\[
\mathbf{R}_p \sum_m \sum_p (m) m^x p^y \mathbf{P}(m, p - 1) =
\mathbf{R}_p \sum_m \sum_{p&#39;=-1}^\infty 
m^{(x + 1)} (p + 1)^y \mathbf{P}(m, p&#39;).
\qquad(60)\]</span></span> We again use the fact that negative molecule copy numbers are assigned with probability zero to begin the sum from 0 rather than -1 and the arbitrary nature of the label <span class="math inline">\(p&#39;\)</span> to write <span id="eq:ch5_eq61"><span class="math display">\[
\mathbf{R}_p \sum_m \sum_{p&#39;=0}^\infty m^{(x + 1)} (p + 1)^y \mathbf{P}(m, p&#39;) =
\mathbf{R}_p \mathbf{\left\langle{m^{(x + 1)} (p + 1)^y}\right\rangle}.
\qquad(61)\]</span></span> Finally, we take care of the protein degradation term involving <span class="math inline">\(\mathbf{P}(m, p + 1)\)</span>. As before, we define <span class="math inline">\(p&#39; = p + 1\)</span> and substitute this to obtain <span id="eq:ch5_eq62"><span class="math display">\[
\mathbf{\Gamma}_p \sum_m \sum_p (p + 1) m^x p^y \mathbf{P}(m, p + 1) =
\mathbf{\Gamma}_p \sum_m \sum_{p&#39;=1}^\infty 
(p&#39;) m^x (p&#39; - 1)^y \mathbf{P}(m, p&#39;).
\qquad(62)\]</span></span> Just as with the mRNA degradation term, having a term <span class="math inline">\(p&#39;\)</span> inside the sum allows us to start the sum over <span class="math inline">\(p&#39;\)</span> from 0 rather than 1. We can therefore write <span id="eq:ch5_eq63"><span class="math display">\[
\mathbf{\Gamma}_p \sum_m \sum_{p&#39;=0}^\infty 
(p&#39;) m^x (p&#39; - 1)^y \mathbf{P}(m, p&#39;) =
\mathbf{\Gamma}_p \mathbf{\left\langle{m^x p (p - 1)^y}\right\rangle}.
\qquad(63)\]</span></span></p>
<p>Putting all these terms together we can write the general moment ODE. This is of the form <span id="eq:ch5_eq64"><span class="math display">\[
\begin{split}
    \frac{d\mathbf{\left\langle m^x p^y \right\rangle}}{dt} &amp;=
    \mathbf{K} \mathbf{\left\langle m^x p^y \right\rangle}
    \text{  (promoter state transition)}\\
    &amp;- \mathbf{R}_m \mathbf{\left\langle m^x p^y \right\rangle} +
    \mathbf{R}_m \mathbf{\left\langle{(m+1)^x p^y}\right\rangle}
    \text{  (mRNA production)}\\
    &amp;- \mathbf{\Gamma}_m \mathbf{\left\langle{m^{(x + 1)} p^y}\right\rangle} + 
    \mathbf{\Gamma}_m \mathbf{\left\langle{m (m - 1)^x p^y}\right\rangle}
    \text{  (mRNA degradation)}\\
    &amp;- \mathbf{R}_p \mathbf{\left\langle{m^{(x + 1)} p^y}\right\rangle} + 
    \mathbf{R}_p \mathbf{\left\langle{m^{(x + 1)} (p + 1)^y}\right\rangle}
    \text{  (protein production)}\\
    &amp;- \mathbf{\Gamma}_p \mathbf{\left\langle{m^x p^{(y + 1)}}\right\rangle} + 
    \mathbf{\Gamma}_p \mathbf{\left\langle{m^x p (p - 1)^y}\right\rangle}
    \text{  (protein degradation)}.
  \end{split}
\qquad(64)\]</span></span></p>
<h3 id="moment-closure-of-the-simple-repression-distribution">Moment closure of the simple-repression distribution</h3>
<p>A very interesting and useful feature of is that for a given value of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> the moment <span class="math inline">\(\mathbf{\left\langle m^x p^y \right\rangle}\)</span> is only a function of lower moments. Specifically <span class="math inline">\(\mathbf{\left\langle m^x p^y \right\rangle}\)</span> is a function of moments <span class="math inline">\(\mathbf{\left\langle{m^{x&#39;} p^{y&#39;}}\right\rangle}\)</span> that satisfy two conditions: <span id="eq:ch5_eq65"><span class="math display">\[
\begin{split}
    &amp;1) y&#39; \leq y,\\
  &amp;2) x&#39; + y&#39; \leq x + y.
\end{split}
\qquad(65)\]</span></span></p>
<p>To prove this we rewrite as <span id="eq:ch5_eq66"><span class="math display">\[
\begin{split}
    \frac{d\mathbf{\left\langle m^x p^y \right\rangle}}{dt} &amp;=
    \mathbf{K} \mathbf{\left\langle m^x p^y \right\rangle}\\
    &amp;+ \mathbf{R}_m 
    \mathbf{\left\langle{p^y \left[ (m + 1)^x -m^x \right]}\right\rangle}\\
    &amp;+ \mathbf{\Gamma}_m 
    \mathbf{\left\langle{m p^y \left[ (m - 1)^x - m^x \right]}\right\rangle}\\
    &amp;+ \mathbf{R}_p 
    \mathbf{\left\langle{m^{(x + 1)}
    \left[ (p + 1)^y - p^y \right]}\right\rangle}\\
    &amp;+ \mathbf{\Gamma}_p 
    \mathbf{\left\langle{m^x p \left[ (p - 1)^y - p^y \right]}\right\rangle},
\end{split}
\qquad(66)\]</span></span> where the factorization is valid given the linearity of expected values. Now the objective is to find the highest moment for each term once the relevant binomial, such as <span class="math inline">\((m-1)^x\)</span>, is expanded. Take, for example, a simple case in which we want to find the second moment of the mRNA distribution. We then set <span class="math inline">\(x = 2\)</span> and <span class="math inline">\(y = 0\)</span>. then becomes <span id="eq:ch5_eq67"><span class="math display">\[
\begin{split}
    \frac{\mathbf{\left\langle{m^2 p^0}\right\rangle}}{dt} &amp;=
    \mathbf{K} \mathbf{\left\langle{m^2 p^0}\right\rangle}\\
    &amp;+ \mathbf{R}_m 
    \mathbf{\left\langle{p^0 \left[ (m + 1)^2 - m^2 \right]}\right\rangle}\\
    &amp;+ \mathbf{\Gamma}_m
    \mathbf{\left\langle{m p^0 \left[ (m - 1)^2 - m^2 \right]}\right\rangle}\\
    &amp;+ \mathbf{R}_p 
    \mathbf{\left\langle{m^{(2 + 1)} 
    \left[ (p + 1)^0 - p^0 \right]}\right\rangle}\\
    &amp;+ \mathbf{\Gamma}_p 
    \mathbf{\left\langle{m^2 p \left[ (p - 1)^0 - p^0 \right]}\right\rangle}.
\end{split}
\qquad(67)\]</span></span> Simplifying this equation gives <span id="eq:ch5_eq68"><span class="math display">\[
\frac{d \mathbf{\left\langle{m^2}\right\rangle}}{dt} =
\mathbf{K} 
\mathbf{\left\langle{m^2}\right\rangle}
+ \mathbf{R}_m 
\mathbf{\left\langle{\left[ 2m + 1 \right]}\right\rangle}
+ \mathbf{\Gamma}_m 
\mathbf{\left\langle{\left[- 2m^2 + m \right]}\right\rangle}.
\qquad(68)\]</span></span></p>
<p>XXX satisfies both of our conditions. Since we set <span class="math inline">\(y\)</span> to be zero, none of the terms depend on any moment that involves the protein number, therefore <span class="math inline">\(y&#39; \leq y\)</span> is satisfied. Also the highest moment in also satisfies <span class="math inline">\(x&#39; + y&#39; \leq x + y\)</span> since the second moment of mRNA doesn’t depend on any moment higher than <span class="math inline">\(\mathbf{\left\langle{m^2}\right\rangle}\)</span>. To demonstrate that this is true for any <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> we now rewrite , making use of the binomial expansion <span id="eq:ch5_eq69"><span class="math display">\[
(z \pm 1)^n = \sum_{k=0}^n {n \choose k} (\pm 1)^{k} z^{n-k}.
\qquad(69)\]</span></span> Just as before let’s look at each term individually. For the mRNA production term we have <span id="eq:ch5_eq70"><span class="math display">\[
\mathbf{R}_m 
\mathbf{\left\langle{p^y \left[ (m + 1)^x -m^x \right]}\right\rangle} =
\mathbf{R}_m 
\mathbf{\left\langle{p^y 
\left[ \sum_{k=0}^x {x \choose k} m^{x-k} - m^x \right]}\right\rangle}.
\qquad(70)\]</span></span> When <span class="math inline">\(k = 0\)</span>, the term inside the sum on the right-hand side cancels with the other <span class="math inline">\(m^x\)</span>, so we can simplify to <span id="eq:ch5_eq71"><span class="math display">\[
\mathbf{R}_m 
\mathbf{\left\langle{p^y \left[ (m + 1)^x -m^x \right]}\right\rangle} =
\mathbf{R}_m 
\mathbf{\left\langle{p^y 
\left[ \sum_{k=1}^x {x \choose k} m^{x-k} \right]}\right\rangle}.
\qquad(71)\]</span></span> Once the sum is expanded we can see that the highest moment in this sum is given by <span class="math inline">\(\mathbf{\left\langle{m^{(x-1)} p^y}\right\rangle}\)</span> which satisfies both of the conditions on XXX.</p>
<p>For the mRNA degradation term we similarly have <span id="eq:ch5_eq72"><span class="math display">\[
\mathbf{\Gamma}_m 
\mathbf{\left\langle{m p^y \left[ (m - 1)^x - m^x \right]}\right\rangle} =
\mathbf{\Gamma}_m 
\mathbf{\left\langle{m p^y 
\left[ \sum_{k=0}^x {x \choose k}(-1)^k m^{x-k} - m^x \right]}\right\rangle}.
\qquad(72)\]</span></span> Simplifying terms we obtain <span id="eq:ch5_eq73"><span class="math display">\[
\mathbf{\Gamma}_m 
\mathbf{\left\langle{m p^y \left[ \sum_{k=0}^x {x \choose k}(-1)^k m^{x-k} -
m^x \right]}\right\rangle} =
\mathbf{\Gamma}_m 
\mathbf{\left\langle{p^y 
\left[ \sum_{k=1}^x {x \choose k}(-1)^k m^{x+1-k} \right]}\right\rangle}.
\qquad(73)\]</span></span> The largest moment in this case is <span class="math inline">\(\mathbf{\left\langle{m^x p^y }\right\rangle}\)</span>, which again satisfies the conditions on XXX.</p>
<p>The protein production term gives <span id="eq:ch5_eq74"><span class="math display">\[
\mathbf{R}_p 
\mathbf{\left\langle{m^{(x + 1)} \left[ (p + 1)^y - p^y \right]}\right\rangle} =
\mathbf{R}_p \mathbf{\left\langle{m^{(x + 1)}
\left[ \sum_{k=0}^y {y \choose k} (-1)^k p^{y-k} - p^y \right]}\right\rangle}.
\qquad(74)\]</span></span> Upon simplification we obtain <span id="eq:ch5_eq75"><span class="math display">\[
\mathbf{R}_p 
\mathbf{\left\langle{m^{(x + 1)} 
\left[ \sum_{k=0}^y {y \choose k} (-1)^k p^{y-k} - p^y \right]}\right\rangle} =
\mathbf{R}_p 
\mathbf{\left\langle{m^{(x + 1)} 
\left[ \sum_{k=1}^y {y \choose k}(-1)^k p^{y-k} \right]}\right\rangle}.
\qquad(75)\]</span></span> Here the largest moment is given by <span class="math inline">\(\mathbf{\left\langle{m^{x+1} p^{y-1}}\right\rangle}\)</span>, that again satisfies both of our conditions. For the last term, for protein degradation we have <span id="eq:ch5_eq76"><span class="math display">\[
\mathbf{R}_p 
\mathbf{\left\langle{m^{(x + 1)} \left[ (p + 1)^y - p^y \right]}\right\rangle} =
\mathbf{R}_p 
\mathbf{\left\langle{m^{(x + 1)} \left[ \sum_{k=1}^y {y \choose k} (-1^k) p^{y - k}
  \right]}\right\rangle}.
\qquad(76)\]</span></span> The largest moment involved in this term is therefore <span class="math inline">\(\mathbf{\left\langle{m^x p^{y-1}}\right\rangle}\)</span>. With this, we show that the four terms involved in our general moment equation depend only on lower moments that satisfy XXX.</p>
<p>As a reminder, what we showed in this section is that the kinetic model introduced in (A) has no moment-closure problem. In other words, moments of the joint mRNA and protein distribution can be computed just from knowledge of lower moments. This allows us to cleanly integrate the moments of the distribution dynamics as cells progress through the cell cycle.</p>
<h3 id="computing-single-promoter-steady-state-moments">Computing single promoter steady-state moments</h3>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/chemical_master_steady_state_moments_general.html">following link</a> as an annotated Jupyter notebook)</p>
<p>As discussed in XXX, one of the main factors contributing to cell-to-cell variability in gene expression is the change in gene copy number during the cell cycle as cells replicate their genome before cell division. Our minimal model accounts for this variability by considering the time trajectory of the distribution moments as given by . These predictions will be contrasted with the predictions from a kinetic model that doesn’t account for changes in gene copy number during the cell cycle in XXX.</p>
<p>If we do not account for change in gene copy number during the cell cycle or for the partition of proteins during division, the dynamics of the moments of the distribution described in this section will reach a steady state. In order to compute the steady-state moments of the kinetic model with a single gene across the cell cycle, we use the moment closure property of our master equation. By equating to zero for a given <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, we can solve the resulting linear system and obtain a solution for <span class="math inline">\(\mathbf{\left\langle m^x p^y \right\rangle}\)</span> at steady state as a function of moments <span class="math inline">\(\mathbf{\left\langle{m^{x&#39;} p^{y&#39;}}\right\rangle}\)</span> that satisfy . Then, by solving for the zero<span class="math inline">\(^\text{th}\)</span> moment <span class="math inline">\(\mathbf{\left\langle{m^0 p^0}\right\rangle}\)</span> subject to the constraint that the probability of the promoter being in any state should add up to one, we can substitute back all of the solutions in terms of moments <span class="math inline">\(\mathbf{\left\langle{m^{x&#39;} p^{y&#39;}}\right\rangle}\)</span> with solutions in terms of the rates shown in . In other words, through an iterative process, we can get at the value of any moment of the distribution. We start by solving for the zero<span class="math inline">\(^\text{th}\)</span> moment. Since all higher moments, depend on lower moments we can use the solution of the zero<span class="math inline">\(^\text{th}\)</span> moment to compute the first mRNA moment. This solution is then used for higher moments in a hierarchical iterative process.</p>
<h2 id="accounting-for-the-variability-in-gene-copy-number-during-the-cell-cycle">Accounting for the variability in gene copy number during the cell cycle</h2>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu/chann_cap/src/theory/html/moment_dynamics_cell_division.html">following link</a> as an annotated Jupyter notebook)</p>
<p>When growing in rich media, bacteria can double every <span class="math inline">\(\approx\)</span> 20 minutes. With two replication forks each traveling at <span class="math inline">\(\approx\)</span> 1000 bp per second, and a genome of <span class="math inline">\(\approx\)</span> 5 Mbp for <em>E. coli</em> <span class="citation" data-cites="Moran2010"> [<a href="#ref-Moran2010" role="doc-biblioref">24</a>]</span>, a cell would need <span class="math inline">\(\approx\)</span> 40 minutes to replicate its genome. The apparent paradox of growth rates faster than one division per 40 minutes is solved by the fact that cells have multiple replisomes, i.e. molecular machines that replicate the genome running in parallel. Cells can have up to 8 copies of the genome being replicated simultaneously depending on the growth rate <span class="citation" data-cites="Bremer1996"> [<a href="#ref-Bremer1996" role="doc-biblioref">17</a>]</span>.</p>
<p>This observation implies that during the cell cycle gene copy number varies. This variation depends on the growth rate and the relative position of the gene with respect to the replication origin, having genes close to the replication origin spending more time with multiple copies compare to genes closer to the replication termination site. This change in gene dosage has a direct effect on the cell-to-cell variability in gene expression <span class="citation" data-cites="Jones2014a Peterson2015"> [<a href="#ref-Peterson2015" role="doc-biblioref">6</a>,<a href="#ref-Jones2014a" role="doc-biblioref">29</a>]</span>.</p>
<h3 id="numerical-integration-of-moment-equations">Numerical integration of moment equations</h3>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/moment_dynamics_cell_division.html">following link</a> as an annotated Jupyter notebook)</p>
<p>For our specific locus (<em>galK</em>) and a doubling time of <span class="math inline">\(\approx\)</span> 60 min for our experimental conditions, cells have on average 1.66 copies of the reporter gene during the cell cycle <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">29</a>]</span>. What this means is that cells spend 60% of the time having one copy of the gene and 40% of the time with two copies. To account for this variability in gene copy number across the cell cycle we numerically integrate the moment equations derived in for a time <span class="math inline">\(t = [0, t_s]\)</span> with an mRNA production rate <span class="math inline">\(r_m\)</span>, where <span class="math inline">\(t_s\)</span> is the time point at which the replication fork reaches our specific locus. For the remaining time before the cell division <span class="math inline">\(t = [t_s, t_d]\)</span> that the cell spends with two promoters, we assume that the only parameter that changes is the mRNA production rate from <span class="math inline">\(r_m\)</span> to <span class="math inline">\(2 r_m\)</span>. This simplifying assumption ignores potential changes in protein translation rate <span class="math inline">\(r_p\)</span> or changes in the repressor copy number that would be reflected in changes on the repressor on rate <span class="math inline">\(k^{(r)}_{\text{on}}\)</span>.</p>
<h3 id="computing-distribution-moments-after-cell-division">Computing distribution moments after cell division</h3>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/binomial_moments.html">following link</a> as an annotated Jupyter notebook)</p>
<p>We have already solved a general form for the dynamics of the moments of the distribution, i.e. we wrote differential equations for the moments <span class="math inline">\(\frac{d\left\langle{m^x p^y}\right\rangle}{dt}\)</span>. Given that we know all parameters for our model we can simply integrate these equations numerically to compute how the moments of the distribution evolve as cells progress through their cell cycle. Once the cell reaches a time <span class="math inline">\(t_d\)</span> when is going to divide the mRNA and proteins that we are interested in undergo a binomial partitioning between the two daughter cells. In other words, each molecule flips a coin and decides whether to go to either daughter. The question then becomes given that we have a value for the moment <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_d}\)</span> at a time before the cell division, what would the value of this moment be after the cell division takes place <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_o}\)</span>?</p>
<p>The probability distribution of mRNA and protein after the cell division <span class="math inline">\(P_{t_o}(m, p)\)</span> must satisfy <span id="eq:ch5_eq77"><span class="math display">\[
P_{t_o}(m, p) = \sum_{m&#39;=m}^\infty \sum_{p&#39;=p}^\infty 
P(m, p \mid m&#39;, p&#39;) P_{t_d}(m&#39;, p&#39;),
\qquad(77)\]</span></span> where we are summing over all the possibilities of having <span class="math inline">\(m&#39;\)</span> mRNA and <span class="math inline">\(p&#39;\)</span> proteins before cell division. Note that the sums start at <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span>; this is because for a cell to have these copy numbers before cell division it is a requirement that the mother cell had at least such copy number since we are not assuming that there is any production at the instantaneous cell division time. Since we assume that the partition of mRNA is independent from the partition of protein, the conditional probability <span class="math inline">\(P(m, p \mid m&#39;, p&#39;)\)</span> is simply given by a product of two binomial distributions, one for the mRNA and one for the protein, i.e. <span id="eq:ch5_eq78"><span class="math display">\[
P(m, p \mid m&#39;, p&#39;) = {m&#39; \choose m} \left( \frac{1}{2} \right)^{m&#39;} \cdot
                      {p&#39; \choose p} \left( \frac{1}{2} \right)^{p&#39;}.
\qquad(78)\]</span></span> Because of these product of binomial probabilities are allowed to extend the sum from to start at <span class="math inline">\(m&#39;=0\)</span> and <span class="math inline">\(p&#39;=0\)</span> as <span id="eq:ch5_eq79"><span class="math display">\[
P_{t_o}(m, p) = \sum_{m&#39;=0}^\infty \sum_{p&#39;=0}^\infty 
                  P(m, p \mid m&#39;, p&#39;) P_{t_d}(m&#39;, p&#39;),
\qquad(79)\]</span></span> since the product of the binomial distributions in is zero for all <span class="math inline">\(m&#39; &lt; m\)</span> and/or <span class="math inline">\(p&#39; &lt; 0\)</span>. So from now on in this section we will assume that a sum of the form <span class="math inline">\(\sum_x \equiv \sum_{x=0}^\infty\)</span> to simplify notation.</p>
<p>We can then compute the distribution moments after the cell division <span class="math inline">\(\left\langle{m^x p^y}\right\rangle_{t_o}\)</span> as <span id="eq:ch5_eq80"><span class="math display">\[
\left\langle m^x p^y \right\rangle_{t_o} = 
\sum_m \sum_p m^x p^y P_{t_o}(m, p),
\qquad(80)\]</span></span> for all <span class="math inline">\(x, y \in \mathbb{N}\)</span>. Substituting results in <span id="eq:ch5_eq81"><span class="math display">\[
\left\langle m^x p^y \right\rangle_{t_o} = \sum_m \sum_p m^x p^y
\sum_{m&#39;} \sum_{p&#39;} P(m, p \mid m&#39;, p&#39;) P_{t_d}(m&#39;, p&#39;).
\qquad(81)\]</span></span> We can rearrange the sums to be <span id="eq:ch5_eq82"><span class="math display">\[
\left\langle m^x p^y \right\rangle_{t_o} = \sum_{m&#39;} \sum_{p&#39;} P_{t_d}(m&#39;, p&#39;)
                     \sum_m \sum_p m^x p^y P(m, p \mid m&#39;, p&#39;).
\qquad(82)\]</span></span> The fact that is the product of two independent events allows us to rewrite the joint probability <span class="math inline">\(P(m, p \mid m&#39;, p&#39;)\)</span> as <span id="eq:ch5_eq83"><span class="math display">\[
P(m, p \mid m&#39;, p&#39;) = P(m \mid m&#39;) \cdot P(p \mid p&#39;).
\qquad(83)\]</span></span> With this we can then write the moment <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_o}\)</span> as <span id="eq:ch5_eq84"><span class="math display">\[
\left\langle m^x p^y \right\rangle_{t_o} = \sum_{m&#39;} \sum_{p&#39;} P_{t_d}(m&#39;, p&#39;)
                     \sum_m  m^x  P(m \mid m&#39;)
                     \sum_p p^y P(p \mid p&#39;).
\qquad(84)\]</span></span> Notice that both terms summing over <span class="math inline">\(m\)</span> and over <span class="math inline">\(p\)</span> are the conditional expected values, i.e. <span id="eq:ch5_eq85"><span class="math display">\[
\sum_z  z^x  P(z \mid z&#39;) \equiv \left\langle{z^x \mid z&#39;}\right\rangle, \; 
{\text{ for } z\in \{m, p \}}.
\qquad(85)\]</span></span> These conditional expected values are the expected values of a binomial random variable <span class="math inline">\(z \sim \text{Bin}(z&#39;, 1/2)\)</span>, which can be easily computed as we will show later in this section. We then rewrite the expected values after the cell division in terms of these moments of a binomial distribution <span id="eq:ch5_eq86"><span class="math display">\[
\left\langle m^x p^y \right\rangle_{t_o} = 
\sum_{m&#39;} \sum_{p&#39;} 
\left\langle{m^x \mid m&#39;}\right\rangle \left\langle{p^y \mid p&#39;}\right\rangle
P_{t_d}(m&#39;, p&#39;).
\qquad(86)\]</span></span></p>
<p>To see how this general formula for the moments after the cell division works let’s compute the mean protein per cell after the cell division <span class="math inline">\(\left\langle{p}\right\rangle_{t_o}\)</span>. That is setting <span class="math inline">\(x = 0\)</span>, and <span class="math inline">\(y = 1\)</span>. This results in <span id="eq:ch5_eq87"><span class="math display">\[
\left\langle{p}\right\rangle_{t_o} = 
\sum_{m&#39;} \sum_{p&#39;} 
\left\langle{m^0 \mid m&#39;}\right\rangle \left\langle{p \mid p&#39;}\right\rangle
P_{t_d}(m&#39;, p&#39;).
\qquad(87)\]</span></span> The zeroth moment <span class="math inline">\(\left\langle{m^0 \mid m&#39;}\right\rangle\)</span> by definition must be one since we have <span id="eq:ch5_eq88"><span class="math display">\[
\left\langle{m^0 \mid m&#39;}\right\rangle = 
\sum_m m^0 P(m \mid m&#39;) = 
\sum_m P(m \mid m&#39;) = 1,
\qquad(88)\]</span></span> since the probability distribution must be normalized. This leaves us then with <span id="eq:ch5_eq89"><span class="math display">\[
\left\langle{p}\right\rangle_{t_o} = 
\sum_{m&#39;} \sum_{p&#39;} P_{t_d}(m&#39;, p&#39;) \left\langle p \mid p&#39; \right\rangle.
\qquad(89)\]</span></span> If we take the sum over <span class="math inline">\(m&#39;\)</span> we simply compute the marginal probability distribution <span class="math inline">\(\sum_{m&#39;} P_{t_d}(m&#39;, p&#39;) = P_{t_d}(p&#39;)\)</span>, then we have <span id="eq:ch5_eq90"><span class="math display">\[
\left\langle p \right\rangle_{t_o} = 
\sum_{p&#39;} \left\langle p \mid p&#39; \right\rangle P_{t_d}(p&#39;).
\qquad(90)\]</span></span> For the particular case of the first moment of the binomial distribution with parameters <span class="math inline">\(p&#39;\)</span> and <span class="math inline">\(1/2\)</span> we know that <span id="eq:ch5_eq91"><span class="math display">\[
\left\langle p \mid p&#39; \right\rangle = \frac{p&#39;}{2}.
\qquad(91)\]</span></span> Therefore the moment after division is equal to <span id="eq:ch5_eq92"><span class="math display">\[
\left\langle p \right\rangle_{t_o} = 
\sum_{p&#39;} \frac{p&#39;}{2} P_{t_d}(p&#39;)
= \frac{1}{2} \sum_{p&#39;} p&#39; P_{t_d}(p&#39;).
\qquad(92)\]</span></span> Notice that this is just 1/2 of the expected value of <span class="math inline">\(p&#39;\)</span> averaging over the distribution prior to cell division, i.e. <span id="eq:ch5_eq93"><span class="math display">\[
\left\langle p \right\rangle_{t_o} = 
\frac{\left\langle{p&#39;}\right\rangle_{t_d}}{2},
\qquad(93)\]</span></span> where <span class="math inline">\(\left\langle{\cdot}\right\rangle_{t_d}\)</span> highlights that is the moment of the distribution prior to the cell division. This result makes perfect sense. What this is saying is that the mean protein copy number right after the cell divides is half of the mean protein copy number just before the cell division. That is exactly we would expect. So in principle to know the first moment of either the mRNA distribution <span class="math inline">\(\langle m \rangle_{t_o}\)</span> or the protein distribution <span class="math inline">\(\langle m \rangle_{t_o}\)</span> right after cell division it suffices to multiply the moments before the cell division <span class="math inline">\(\langle m \rangle_{t_d}\)</span> or <span class="math inline">\(\left\langle p \right\rangle_{t_d}\)</span> by 1/2. Let’s now explore how this generalizes to any other moment <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_o}\)</span>.</p>
<h4 id="computing-the-moments-of-a-binomial-distribution">Computing the moments of a binomial distribution</h4>
<p>The result from last section was dependent on us knowing the functional form of the first moment of the binomial distribution. For higher moments we need some systematic way to compute such moments. Luckily for us we can do so by using the so-called moment generating function (MGF). The MGF of a random variable <span class="math inline">\(X\)</span> is defined as <span id="eq:ch5_eq94"><span class="math display">\[
M_X(t) = \left\langle{e^{tX}}\right\rangle,
\qquad(94)\]</span></span> where <span class="math inline">\(t\)</span> is a dummy variable. Once we know the MGF we can obtain any moment of the distribution by simply computing <span id="eq:ch5_eq95"><span class="math display">\[
\left\langle{X^n}\right\rangle = 
\left. \frac{d^n}{dt^n} M_X(t) \right\vert_{t=0},
\qquad(95)\]</span></span> i.e. taking the <span class="math inline">\(n\)</span>-th derivative of the MGF returns the <span class="math inline">\(n\)</span>-th moment of the distribution. For the particular case of the binomial distribution <span class="math inline">\(X \sim \text{Bin}(N, q)\)</span> it can be shown that the MGF is of the form <span id="eq:ch5_eq96"><span class="math display">\[
M_X(t) = \left[ (1 - q) + qe^{t} \right]^N.
\qquad(96)\]</span></span> As an example let’s compute the first moment of this binomially distributed variable. For this, the first derivative of the MGF results in <span id="eq:ch5_eq97"><span class="math display">\[
\frac{d M_X(t)}{dt} = N [(1 - q) + qe^t]^{N - 1} q e^t.
\qquad(97)\]</span></span> We just need to follow and set <span class="math inline">\(t = 0\)</span> to obtain the first moment <span id="eq:ch5_eq98"><span class="math display">\[
\left. \frac{d M_X(t)}{dt} \right\vert_{t=0} = N q,
\qquad(98)\]</span></span> which is exactly the expected value of a binomially distributed random variable.</p>
<p>So according to to compute any moment <span class="math inline">\(\left\langle{m^x p^y}\right\rangle\)</span> after cell division we can just take the <span class="math inline">\(x\)</span>-th derivative and the <span class="math inline">\(y\)</span>-th derivative of the binomial MGF to obtain <span class="math inline">\(\left\langle{m^x \mid m&#39;}\right\rangle\)</span> and <span class="math inline">\(\left\langle{p^y \mid p&#39;}\right\rangle\)</span>, respectively, and take the expected value of the result. Let’s follow on detail the specific case for the moment <span class="math inline">\(\left\langle{m p}\right\rangle\)</span>. When computing the moment after cell division <span class="math inline">\(\left\langle{mp}\right\rangle_{t_o}\)</span> which is of the form <span id="eq:ch5_eq99"><span class="math display">\[
\left\langle{mp}\right\rangle_{t_o} = 
\sum_{m&#39;} \sum{p&#39;} 
\left\langle{m \mid m&#39;}\right\rangle \left\langle p \mid p&#39; \right\rangle 
P_{t_d}(m&#39;, p&#39;),
\qquad(99)\]</span></span> the product <span class="math inline">\(\left\langle{m \mid m&#39;}\right\rangle \left\langle p \mid p&#39; \right\rangle\)</span> is then <span id="eq:ch5_eq100"><span class="math display">\[
\left\langle{m \mid m&#39;}\right\rangle \left\langle p \mid p&#39; \right\rangle =
\frac{m&#39;}{2} \cdot \frac{p&#39;}{2},
\qquad(100)\]</span></span> where we used the result in , substituting <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> for <span class="math inline">\(X\)</span>, respectively, and <span class="math inline">\(q\)</span> for 1/2. Substituting this result into the moment gives <span id="eq:ch5_eq101"><span class="math display">\[
\left\langle{mp}\right\rangle_{t_o} = 
\sum_{m&#39;} \sum_{p&#39;} \frac{m&#39; p&#39;}{4} P_{t_d}(m&#39;, p&#39;) 
= \frac{\left\langle{m&#39; p&#39;}\right\rangle_{t_d}}{4}.
\qquad(101)\]</span></span> Therefore to compute the moment after cell division <span class="math inline">\(\left\langle{mp}\right\rangle_{t_o}\)</span> we simply have to divide by 4 the corresponding equivalent moment before the cell division.</p>
<p>Not all moments after cell division depend only on the equivalent moment before cell division. For example if we compute the third moment of the protein distribution <span class="math inline">\(\left\langle{p^3}\right\rangle_{t_o}\)</span>, we find <span id="eq:ch5_eq102"><span class="math display">\[
\left\langle{p^3}\right\rangle_{t_o} = 
\frac{\left\langle{p^3}\right\rangle_{t_d}}{8} + 
\frac{3 \left\langle{p^2}\right\rangle_{t_d}}{8}.
\qquad(102)\]</span></span> So for this particular case the third moment of the protein distribution depends on the third moment and the second moment before the cell division. In general all moments after cell division <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_o}\)</span> linearly depend on moments before cell division. Furthermore, there is “moment closure” for this specific case in the sense that all moments after cell division depend on lower moments before cell division. To generalize these results to all the moments computed in this work let us then define a vector to collect all moments before the cell division up the <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_d}\)</span> moment, i.e. <span id="eq:ch5_eq103"><span class="math display">\[
\mathbf{\left\langle m^x p^y \right\rangle}_{t_d} = \left(
\left\langle{m^0 p^0}\right\rangle_{t_d}, \left\langle{m^1}\right\rangle_{t_d},
\ldots , \left\langle m^x p^y \right\rangle_{t_d}
\right).
\qquad(103)\]</span></span> Then any moment after cell division <span class="math inline">\(\left\langle{m^{x&#39;} p^{y&#39;}}\right\rangle_{t_o}\)</span> for <span class="math inline">\(x&#39; \leq x\)</span> and <span class="math inline">\(y&#39; \leq y\)</span> can be computed as <span id="eq:ch5_eq104"><span class="math display">\[
\left\langle{m^{x&#39;} p^{y&#39;}}\right\rangle_{t_o} = 
\mathbf{z}_{x&#39;y&#39;} \cdot \mathbf{\left\langle m^x p^y \right\rangle}_{t_d},
\qquad(104)\]</span></span> where we define the vector <span class="math inline">\(\mathbf{z}_{x&#39;y&#39;}\)</span> as the vector containing all the coefficients that we obtain with the product of the two binomial distributions. For example for the case of the third protein moment <span class="math inline">\(\left\langle{p^3}\right\rangle_{t_o}\)</span> the vector <span class="math inline">\(\mathbf{z}_{x&#39;y&#39;}\)</span> would have zeros for all entries except for the corresponding entry for <span class="math inline">\(\left\langle{p^2}\right\rangle_{t_d}\)</span> and for <span class="math inline">\(\left\langle{p^3}\right\rangle_{t_d}\)</span>, where it would have <span class="math inline">\(3/8\)</span> and <span class="math inline">\(1/8\)</span> accordingly.</p>
<p>If we want then to compute all the moments after the cell division up to <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_o}\)</span> let us define an equivalent vector <span id="eq:ch5_eq105"><span class="math display">\[
\mathbf{\left\langle m^x p^y \right\rangle}_{t_o} = \left(
\left\langle{m^0 p^0}\right\rangle_{t_o}, \left\langle{m^1}\right\rangle_{t_o}, 
\ldots , \left\langle m^x p^y \right\rangle_{t_o}
\right).
\qquad(105)\]</span></span> Then we need to build a square matrix <span class="math inline">\(\mathbf{Z}\)</span> such that each row of the matrix contains the corresponding vector <span class="math inline">\(\mathbf{z}_{x&#39; y&#39;}\)</span> for each of the moments. Having this matrix we would simply compute the moments after the cell division as <span id="eq:ch5_eq106"><span class="math display">\[
\mathbf{\left\langle{m^x p^x}\right\rangle}_{t_o} = 
\mathbf{Z} \cdot \mathbf{\left\langle{m^x p^x}\right\rangle}_{t_d}.
\qquad(106)\]</span></span> In other words, matrix <span class="math inline">\(\mathbf{Z}\)</span> will contain all the coefficients that we need to multiply by the moments before the cell division in order to obtain the moments after cell division. Matrix <span class="math inline">\(\mathbf{Z}\)</span> was then generated automatically using Python’s analytical math library sympy <span class="citation" data-cites="sympy"> [<a href="#ref-sympy" role="doc-biblioref">27</a>]</span>.</p>
<p>(adapted from (B)) shows how the first moment of both mRNA and protein changes over several cell cycles. The mRNA quickly relaxes to the steady state corresponding to the parameters for both a single and two promoter copies. This is expected since the parameters for the mRNA production were determined in the first place under this assumption (See ). We note that there is no apparent delay before reaching steady state of the mean mRNA count after the cell divides. This is because the mean mRNA count for the two promoters copies state is exactly twice the expected mRNA count for the single promoter state (See ). Therefore once the mean mRNA count is halved after the cell division, it is already at the steady state value for the single promoter case. On the other hand, given that the relaxation time to steady state is determined by the degradation rate, the mean protein count does not reach its corresponding steady state value for either promoter copy number state. Interestingly once a couple of cell cycles have passed the first moment has a repetitive trajectory over cell cycles. We have observed this experimentally by tracking cells as they grow under the microscope. Comparing cells at the beginning of the cell cycle with the daughter cells that appear after cell division shown that on average all cells have the same amount of protein at the beginning of the cell cycle (See Fig. 18 of <span class="citation" data-cites="Phillips2019"> [<a href="#ref-Phillips2019" role="doc-biblioref">26</a>]</span>), suggesting that these dynamical steady state takes place <em>in vivo</em>.</p>
<figure>
<img src="ch5_fig08.png" id="fig:ch5_fig08" data-short-caption="First and second moment dynamics over cell the cell cycle" alt="Figure 8: First and second moment dynamics over cell the cell cycle. Mean \pm standard deviation mRNA (upper panel) and mean \pm standard deviation protein copy number (lower panel) as the cell cycle progresses. The dark shaded region delimits the fraction of the cell cycle that cells spend with a single copy of the promoter. The light shaded region delimits the fraction of the cell cycle that cells spend with two copies of the promoter. For a 100 min doubling time at the galK locus cells spend 60% of the time with one copy of the promoter and the rest with two copies." /><figcaption aria-hidden="true">Figure 8: <strong>First and second moment dynamics over cell the cell cycle.</strong> Mean <span class="math inline">\(\pm\)</span> standard deviation mRNA (upper panel) and mean <span class="math inline">\(\pm\)</span> standard deviation protein copy number (lower panel) as the cell cycle progresses. The dark shaded region delimits the fraction of the cell cycle that cells spend with a single copy of the promoter. The light shaded region delimits the fraction of the cell cycle that cells spend with two copies of the promoter. For a 100 min doubling time at the <em>galK</em> locus cells spend 60% of the time with one copy of the promoter and the rest with two copies.</figcaption>
</figure>
<p>In principle when measuring gene expression levels experimentally from an asynchronous culture, cells are sampled from any time point across their individual cell cycles. This means that the moments determined experimentally correspond to an average over the cell cycle. In the following section we discuss how to account for the fact that cells are not uniformly distributed across the cell cycle in order to compute these averages.</p>
<h3 id="exponentially-distributed-ages">Exponentially distributed ages</h3>
<p>As mentioned in , cells in exponential growth have exponentially distributed ages across the cell cycle, having more young cells compared to old ones. Specifically the probability of a cell being at any time point in the cell cycle is given by <span class="citation" data-cites="Powell1956"> [<a href="#ref-Powell1956" role="doc-biblioref">16</a>]</span> <span id="eq:ch5_eq107"><span class="math display">\[
P(a) = (\ln 2) \cdot 2^{1 - a},
\qquad(107)\]</span></span> where <span class="math inline">\(a \in [0, 1]\)</span> is the stage of the cell cycle, with <span class="math inline">\(a = 0\)</span> being the start of the cycle and <span class="math inline">\(a = 1\)</span> being the cell division. In we reproduce this derivation. It is a surprising result, but can be intuitively thought as follows: If the culture is growing exponentially, that means that all the time there is an increasing number of cells. That means for example that if in a time interval <span class="math inline">\(\Delta t\)</span> <span class="math inline">\(N\)</span> “old” cells divided, these produced <span class="math inline">\(2N\)</span> “young” cells. So at any point there is always more younger than older cells.</p>
<p>Our numerical integration of the moment equations gave us a time evolution of the moments as cells progress through the cell cycle. Since experimentally we sample asynchronous cells that follow , each time point along the moment dynamic must be weighted by the probability of having sampled a cell at such specific time point of the cell cycle. Without loss of generality let’s focus on the first mRNA moment <span class="math inline">\(\left\langle{m(t)}\right\rangle\)</span> (the same can be applied to all other moments). As mentioned before, in order to calculate the first moment across the entire cell cycle we must weigh each time point by the corresponding probability that a cell is found in such point of its cell cycle. This translates to computing the integral <span id="eq:ch5_eq108"><span class="math display">\[
\langle m \rangle_c = 
\int_{\text{beginning cell cycle}}^{\text{end cell cycle}}
                       \left\langle{m(t)}\right\rangle P(t) dt,
\qquad(108)\]</span></span> where <span class="math inline">\(\langle m \rangle_c\)</span> is the mean mRNA copy number averaged over the entire cell cycle trajectory, and <span class="math inline">\(P(t)\)</span> is the probability of a cell being at a time <span class="math inline">\(t\)</span> of its cell cycle.</p>
<p>If we set the time in units of the cell cycle length we can use and compute instead <span id="eq:ch5_eq109"><span class="math display">\[
\langle m \rangle = \int_0^1 \left\langle{m(a)}\right\rangle P(a) da,
\qquad(109)\]</span></span> where <span class="math inline">\(P(a)\)</span> is given by XXX.</p>
<p>What implies is that in order to compute the first moment (or any moment of the distribution) we must weigh each point in the moment dynamics by the corresponding probability of a cell being at that point along its cell cycle. That is why when computing a moment we take the time trajectory of a single cell cycle as the ones shown in and compute the average using to weigh each time point. We perform this integral numerically for all moments using Simpson’s rule.</p>
<h3 id="reproducing-the-equilibrium-picture">Reproducing the equilibrium picture</h3>
<p>Given the large variability of the first moments depicted in it is worth considering why a simplistic equilibrium picture has shown to be very successful in predicting the mean expression level under diverse conditions <span class="citation" data-cites="Garcia2011c Brewster2014 Barnes2019 Razo-Mejia2018"> [<a href="#ref-Barnes2019" role="doc-biblioref">8</a>,<a href="#ref-Razo-Mejia2018" role="doc-biblioref">18</a>,<a href="#ref-Garcia2011c" role="doc-biblioref">35</a>,<a href="#ref-Brewster2014" role="doc-biblioref">36</a>]</span>. In this section we compare the simple repression thermodynamic model with this dynamical picture of the cell cycle. But before diving into this comparison, it is worth recapping the assumptions that go into the equilibrium model.</p>
<h4 id="steady-state-under-the-thermodynamic-model">Steady state under the thermodynamic model</h4>
<p>Given the construction of the thermodynamic model of gene regulation for which the probability of the promoter microstates rather than the probability of mRNA or protein counts is accounted for, we are only allowed to describe the dynamics of the first moment using this theoretical framework <span class="citation" data-cites="Phillips2015a"> [<a href="#ref-Phillips2015a" role="doc-biblioref">33</a>]</span>. Again let’s only focus on the mRNA first moment <span class="math inline">\(\langle m \rangle\)</span>. The same principles apply if we consider the protein first moment. We can write a dynamical system of the form <span id="eq:ch5_eq110"><span class="math display">\[
\frac{d \langle m \rangle}{dt} = 
r_m \cdot p_{\text{bound}} - \gamma _m \langle m \rangle,
\qquad(110)\]</span></span> where as before <span class="math inline">\(r_m\)</span> and <span class="math inline">\(\gamma _m\)</span> are the mRNA production and degradation rates respectively, and <span class="math inline">\(p_{\text{bound}}\)</span> is the probability of finding the RNAP bound to the promoter <span class="citation" data-cites="Bintu2005a"> [<a href="#ref-Bintu2005a" role="doc-biblioref">30</a>]</span>. This dynamical system is predicted to have a single stable fixed point that we can find by computing the steady state. When we solve for the mean mRNA copy number at steady state <span class="math inline">\(\langle m \rangle_{ss}\)</span> we find <span id="eq:ch5_eq111"><span class="math display">\[
\langle m \rangle_{ss} = \frac{r_m}{\gamma _m} p_{\text{bound}}.
\qquad(111)\]</span></span></p>
<p>Since we assume that the only effect that the repressor has over the regulation of the promoter is exclusion of the RNAP from binding to the promoter, we assume that only <span class="math inline">\(p_{\text{bound}}\)</span> depends on the repressor copy number <span class="math inline">\(R\)</span>. Therefore when computing the fold-change in gene expression we are left with <span id="eq:ch5_eq112"><span class="math display">\[
\text{fold-change} = \frac{\left\langle{m (R \neq 0)}\right\rangle_{ss}}{
\left\langle{m (R = 0)}\right\rangle_{ss}}
= \frac{p_{\text{bound}} (R \neq 0)}{p_{\text{bound}} (R = 0)}.
\qquad(112)\]</span></span> As derived in <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">35</a>]</span> this can be written in the language of equilibrium statistical mechanics as <span id="eq:ch5_eq113"><span class="math display">\[
\text{fold-change} = 
\left(1 + \frac{R}{N_{NS}}e^{-\beta \Delta\varepsilon_r}  \right)^{-1},
\qquad(113)\]</span></span> where <span class="math inline">\(\beta \equiv (k_BT)^{-1}\)</span>, <span class="math inline">\(\Delta\varepsilon_r\)</span> is the repressor-DNA binding energy, and <span class="math inline">\(N_{NS}\)</span> is the number of non-specific binding sites where the repressor can bind.</p>
<p>To arrive at we ignore the physiological changes that occur during the cell cycle; one of the most important being the variability in gene copy number that we are exploring in this section. It is therefore worth thinking about whether or not the dynamical picture exemplified in can be reconciled with the predictions made by both at the mRNA and protein level.</p>
<p>compares the predictions of both theoretical frameworks for varying repressor copy numbers and repressor-DNA affinities. The solid lines are directly computed from . The hollow triangles and the solid circles, represent the fold-change in mRNA and protein respectively as computed from the moment dynamics. To compute the fold-change from the kinetic picture we first numerically integrate the moment dynamics for both the two- and the three-state promoter (See for the unregulated case) and then average the time series accounting for the probability of cells being sampled at each stage of the cell cycle as defined in . The small systematic deviations between both models come partly from the simplifying assumption that the repressor copy number, and therefore the repressor on rate <span class="math inline">\(k^{(r)}_{\text{on}}\)</span> remains constant during the cell cycle. In principle the gene producing the repressor protein itself is also subjected to the same duplication during the cell cycle, changing therefore the mean repressor copy number for both stages.</p>
<figure>
<img src="ch5_fig09.png" id="fig:ch5_fig09" data-short-caption="Comparison of the equilibrium and kinetic reressor titration predictions" alt="Figure 9: Comparison of the equilibrium and kinetic reressor titration predictions. The equilibrium model (solid lines) and the kinetic model with variation over the cell cycle (solid circles and white triangles) predictions are compared for varying repressor copy numbers and operator binding energy. The equilibrium model is directly computed from while the kinetic model is computed by numerically integrating the moment equations over several cell cycles, and then averaging over the extent of the cell cycle as defined in ." /><figcaption aria-hidden="true">Figure 9: <strong>Comparison of the equilibrium and kinetic reressor titration predictions.</strong> The equilibrium model (solid lines) and the kinetic model with variation over the cell cycle (solid circles and white triangles) predictions are compared for varying repressor copy numbers and operator binding energy. The equilibrium model is directly computed from while the kinetic model is computed by numerically integrating the moment equations over several cell cycles, and then averaging over the extent of the cell cycle as defined in .</figcaption>
</figure>
<p>For completeness compares the kinetic and equilibrium models for the extended model of <span class="citation" data-cites="Razo-Mejia2018"> [<a href="#ref-Razo-Mejia2018" role="doc-biblioref">18</a>]</span> in which the inducer concentration enters into the equation. The solid line is directly computed from Eq. 5 of <span class="citation" data-cites="Razo-Mejia2018"> [<a href="#ref-Razo-Mejia2018" role="doc-biblioref">18</a>]</span>. The hollow triangles and solid points follow the same procedure as for , where the only effect that the inducer is assume to have in the kinetics is an effective change in the number of active repressors, affecting therefore <span class="math inline">\(k^{(r)}_{\text{on}}\)</span>.</p>
<figure>
<img src="ch5_fig10.png" id="fig:ch5_fig10" data-short-caption="Comparison of the equilibrium and kinetic inducer titration predictions" alt="Figure 10: Comparison of the equilibrium and kinetic inducer titration predictions. The equilibrium model (solid lines) and the kinetic model with variation over the cell cycle (solid circles and white triangles) predictions are compared for varying repressor copy numbers and inducer concentrations. The equilibrium model is directly computed as Eq. 5 of reference  [18] with repressor-DNA binding energy \Delta\varepsilon_r = -13.5 \; k_BT while the kinetic model is computed by numerically integrating the moment dynamics over several cell cycles, and then averaging over the extent of a single cell cycle as defined in ." /><figcaption aria-hidden="true">Figure 10: <strong>Comparison of the equilibrium and kinetic inducer titration predictions.</strong> The equilibrium model (solid lines) and the kinetic model with variation over the cell cycle (solid circles and white triangles) predictions are compared for varying repressor copy numbers and inducer concentrations. The equilibrium model is directly computed as Eq. 5 of reference <span class="citation" data-cites="Razo-Mejia2018"> [<a href="#ref-Razo-Mejia2018" role="doc-biblioref">18</a>]</span> with repressor-DNA binding energy <span class="math inline">\(\Delta\varepsilon_r = -13.5 \; k_BT\)</span> while the kinetic model is computed by numerically integrating the moment dynamics over several cell cycles, and then averaging over the extent of a single cell cycle as defined in .</figcaption>
</figure>
<h3 id="comparison-between-single--and-multi-promoter-kinetic-model">Comparison between single- and multi-promoter kinetic model</h3>
<p>After these calculations it is worth questioning whether the inclusion of this change in gene dosage is drastically different with respect to the simpler picture of a kinetic model that ignores the gene copy number variability during the cell cycle. To this end we systematically computed the average moments for varying repressor copy number and repressor-DNA affinities. We then compare these results with the moments obtained from a single-promoter model and their corresponding parameters. The derivation of the steady-state moments of the distribution for the single-promoter model are detailed in .</p>
<p>and both suggest that since the dynamic multi-promoter model can reproduce the results of the equilibrium model at the first moment level it must then also be able to reproduce the results of the single-promoter model at this level (See ). The interesting comparison comes with higher moments. A useful metric to consider for gene expression variability is the noise in gene expression <span class="citation" data-cites="Shahrezaei2008"> [<a href="#ref-Shahrezaei2008" role="doc-biblioref">31</a>]</span>. This quantity, defined as the standard deviation divided by the mean, is a dimensionless metric of how much variability there is with respect to the mean of a distribution. As we will show below this quantity differs from the also commonly used metric known as the Fano factor (variance / mean) in the sense that for experimentally determined expression levels in fluorescent arbitrary units, the noise is a dimensionless quantity while the Fano factor is not.</p>
<p>shows the comparison of the predicted protein noise between the single- (dashed lines) and the multi-promoter model (solid lines) for different operators and repressor copy numbers. A striking difference between both is that the single-promoter model predicts that as the inducer concentration increases, the standard deviation grows much slower than the mean, giving a very small noise. In comparison the multi-promoter model has a much higher floor for the lowest value of the noise, reflecting the expected result that the variability in gene copy number across the cell cycle should increase the cell-to-cell variability in gene expression <span class="citation" data-cites="Peterson2015 Jones2014a"> [<a href="#ref-Peterson2015" role="doc-biblioref">6</a>,<a href="#ref-Jones2014a" role="doc-biblioref">29</a>]</span></p>
<figure>
<img src="ch5_fig11.png" id="fig:ch5_fig11" data-short-caption="Comparison of the predicted protein noise between a single- and a multi-promoter kinetic model" alt="Figure 11: Comparison of the predicted protein noise between a single- and a multi-promoter kinetic model. Comparison of the noise (standard deviation/mean) between a kinetic model that considers a single promoter at all times (dashed line) and the multi-promoter model developed in this section (solid line) for different repressor operators. (A) Operator O1, \Delta\varepsilon_r = -15.3 \; k_BT, (B) O2, \Delta\varepsilon_r = -13.9 \; k_BT, (C) O3, \Delta\varepsilon_r = -9.7 \; k_BT" /><figcaption aria-hidden="true">Figure 11: <strong>Comparison of the predicted protein noise between a single- and a multi-promoter kinetic model.</strong> Comparison of the noise (standard deviation/mean) between a kinetic model that considers a single promoter at all times (dashed line) and the multi-promoter model developed in this section (solid line) for different repressor operators. (A) Operator O1, <span class="math inline">\(\Delta\varepsilon_r = -15.3 \; k_BT\)</span>, (B) O2, <span class="math inline">\(\Delta\varepsilon_r = -13.9 \; k_BT\)</span>, (C) O3, <span class="math inline">\(\Delta\varepsilon_r = -9.7 \; k_BT\)</span></figcaption>
</figure>
<h3 id="comparison-with-experimental-data">Comparison with experimental data</h3>
<p>Having shown that the kinetic model presented in this section can not only reproduce the results from the equilibrium picture at the mean level (See and ), but make predictions for the cell-to-cell variability as quantified by the noise (See ), we can assess whether or not this model is able to predict experimental measurements of the noise. For this we take the single cell intensity measurements (See Methods) to compute the noise at the protein level.</p>
<p>As mentioned before this metric differs from the Fano factor since for fluorescent arbitrary units the noise is a dimensionless quantity. To see why consider that the noise is defined as <span id="eq:ch5_eq114"><span class="math display">\[
\text{noise} \equiv \frac{\sqrt{\left\langle p^2 \right\rangle -
                        \left\langle p \right\rangle^2}}
                        {\left\langle p \right\rangle}.
\qquad(114)\]</span></span> We assume that the intensity level of a cell <span class="math inline">\(I\)</span> is linearly proportional to the absolute protein count, i.e. <span id="eq:ch5_eq115"><span class="math display">\[
I = \alpha p,
\qquad(115)\]</span></span> where <span class="math inline">\(\alpha\)</span> is the proportionality constant between arbitrary units and protein absolute number <span class="math inline">\(p\)</span>. Substituting this definition on gives <span id="eq:ch5_eq116"><span class="math display">\[
\text{noise} = \frac{\sqrt{\left\langle{(\alpha I)^2}\right\rangle - 
\left\langle{\alpha I}\right\rangle^2}}{
\left\langle{\alpha I}\right\rangle}.
\qquad(116)\]</span></span></p>
<p>Since <span class="math inline">\(\alpha\)</span> is a constant it can be taken out of the average operator <span class="math inline">\(\left\langle{\cdot}\right\rangle\)</span>, obtaining <span id="eq:ch5_eq117"><span class="math display">\[
\text{noise} = \frac{\sqrt{\alpha^2 \left(\left\langle{I^2}\right\rangle -
\left\langle{I}\right\rangle^2 \right)}}{
\alpha \left\langle{I}\right\rangle}
= \frac{\sqrt{\left(\left\langle{I^2}\right\rangle - \left\langle{I}\right\rangle^2 \right)}}{
\left\langle{I}\right\rangle}.
\qquad(117)\]</span></span></p>
<p>Notice that in the linear proportionality between intensity and protein count has no intercept. This ignores the autofluorescence that cells without reporter would generate. To account for this, in practice we compute <span id="eq:ch5_eq118"><span class="math display">\[
\text{noise} = 
\frac{\sqrt{\left(\left\langle{(I - \left\langle
{I_\text{auto}}\right\rangle)^2}\right\rangle -
\left\langle{I - \left\langle{I_\text{auto}}\right\rangle}\right\rangle^2
\right)}}{
\left\langle{I - \left\langle{I_\text{auto}}\right\rangle}\right\rangle}.
\qquad(118)\]</span></span> where <span class="math inline">\(I\)</span> is the intensity of the strain of interest and <span class="math inline">\(\left\langle{I_\text{auto}}\right\rangle\)</span> is the mean autofluorescence intensity, obtained from a strain that does not carry the fluorescent reporter gene.</p>
<p>Fig. 12 shows the comparison between theoretical predictions and experimental measurements for the unregulated promoter. The reason we split the data by operator despite the fact that since these are unregulated promoters, they should in principle have identical expression profiles is to precisely make sure that this is the case. We have found in the past that sequences downstream of the RNAP binding site can affect the expression level of constitutively expressed genes. We can see that both models, the single-promoter (gray dotted line) and the multi-promoter (black dashed line) underestimate the experimental noise to different degrees. The single-promoter model does a worse job at predicting the experimental data since it doesn’t account for the differences in gene dosage during the cell cycle. But still we can see that accounting for this variability takes us to within a factor of two of the experimentally determined noise for these unregulated strains.</p>
<figure>
<img src="ch5_fig12.png" id="fig:ch5_fig12" data-short-caption="Protein noise of the unregulated promoter" alt="Figure 12: Protein noise of the unregulated promoter. Comparison of the experimental noise for different operators with the theoretical predictions for the single-promoter (gray dotted line) and the multi-promoter model (black dashed line). Each datum represents a single date measurement of the corresponding \Delta lacI strain with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples." /><figcaption aria-hidden="true">Figure 12: <strong>Protein noise of the unregulated promoter.</strong> Comparison of the experimental noise for different operators with the theoretical predictions for the single-promoter (gray dotted line) and the multi-promoter model (black dashed line). Each datum represents a single date measurement of the corresponding <span class="math inline">\(\Delta lacI\)</span> strain with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples.</figcaption>
</figure>
<p>To further test the model predictive power we compare the predictions for the three-state regulated promoter. Fig. 13 shows the theoretical predictions for the single- and multi-promoter model for varying repressor copy numbers and repressor-DNA binding affinities as a function of the inducer concentration. We can see again that our zero-parameter fits systematically underestimates the noise for all strains and all inducer concentrations. We highlight that the <span class="math inline">\(y\)</span>-axis is shown in a log-scale to emphasize more this deviation; but, as we will show in the next section, our predictions still fall within a factor of two from the experimental data.</p>
<figure>
<img src="ch5_fig13.png" id="fig:ch5_fig13" data-short-caption="Protein noise of the regulated promoter" alt="Figure 13: Protein noise of the regulated promoter. Comparison of the experimental noise for different operators ((A) O1, \Delta\varepsilon_r = -15.3 \; k_BT, (B) O2, \Delta\varepsilon_r = -13.9 \; k_BT, (C) O3, \Delta\varepsilon_r = -9.7 \; k_BT) with the theoretical predictions for the single-promoter (dashed lines) and the multi-promoter model (solid lines). Points represent the experimental noise as computed from single-cell fluorescence measurements of different E. coli strains under 12 different inducer concentrations. Dotted line indicates plot in linear rather than logarithmic scale. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. White-filled dots are plot at a different scale for better visualization." /><figcaption aria-hidden="true">Figure 13: <strong>Protein noise of the regulated promoter.</strong> Comparison of the experimental noise for different operators ((A) O1, <span class="math inline">\(\Delta\varepsilon_r = -15.3 \; k_BT\)</span>, (B) O2, <span class="math inline">\(\Delta\varepsilon_r = -13.9 \; k_BT\)</span>, (C) O3, <span class="math inline">\(\Delta\varepsilon_r = -9.7 \; k_BT\)</span>) with the theoretical predictions for the single-promoter (dashed lines) and the multi-promoter model (solid lines). Points represent the experimental noise as computed from single-cell fluorescence measurements of different <em>E. coli</em> strains under 12 different inducer concentrations. Dotted line indicates plot in linear rather than logarithmic scale. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. White-filled dots are plot at a different scale for better visualization.</figcaption>
</figure>
<h4 id="systematic-deviation-of-the-noise-in-gene-expression">Systematic deviation of the noise in gene expression</h4>
<p>Fig. 12 and Fig. 13 highlight that our model underestimates the cell-to-cell variability as measured by the noise. To further explore this systematic deviation shows the theoretical vs. experimental noise both in linear and log scale. As we can see the data is systematically above the identity line. The data is colored by their corresponding experimental fold-change values. The data that has the largest deviations from the identity line also corresponds to the data with the largest error bars and the smallest fold-change. This is because measurements with very small fold-changes correspond to intensities very close to the autofluorescence background. Therefore minimal changes when computing the noise are amplified given the ratio of std/mean. In we will explore empirical ways to improve the agreement between our minimal model and the experimental data to guide future efforts to improve the minimal.</p>
<figure>
<img src="ch5_fig14.png" id="fig:ch5_fig14" data-short-caption="Systematic comparison of theoretical vs experimental noise in gene expression" alt="Figure 14: Systematic comparison of theoretical vs experimental noise in gene expression. Theoretical vs. experimental noise both in linear (left) and log (right) scale. The dashed line shows the identity line of slope 1 and intercept zero. All data are colored by the corresponding value of the experimental fold-change in gene expression as indicated by the color bar. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples." /><figcaption aria-hidden="true">Figure 14: <strong>Systematic comparison of theoretical vs experimental noise in gene expression.</strong> Theoretical vs. experimental noise both in linear (left) and log (right) scale. The dashed line shows the identity line of slope 1 and intercept zero. All data are colored by the corresponding value of the experimental fold-change in gene expression as indicated by the color bar. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples.</figcaption>
</figure>
<h2 id="maximum-entropy-approximation-of-distributions">Maximum entropy approximation of distributions</h2>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/MaxEnt_approx_joint.html">following link</a> as an annotated Jupyter notebook)</p>
<p>On the one hand the solution of chemical master equations like the one in represent a hard mathematical challenge. As presented in Peccoud and Ycart derived a closed-form solution for the two-state promoter at the mRNA level <span class="citation" data-cites="Peccoud1995"> [<a href="#ref-Peccoud1995" role="doc-biblioref">10</a>]</span>. In an impressive display of mathematical skills, Shahrezaei and Swain were able to derive an approximate solution for the one- (not considered in this work) and two-state promoter master equation at the protein level <span class="citation" data-cites="Shahrezaei2008"> [<a href="#ref-Shahrezaei2008" role="doc-biblioref">31</a>]</span>. Nevertheless both of these solutions do not give instantaneous insights about the distributions as they involve complicated terms such as confluent hypergeometric functions.</p>
<p>On the other hand there has been a great deal of work to generate methods that can approximate the solution of these discrete state Markovian models <span class="citation" data-cites="Ale2013 Andreychenko2017 Frohlich2016 Schnoerr2017 Smadbeck2013"> [<a href="#ref-Frohlich2016" role="doc-biblioref">4</a>,<a href="#ref-Schnoerr2017" role="doc-biblioref">5</a>,<a href="#ref-Ale2013" role="doc-biblioref">13</a>,<a href="#ref-Andreychenko2017" role="doc-biblioref">15</a>,<a href="#ref-Smadbeck2013" role="doc-biblioref">19</a>]</span>. In particular for master equations like the one that concerns us here whose moments can be easily computed, the moment expansion method provides a simple method to approximate the full joint distribution of mRNA and protein <span class="citation" data-cites="Smadbeck2013"> [<a href="#ref-Smadbeck2013" role="doc-biblioref">19</a>]</span>. In this section we will explain the principles behind this method and show the implementation for our particular case study.</p>
<h3 id="the-maxent-principle">The MaxEnt principle</h3>
<p>The principle of maximum entropy (MaxEnt) first proposed by E. T. Jaynes in 1957 tackles the question of given limited information what is the least biased inference one can make about a particular probability distribution <span class="citation" data-cites="Jaynes1957"> [<a href="#ref-Jaynes1957" role="doc-biblioref">2</a>]</span>. In particular Jaynes used this principle to show the correspondence between statistical mechanics and information theory, demonstrating, for example, that the Boltzmann distribution is the probability distribution that maximizes Shannon’s entropy subject to a constraint that the average energy of the system is fixed.</p>
<p>To illustrate the principle let us focus on a univariate distribution <span class="math inline">\(P_X(x)\)</span>. The <span class="math inline">\(n^{\text{th}}\)</span> moment of the distribution for a discrete set of possible values of <span class="math inline">\(x\)</span> is given by <span id="eq:ch5_eq119"><span class="math display">\[
\left\langle{x^n}\right\rangle \equiv \sum_x x^n P_X(x).
\qquad(119)\]</span></span></p>
<p>Now assume that we have knowledge of the first <span class="math inline">\(m\)</span> moments <span class="math inline">\(\mathbf{\left\langle{x}\right\rangle}_m = (\left\langle{x}\right\rangle, \left\langle{x^2}\right\rangle, \ldots, \left\langle{x^m}\right\rangle )\)</span>. The question is then how can we use this information to build an estimator <span class="math inline">\(P_H(x \mid \mathbf{\left\langle{x}\right\rangle}_m)\)</span> of the distribution such that <span id="eq:ch5_eq120"><span class="math display">\[
\lim_{m \rightarrow \infty} 
P_H(x \mid \mathbf{\left\langle{x}\right\rangle}_m) \rightarrow P_X(x),
\qquad(120)\]</span></span> i.e. that the more moments we add to our approximation, the more the estimator distribution converges to the real distribution.</p>
<p>The MaxEnt principle tells us that our best guess for this estimator is to build it on the base of maximizing the Shannon entropy, constrained by the information we have about these <span class="math inline">\(m\)</span> moments. The maximization of Shannon’s entropy guarantees that we are the least committed possible to information that we do not posses. The Shannon entropy for an univariate discrete distribution is given by <span class="citation" data-cites="Shannon1948"> [<a href="#ref-Shannon1948" role="doc-biblioref">11</a>]</span> <span id="eq:ch5_eq121"><span class="math display">\[
H(x) \equiv - \sum_x P_X(x) \log P_X(x).
\qquad(121)\]</span></span></p>
<p>For an optimization problem subject to constraints we make use of the method of the Lagrange multipliers. For this we define the constraint equation <span class="math inline">\(\mathcal{L}(x)\)</span> as <span id="eq:ch5_eq122"><span class="math display">\[
\mathcal{L}(x) \equiv H(x) - \sum_{i=0}^m
\left[ \lambda_i \left( \left\langle{x^i}\right\rangle -
\sum_x x^i P_X(x) \right) \right],
\qquad(122)\]</span></span> where <span class="math inline">\(\lambda_i\)</span> is the Lagrange multiplier associated with the <span class="math inline">\(i^\text{th}\)</span> moment. The inclusion of the zeroth moment is an additional constraint to guarantee the normalization of the resulting distribution. Since <span class="math inline">\(P_X(x)\)</span> has a finite set of discrete values, when taking the derivative of the constraint equation with respect to <span class="math inline">\(P_X(x)\)</span>, we chose a particular value of <span class="math inline">\(X = x\)</span>. Therefore from the sum over all possible <span class="math inline">\(x\)</span> values only a single term survives. With this in mind we take the derivative of the constraint equation obtaining <span id="eq:ch5_eq123"><span class="math display">\[
\frac{d\mathcal{L}}{d P_X(x)} = -\log P_X(x) - 1 -
\sum_{i=0}^m \lambda_i x^i.
\qquad(123)\]</span></span></p>
<p>Equating this derivative to zero and solving for the distribution (that we now start calling <span class="math inline">\(P_H(x)\)</span>, our MaxEnt estimator) gives <span id="eq:ch5_eq124"><span class="math display">\[
P_H(x) = \exp \left(- 1 - \sum_{i=0}^m \lambda_i x^i \right)
= \frac{1}{\mathcal{Z}}
\exp \left( - \sum_{i=1}^m \lambda_i x^i \right),
\qquad(124)\]</span></span> where <span class="math inline">\(\mathcal{Z}\)</span> is the normalization constant that can be obtained by substituting this solution into the normalization constraint. This results in <span id="eq:ch5_eq125"><span class="math display">\[
\mathcal{Z} \equiv \exp\left( 1 + \lambda_0 \right) =
\sum_x \exp \left( - \sum_{i=1}^m \lambda_i x^i \right).
\qquad(125)\]</span></span></p>
<p>XXX is the general form of the MaxEnt distribution for a univariate distribution. The computational challenge then consists in finding numerical values for the Lagrange multipliers <span class="math inline">\(\{ \lambda_i \}\)</span> such that <span class="math inline">\(P_H(x)\)</span> satisfies our constraints. In other words, the Lagrange multipliers weight the contribution of each term in the exponent such that when computing any of the moments we recover the value of our constraint. Mathematically what this means is that <span class="math inline">\(P_H(x)\)</span> must satisfy <span id="eq:ch5_eq126"><span class="math display">\[
\sum_x x^n P_H(x) =
\sum_x \frac{x^n}{\mathcal{Z}}
\exp \left( - \sum_{i=1}^m \lambda_i x^i \right) = 
\left\langle{x^n}\right\rangle.
\qquad(126)\]</span></span></p>
<p>As an example of how to apply the MaxEnt principle let us use the classic problem of a six-face die. If we are only told that after a large number of die rolls the mean value of the face is <span class="math inline">\(\left\langle{x}\right\rangle = 4.5\)</span> (note that a fair die has a mean of <span class="math inline">\(3.5\)</span>), what would the least biased guess for the distribution look like? The MaxEnt principle tells us that our best guess would be of the form <span id="eq:ch5_eq127"><span class="math display">\[
P_H(x) = \frac{1}{\mathcal{Z}} \exp \left( \lambda x \right).
\qquad(127)\]</span></span> Using any numerical minimization package we can easily find the value of the Lagrange multiplier <span class="math inline">\(\lambda\)</span> that satisfies our constraint. shows two two examples of distributions that satisfy the constraint. Panel (A) shows a distribution consistent with the 4.5 average where both 4 and 5 are equally likely. Nevertheless in the information we got about the nature of the die it was never stated that some of the faces were forbidden. In that sense the distribution is committing to information about the process that we do not posses. Panel (B) by contrast shows the MaxEnt distribution that satisfies this constraint. Since this distribution maximizes Shannon’s entropy it is guaranteed to be the least biased distribution given the available information.</p>
<figure>
<img src="ch5_fig15.png" id="fig:ch5_fig15" data-short-caption="Maximum entropy distribution of six-face die" alt="Figure 15: Maximum entropy distribution of six-face die. (A)biased distribution consistent with the constraint \left\langle{x}\right\rangle = 4.5. (B) MaxEnt distribution also consistent with the constraint." /><figcaption aria-hidden="true">Figure 15: <strong>Maximum entropy distribution of six-face die.</strong> (A)biased distribution consistent with the constraint <span class="math inline">\(\left\langle{x}\right\rangle = 4.5\)</span>. (B) MaxEnt distribution also consistent with the constraint.</figcaption>
</figure>
<h4 id="the-mrna-and-protein-joint-distribution">The mRNA and protein joint distribution</h4>
<p>The MaxEnt principle can easily be extended to multivariate distributions. For our particular case we are interested in the mRNA and protein joint distribution <span class="math inline">\(P(m, p)\)</span>. The definition of a moment <span class="math inline">\(\left\langle m^x p^y \right\rangle\)</span> is a natural extension of of the form <span id="eq:ch5_eq128"><span class="math display">\[
\left\langle m^x p^y \right\rangle = \sum_m \sum_p m^x p^y P(m, p).
\qquad(128)\]</span></span></p>
<p>As a consequence the MaxEnt joint distribution <span class="math inline">\(P_H(m, p)\)</span> is of the form <span id="eq:ch5_eq129"><span class="math display">\[
P_H(m, p) = \frac{1}{\mathcal{Z}}
              \exp \left( - \sum_{(x,y)} \lambda_{(x,y)} m^x p^y \right),
\qquad(129)\]</span></span> where <span class="math inline">\(\lambda_{(x,y)}\)</span> is the Lagrange multiplier associated with the moment <span class="math inline">\(\left\langle m^x p^y \right\rangle\)</span>, and again <span class="math inline">\(\mathcal{Z}\)</span> is the normalization constant given by <span id="eq:ch5_eq130"><span class="math display">\[
\mathcal{Z} = \sum_m \sum_p
              \exp \left( - \sum_{(x, y)} \lambda_{(x, y)} m^x p^y \right).
\qquad(130)\]</span></span> Note that the sum in the exponent is taken over all available <span class="math inline">\((x, y)\)</span> pairs that define the moment constraints for the distribution.</p>
<h3 id="the-bretthorst-rescaling-algorithm">The Bretthorst rescaling algorithm</h3>
<p>The determination of the Lagrange multipliers suffer from a numerical under and overflow problem due to the difference in magnitude between the constraints. This becomes a problem when higher moments are taken into account. The resulting numerical values for the Lagrange multipliers end up being separated by several orders of magnitude. For routines such as Newton-Raphson or other minimization algorithms that can be used to find these Lagrange multipliers these different scales become problematic.</p>
<p>To get around this problem we implemented a variation to the algorithm due to G. Larry Bretthorst, E.T. Jaynes’ last student. With a very simple argument we can show that linearly rescaling the constraints, the Lagrange multipliers and the “rules” for how to compute each of the moments, i.e. each of the individual products that go into the moment calculation, should converge to the same MaxEnt distribution. In order to see this let’s consider again a univariate distribution <span class="math inline">\(P_X(x)\)</span> that we are trying to reconstruct given the first two moments <span class="math inline">\(\left\langle{x}\right\rangle\)</span>, and <span class="math inline">\(\left\langle{x^2}\right\rangle\)</span>. The MaxEnt distribution can be written as <span id="eq:ch5_eq131"><span class="math display">\[
P_H(x) = \frac{1}{\mathcal{Z}}
  \exp \left(- \lambda_1 x - \lambda_2 x^2 \right) =
  \frac{1}{\mathcal{Z}}
  \exp \left(- \lambda_1 x \right) \exp \left( - \lambda_2 x^2 \right).
\qquad(131)\]</span></span> We can always rescale the terms in any way and obtain the same result. Let’s say that for some reason we want to rescale the quadratic terms by a factor <span class="math inline">\(a\)</span>. We can define a new Lagrange multiplier <span class="math inline">\(\lambda_2&#39; \equiv \frac{\lambda_2}{a}\)</span> that compensates for the rescaling of the terms, obtaining <span id="eq:ch5_eq132"><span class="math display">\[
P_H(x) = \frac{1}{\mathcal{Z}}
  \exp \left(- \lambda_1 x \right) \exp \left( - \lambda_2&#39; ax^2 \right).
\qquad(132)\]</span></span> Computationally it might be more efficient to find the numerical value of <span class="math inline">\(\lambda_2&#39;\)</span> rather than <span class="math inline">\(\lambda_2\)</span> maybe because it is of the same order of magnitude as <span class="math inline">\(\lambda_1\)</span>. Then we can always multiply <span class="math inline">\(\lambda_2&#39;\)</span> by <span class="math inline">\(a\)</span> to obtain back the constraint for our quadratic term. What this means is that that we can always rescale the MaxEnt problem to make it numerically more stable, then we can rescale back to obtain the value of the Lagrange multipliers. The key to the Bretthorst algorithm lies in the selection of what rescaling factor to choose in order to make the numerical inference more efficient.</p>
<p>Bretthorst’s algorithm goes even further by further transforming the constraints and the variables to make the constraints orthogonal, making the computation much more effective. We now explain the implementation of the algorithm for our joint distribution of interest <span class="math inline">\(P(m, p)\)</span>.</p>
<h4 id="algorithm-implementation">Algorithm implementation</h4>
<p>Let the <span class="math inline">\(M \times N\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> contain all the factors used to compute the moments that serve as constraints, where each entry is of the form <span id="eq:ch5_eq133"><span class="math display">\[
A_{ij} = m_i^{x_j} \cdot p_i^{y_j}.
\qquad(133)\]</span></span> In other words, recall that to obtain any moment <span class="math inline">\(\left\langle m^x p^y \right\rangle\)</span> we compute <span id="eq:ch5_eq134"><span class="math display">\[
\left\langle m^x p^y \right\rangle = \sum_m \sum_p m^x p^y P(m, x).
\qquad(134)\]</span></span> If we have <span class="math inline">\(M\)</span> possible <span class="math inline">\((m, p)\)</span> pairs in our truncated sample space (because we can’t include the sample space up to infinity) <span class="math inline">\(\{(m, p)_1, (m, p)_2, \ldots (m, p)_N \}\)</span>, and we have <span class="math inline">\(N\)</span> exponent pairs <span class="math inline">\((x, y)\)</span> corresponding to the <span class="math inline">\(N\)</span> moments used to constraint the maximum entropy distribution <span class="math inline">\(\{(x, y)_1, (x, y)_2, \ldots, (x, y)_N \}\)</span>, then matrix <span class="math inline">\(\mathbf{A}\)</span> contains all the possible <span class="math inline">\(M\)</span> by <span class="math inline">\(N\)</span> terms of the form described in . Let also <span class="math inline">\(\mathbf{v}\)</span> be a vector of length <span class="math inline">\(N\)</span> containing all the constraints with each entry of the form <span id="eq:ch5_eq135"><span class="math display">\[
v_j = \left\langle{m^{x_j} p^{y_j}}\right\rangle,
\qquad(135)\]</span></span> i.e. the information that we have about the distribution. That means that the constraint equation <span class="math inline">\(\mathcal{L}\)</span> to be used for this problem takes the form <span id="eq:ch5_eq136"><span class="math display">\[
\mathcal{L} = -\sum_i P_i \ln P_i + \lambda_0 \left( 1 - \sum_i P_i \right)
  + \sum_{j&gt;0} \lambda_j \left( v_j - \sum_i A_{ij} P_i \right),
\qquad(136)\]</span></span> where <span class="math inline">\(\lambda_0\)</span> is the Lagrange multiplier associated with the normalization constraint, and <span class="math inline">\(\lambda_j\)</span> is the Lagrange multiplier associated with the <span class="math inline">\(j^\text{th}\)</span> constraint. This constraint equation is equivalent to , but now all the details of how to compute the moments are specified in matrix <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>With this notation in hand we now proceed to rescale the problem. The first step consists of rescaling the terms to compute the entries of matrix <span class="math inline">\(\mathbf{A}\)</span>. As mentioned before, this is the key feature of the Bretthorst algorithm; the particular choice of rescaling factor used in the algorithm empirically promotes that the rescaled Lagrange multipliers are of the same order of magnitude. The rescaling takes the form <span id="eq:ch5_eq137"><span class="math display">\[
A_{ij}&#39; = \frac{A_{ij}}{G_j},
\qquad(137)\]</span></span> where <span class="math inline">\(G_j\)</span> serves to rescale the moments, providing numerical stability to the inference problem. Bretthorst proposes an empirical rescaling that satisfies <span id="eq:ch5_eq138"><span class="math display">\[
G_j^2 = \sum_i A_{ij}^2,
\qquad(138)\]</span></span> or in terms of our particular problem <span id="eq:ch5_eq139"><span class="math display">\[
G_j^2 = \sum_m \sum_p \left( m^{x_j} p^{y_j} \right)^2.
\qquad(139)\]</span></span> What this indicates is that each pair <span class="math inline">\(m_i^{x_j} p_i^{y_j}\)</span> is normalized by the square root of the sum of the all pairs of the same form squared.</p>
<p>Since we rescale the factors involved in computing the constraints, the constraints must also be rescaled simply as <span id="eq:ch5_eq140"><span class="math display">\[
v_j&#39; = \left\langle{m^{x_j} p^{y_j}}\right\rangle&#39; = 
\frac{\left\langle{m^{x_j} p^{y_j}}\right\rangle}{G_j}.
\qquad(140)\]</span></span> The Lagrange multipliers must compensate this rescaling since at the end of the day the probability must add up to the same value. Therefore we rescale the <span class="math inline">\(\lambda_j\)</span> terms as <span id="eq:ch5_eq141"><span class="math display">\[
\lambda_j&#39; = \lambda_j G_j,
\qquad(141)\]</span></span> such that any <span class="math inline">\(\lambda_j A_{ij} = \lambda_j&#39; A_{ij}&#39;\)</span>. If this empirical value for the rescaling factor makes the rescaled Lagrange multipliers <span class="math inline">\(\lambda_j&#39;\)</span> be of the same order of magnitude, this by itself would already improve the algorithm convergence. Bretthorst proposes another linear transformation to make the optimization routine even more efficient. For this we generate orthogonal constraints that make Newton-Raphson and similar algorithms converge faster. The transformation is as follows <span id="eq:ch5_eq142"><span class="math display">\[
A_{ik}&#39;&#39; = \sum_j {e}_{jk} A_{ij}&#39;,
\qquad(142)\]</span></span> for the entires of matrix <span class="math inline">\(\mathbf{A}\)</span>, and <span id="eq:ch5_eq143"><span class="math display">\[
v_k&#39;&#39; = \sum_j {e}_{jk} u_j&#39;,
\qquad(143)\]</span></span> for entires of the constraint vector <span class="math inline">\(\mathbf{v}\)</span>, finally <span id="eq:ch5_eq144"><span class="math display">\[
\lambda_k&#39;&#39; = \sum_j {e}_{jk} \beta_j,
\qquad(144)\]</span></span> for the Lagrange multipliers. Here <span class="math inline">\({e}_{jk}\)</span> is the <span class="math inline">\(j^\text{th}\)</span> component of the <span class="math inline">\(k^\text{th}\)</span> eigenvector of the matrix <span class="math inline">\(\mathbf{E}\)</span> with entries <span id="eq:ch5_eq145"><span class="math display">\[
{E}_{kj} = \sum_i {A}_{ik}&#39; {A}_{ij}&#39;.
\qquad(145)\]</span></span> This transformation guarantees that the matrix <span class="math inline">\(\mathbf{A}&#39;&#39;\)</span> has the property <span id="eq:ch5_eq146"><span class="math display">\[
\sum_i A_{ij}&#39;&#39; A_{jk}&#39;&#39; = \beta_j \delta_{jk},
\qquad(146)\]</span></span> where <span class="math inline">\(\beta_j\)</span> is the <span class="math inline">\(j^\text{th}\)</span> eigenvalue of the matrix <span class="math inline">\(\mathbf{E}\)</span> and <span class="math inline">\(\delta_{jk}\)</span> is the Kronecker delta function. What this means is that, as desired, the constraints are orthogonal to each other, improving the algorithm convergence speed.</p>
<h3 id="predicting-distributions-for-simple-repression-constructs">Predicting distributions for simple repression constructs</h3>
<p>Having explained the theoretical background along with the practical difficulties and a workaround strategy proposed by Bretthorst, we implemented the inference using the moments obtained from averaging over the variability along the cell cycle (See ). and present these inferences for both mRNA and protein levels respectively for different values of the repressor-DNA binding energy and repressor copy numbers per cell. From these plots we can easily appreciate that despite the fact that the mean of each distribution changes as the induction level changes, there is a lot of overlap between distributions. This as a consequence means that at the single-cell level cells cannot perfectly resolve between different inputs.</p>
<figure>
<img src="ch5_fig16.png" id="fig:ch5_fig16" data-short-caption="Maximum entropy mRNA distributions for simple repression constructs" alt="Figure 16: Maximum entropy mRNA distributions for simple repression constructs. mRNA distributions for different biophysical parameters. From left to right the repressor-DNA affinity decreases as defined by the three lacI operators O1 (-15.3 \; k_BT), O2 (-13.9 \; k_BT), and O3 (-9.7 \; k_BT). From top to bottom the mean repressor copy number per cell increases. The curves on each plot represent different IPTG concentrations. Each distribution was fitted using the first three moments of the mRNA distribution." /><figcaption aria-hidden="true">Figure 16: <strong>Maximum entropy mRNA distributions for simple repression constructs.</strong> mRNA distributions for different biophysical parameters. From left to right the repressor-DNA affinity decreases as defined by the three lacI operators O1 (<span class="math inline">\(-15.3 \; k_BT\)</span>), O2 (<span class="math inline">\(-13.9 \; k_BT\)</span>), and O3 (<span class="math inline">\(-9.7 \; k_BT\)</span>). From top to bottom the mean repressor copy number per cell increases. The curves on each plot represent different IPTG concentrations. Each distribution was fitted using the first three moments of the mRNA distribution.</figcaption>
</figure>
<figure>
<img src="ch5_fig17.png" id="fig:ch5_fig17" data-short-caption="Maximum entropy protein distributions for simple repression constructs" alt="Figure 17: Maximum entropy protein distributions for simple repression constructs. Protein distributions for different biophysical parameters. From left to right the repressor-DNA affinity decreases as defined by the three lacI operators O1 (-15.3 \; k_BT), O2 (-13.9 \; k_BT), and O3 (-9.7 \; k_BT). From top to bottom the mean repressor copy number per cell increases. The curves on each plot represent different IPTG concentrations. Each distribution was fitted using the first six moments of the protein distribution." /><figcaption aria-hidden="true">Figure 17: <strong>Maximum entropy protein distributions for simple repression constructs.</strong> Protein distributions for different biophysical parameters. From left to right the repressor-DNA affinity decreases as defined by the three lacI operators O1 (<span class="math inline">\(-15.3 \; k_BT\)</span>), O2 (<span class="math inline">\(-13.9 \; k_BT\)</span>), and O3 (<span class="math inline">\(-9.7 \; k_BT\)</span>). From top to bottom the mean repressor copy number per cell increases. The curves on each plot represent different IPTG concentrations. Each distribution was fitted using the first six moments of the protein distribution.</figcaption>
</figure>
<h3 id="comparison-with-experimental-data-1">Comparison with experimental data</h3>
<p>Now that we have reconstructed an approximation of the probability distribution <span class="math inline">\(P(m, p)\)</span> we can compare this with our experimental measurements. But just as detailed in the single-cell microscopy measurements are given in arbitrary units of fluorescence. Therefore we cannot compare directly our predicted protein distributions with these values. To get around this issue we use the fact that the fold-change in gene expression that we defined as the ratio of the gene expression level in the presence of the repressor and the expression level of a knockout strain is a non-dimensional quantity. Therefore we normalize all of our single-cell measurements by the mean fluorescence value of the <span class="math inline">\(\Delta lacI\)</span> strain with the proper background fluorescence subtracted as explained in for the noise measurements. In the case of the theoretical predictions of the protein distribution we also normalize each protein value by the predicted mean protein level <span class="math inline">\(\left\langle p \right\rangle\)</span>, having now non-dimensional scales that can be directly compared. shows the experimental (color curves) and theoretical (dark dashed line) cumulative distribution functions for the three <span class="math inline">\(\Delta lacI\)</span> strains. As in , we do not expect differences between the operators, but we explicitly plot them separately to make sure that this is the case. We can see right away that as we would expect given the limitations of the model to accurately predict the noise and skewness of the distribution, the model doesn’t accurately predict the data. Our model predicts a narrower distribution compared to what we measured with single-cell microscopy.</p>
<figure>
<img src="ch5_fig18.png" id="fig:ch5_fig18" data-short-caption="Experiment vs. theory comparison for $\Delta lacI$ strain" alt="Figure 18: Experiment vs. theory comparison for \Delta lacI strain. Example fold-change empirical cumulative distribution functions (ECDF) for strains with no repressors and different operators. The color curves represent single-cell microscopy measurements while the dashed black lines represent the theoretical distributions as reconstructed by the maximum entropy principle. The theoretical distributions were fitted using the first six moments of the protein distribution." /><figcaption aria-hidden="true">Figure 18: <strong>Experiment vs. theory comparison for <span class="math inline">\(\Delta lacI\)</span> strain.</strong> Example fold-change empirical cumulative distribution functions (ECDF) for strains with no repressors and different operators. The color curves represent single-cell microscopy measurements while the dashed black lines represent the theoretical distributions as reconstructed by the maximum entropy principle. The theoretical distributions were fitted using the first six moments of the protein distribution.</figcaption>
</figure>
<p>The same narrower prediction applies to the regulated promoters. XXX, shows the theory-experiment comparison of the cumulative distribution functions for different repressor binding sites (different figures), repressor copy numbers (rows), and inducer concentrations (columns). In general the predictions are systematically narrower compared to the actual experimental data.</p>
<figure>
<img src="ch5_fig19.png" id="fig:ch5_fig19" data-short-caption="Experiment vs. theory comparison for regulated promoters" alt="Figure 19: Experiment vs. theory comparison for regulated promoters. Example fold-change empirical cumulative distribution functions (ECDF) for regulated strains with the three operators (different colors) as a function of repressor copy numbers (rows) and inducer concentrations (columns). The color curves represent single-cell microscopy measurements while the dashed black lines represent the theoretical distributions as reconstructed by the maximum entropy principle. The theoretical distributions were fitted using the first six moments of the protein distribution" /><figcaption aria-hidden="true">Figure 19: <strong>Experiment vs. theory comparison for regulated promoters.</strong> Example fold-change empirical cumulative distribution functions (ECDF) for regulated strains with the three operators (different colors) as a function of repressor copy numbers (rows) and inducer concentrations (columns). The color curves represent single-cell microscopy measurements while the dashed black lines represent the theoretical distributions as reconstructed by the maximum entropy principle. The theoretical distributions were fitted using the first six moments of the protein distribution</figcaption>
</figure>
<h2 id="gillespie-simulation-of-master-equation">Gillespie simulation of master equation</h2>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/gillespie_simulation.html">following link</a> as an annotated Jupyter notebook)</p>
<p>So far we have generated a way to compute an approximated form of the joint distribution of protein and mRNA <span class="math inline">\(P(m, p)\)</span> as a function of the moments of the distribution <span class="math inline">\(\left\langle m^x p^y \right\rangle\)</span>. This is a non-conventional form to work with the resulting distribution of the master equation. A more conventional approach to work with master equations whose closed-form solutions are not known or not computable is to use stochastic simulations commonly known as Gillespie simulations. To benchmark the performance of our approach based on distribution moments and maximum entropy we implemented the Gillespie algorithm. Our implementation as detailed in the corresponding Jupyter notebook makes use of just-in-time compilation as implemented with the Python package <a href="http://numba.pydata.org">numba</a>.</p>
<h3 id="mrna-distribution-with-gillespie-simulations">mRNA distribution with Gillespie simulations</h3>
<p>To confirm that the implementation of the Gillespie simulation was correct we perform the simulation at the mRNA level for which the closed-form solution of the steady-state distribution is known as detailed in . Fig. 20 shows example trajectories of mRNA counts. Each of these trajectories were computed over several cell cyles, where the cell division was implemented generating a binomially distributed random variable that depended on the last mRNA count before the division event.</p>
<figure>
<img src="ch5_fig20.png" id="fig:ch5_fig20" data-short-caption="Stochastic trajectories of mRNA counts" alt="Figure 20: Stochastic trajectories of mRNA counts. 100 stochastic trajectories generated with the Gillespie algorithm for mRNA counts over time for a two-state unregulated promoter. Cells spend a fraction of the cell cycle with a single copy of the promoter (light brown) and the rest of the cell cycle with two copies (light yellow). When trajectories reach a new cell cycle, the mRNA counts undergo a binomial partitioning to simulate the cell division." /><figcaption aria-hidden="true">Figure 20: <strong>Stochastic trajectories of mRNA counts.</strong> 100 stochastic trajectories generated with the Gillespie algorithm for mRNA counts over time for a two-state unregulated promoter. Cells spend a fraction of the cell cycle with a single copy of the promoter (light brown) and the rest of the cell cycle with two copies (light yellow). When trajectories reach a new cell cycle, the mRNA counts undergo a binomial partitioning to simulate the cell division.</figcaption>
</figure>
<p>To check the implementation of our stochastic algorithm we generated several of these stochastic trajectories in order to reconstruct the mRNA steady-state distribution. These reconstructed distributions for a single- and double-copy of the promoter can be compared with - the steady-state distribution for the two-state promoter. Fig. 21 shows the great agreement between the stochastic simulation and the analytical result, confirming that our implementation of the Gillespie simulation is correct.</p>
<figure>
<img src="ch5_fig21.png" id="fig:ch5_fig21" data-short-caption="Comparison of analytical and simulated mRNA distribution" alt="Figure 21: Comparison of analytical and simulated mRNA distribution. Solid lines show the steady-state mRNA distributions for one copy (light blue) and two copies of the promoter (dark blue) as defined by . Shaded regions represent the corresponding distribution obtained using 2500 stochastic mRNA trajectories and taking the last cell-cyle to approximate the distribution." /><figcaption aria-hidden="true">Figure 21: <strong>Comparison of analytical and simulated mRNA distribution.</strong> Solid lines show the steady-state mRNA distributions for one copy (light blue) and two copies of the promoter (dark blue) as defined by . Shaded regions represent the corresponding distribution obtained using 2500 stochastic mRNA trajectories and taking the last cell-cyle to approximate the distribution.</figcaption>
</figure>
<h3 id="protein-distribution-with-gillespie-simulations">Protein distribution with Gillespie simulations</h3>
<p>Having confirmed that our implementation of the Gillespie algorithm that includes the binomial partitioning of molecules reproduces analytical results we extended the implementation to include protein counts. Fig. 22 shows representative trajectories for both mRNA and protein counts over several cell cycles. Specially for the protein we can see that it takes several cell cycles for counts to converge to the dynamical steady-state observed with the deterministic moment equations. Once this steady-state is reached, the ensemble of trajectories between cell cycles look very similar.</p>
<figure>
<img src="ch5_fig22.png" id="fig:ch5_fig22" data-short-caption="Stochastic trajectories of mRNA and protein counts" alt="Figure 22: Stochastic trajectories of mRNA and protein counts. 2500 protein counts over time for a two-state unregulated promoter. Cells spend a fraction of the cell cycle with a single copy of the promoter (light brown) and the rest of the cell cycle with two copies (light yellow). When trajectories reach a new cell cycle, the molecule counts undergo a binomial partitioning to simulate the cell division." /><figcaption aria-hidden="true">Figure 22: <strong>Stochastic trajectories of mRNA and protein counts.</strong> 2500 protein counts over time for a two-state unregulated promoter. Cells spend a fraction of the cell cycle with a single copy of the promoter (light brown) and the rest of the cell cycle with two copies (light yellow). When trajectories reach a new cell cycle, the molecule counts undergo a binomial partitioning to simulate the cell division.</figcaption>
</figure>
<p>From these trajectories we can compute the protein steady-state distribution, taking into account the cell-age distribution as detailed in Fig. 23. shows the comparison between this distribution and the one generated using the maximum entropy algorithm. Despite the notorious differences between the distributions, the Gillespie simulation and the maximum entropy results are indistinguishable in terms of the mean, variance, and skewness of the distribution. We remind the reader that the maximum entropy is an approximation of the distribution that gets better the more moments we add. We therefore claim that the approximation works sufficiently well for our purpose. The enormous advantage of the maximum entropy approach comes from the computation time. for the number of distributions that were needed for our calculations the Gillespie algorithm proved to be a very inefficient method given the large sample space. Our maximum entropy approach reduces the computation time by several orders of magnitude, allowing us to extensively explore different parameters of the regulatory model.</p>
<figure>
<img src="ch5_fig23.png" id="fig:ch5_fig23" data-short-caption="Comparison of protein distributions" alt="Figure 23: Comparison of protein distributions. Comparison of the protein distribution generated with Gillespie stochastic simulations (blue curve) and the maximum entropy approach presented in (orange curve). The upper panel shows the probability mass function. The lower panel compares the cumulative distribution functions." /><figcaption aria-hidden="true">Figure 23: <strong>Comparison of protein distributions.</strong> Comparison of the protein distribution generated with Gillespie stochastic simulations (blue curve) and the maximum entropy approach presented in (orange curve). The upper panel shows the probability mass function. The lower panel compares the cumulative distribution functions.</figcaption>
</figure>
<h2 id="computational-determination-of-the-channel-capacity">Computational determination of the channel capacity</h2>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/blahut_algorithm_channel_capacity.html">following link</a> as an annotated Jupyter notebook)</p>
<p>In this section we detail the computation of the channel capacity of the simple genetic circuit shown in . As detailed in the channel capacity is defined as the mutual information between input <span class="math inline">\(c\)</span> and output <span class="math inline">\(p\)</span> maximized over all possible input distributions <span class="math inline">\(P(c)\)</span> <span class="citation" data-cites="Shannon1948"> [<a href="#ref-Shannon1948" role="doc-biblioref">11</a>]</span>. In principle there are an infinite number of input distributions, so the task of finding <span class="math inline">\(\hat{P}(c)\)</span>, the input distribution at channel capacity, requires an algorithmic approach that guarantees the convergence to this distribution. Tkačik, Callan and Bialek developed a clever analytical approximation to find the <span class="math inline">\(\hat{P}(c)\)</span> distribution <span class="citation" data-cites="Tkacik2008a"> [<a href="#ref-Tkacik2008a" role="doc-biblioref">12</a>]</span>. The validity of their so-called small noise approximation requires the standard deviation of the output distribution <span class="math inline">\(P(p \mid c)\)</span> to be much smaller than the domain of the distribution. For our particular case such condition is not satisfied given the spread of the inferred protein distributions shown in .</p>
<p>Fortunately there exists a numerical algorithm to approximate <span class="math inline">\(\hat{P}(c)\)</span> for discrete distributions. In 1972 Richard Blahut and Suguru Arimoto independently came up with an algorithm mathematically shown to converge to <span class="math inline">\(\hat{P}(c)\)</span> <span class="citation" data-cites="Blahut1972"> [<a href="#ref-Blahut1972" role="doc-biblioref">9</a>]</span>. To compute both the theoretical and the experimental channel capacity shown in , we implemented Blahut’s algorithm. In the following section we detail the definitions needed for the algorithm. Then we detail how to compute the experimental channel capacity when the bins of the distribution are not clear given the intrinsic arbitrary nature of microscopy fluorescence measurements.</p>
<h3 id="blahuts-algorithm">Blahut’s algorithm</h3>
<p>Following <span class="citation" data-cites="Blahut1972"> [<a href="#ref-Blahut1972" role="doc-biblioref">9</a>]</span> we implemented the algorithm to compute the channel capacity. We define <span class="math inline">\(\mathbf{p_c}\)</span> to be an array containing the probability of each of the input inducer concentrations (twelve concentrations, See Methods). Each entry <span class="math inline">\(j\)</span> of the array is then of the form <span id="eq:ch5_eq147"><span class="math display">\[
p_c^{(j)} = P(c = c_j),
\qquad(147)\]</span></span> with <span class="math inline">\(j \in \{1, 2, \ldots, 12 \}\)</span>. The objective of the algorithm is to find the entries <span class="math inline">\(p_c^{(j)}\)</span> that maximize the mutual information between inputs and outputs. We also define <span class="math inline">\(\mathbf{Q}\)</span> to be a <span class="math inline">\(\vert \mathbf{p_c} \vert\)</span> by <span class="math inline">\(\vert \mathbf{p_{p \mid c}} \vert\)</span> matrix, where <span class="math inline">\(\vert \cdot \vert\)</span> specifies the length of the array, and <span class="math inline">\(\mathbf{p_{p \mid c}}\)</span> is an array containing the probability distribution of an output given a specific value of the input. In other words, the matrix <span class="math inline">\(\mathbf{Q}\)</span> recollects all of the individual output distribution arrays <span class="math inline">\(\mathbf{p_{p \mid c}}\)</span> into a single object. Then each entry of the matrix <span class="math inline">\(\mathbf{Q}\)</span> is of the form <span id="eq:ch5_eq148"><span class="math display">\[
Q^{(i, j)} = P(p = p_i \mid c = c_j).
\qquad(148)\]</span></span></p>
<p>For the case of the theoretical predictions of the channel capacity (Solid lines in ) the entries of matrix <span class="math inline">\(\mathbf{Q}\)</span> are given by the inferred maximum entropy distributions as shown in . In the next section we will discuss how to define this matrix for the case of the single-cell fluorescence measurements. Having defined these matrices we proceed to implement the algorithm shown in Figure 1 of <span class="citation" data-cites="Blahut1972"> [<a href="#ref-Blahut1972" role="doc-biblioref">9</a>]</span>.</p>
<h3 id="channel-capacity-from-arbitrary-units-of-fluorescence">Channel capacity from arbitrary units of fluorescence</h3>
<p>A difficulty when computing the channel capacity between inputs and outputs from experimental data is that ideally we would like to compute <span id="eq:ch5_eq149"><span class="math display">\[
C(g; c) \equiv \sup_{P(c)} I(g; c),
\qquad(149)\]</span></span> where <span class="math inline">\(g\)</span> is the gene expression level, and <span class="math inline">\(c\)</span> is the inducer concentration. But in reality we are computing <span id="eq:ch5_eq150"><span class="math display">\[
C(f(g); c) \equiv \sup_{P(c)} I(f(g); c),
\qquad(150)\]</span></span> where <span class="math inline">\(f(g)\)</span> is a function of gene expression that has to do with our mapping from the YFP copy number to some arbitrary fluorescent value as computed from the images taken with the microscope. The data processing inequality, as derived by Shannon himself, tells us that for a Markov chain of the form <span class="math inline">\(c \rightarrow g \rightarrow f(g)\)</span> it must be true that <span class="citation" data-cites="Shannon1948"> [<a href="#ref-Shannon1948" role="doc-biblioref">11</a>]</span> <span id="eq:ch5_eq151"><span class="math display">\[
I(g; c) \geq I(f(g); c),
\qquad(151)\]</span></span> meaning that information can only be lost when mapping from the real relationship between gene expression and inducer concentration to a fluorescence value.</p>
<p>On top of that, given the limited number of samples that we have access to when computing the channel capacity, there is a bias in our estimate given this undersampling. The definition of accurate unbiased descriptors of the mutual information is still an area of active research. For our purposes we will use the method described in <span class="citation" data-cites="Cheong2011a"> [<a href="#ref-Cheong2011a" role="doc-biblioref">3</a>]</span>. The basic idea of the method is to write the mutual information as a series expansion in terms of inverse powers of the sample size, i.e. <span id="eq:ch5_eq152"><span class="math display">\[
I_{\text{biased}} = I_\infty + \frac{a_1}{N} + \frac{a_2}{N^2} + \cdots,
\qquad(152)\]</span></span> where <span class="math inline">\(I_{\text{biased}}\)</span> is the biased estimate of the mutual information as computed from experimental data, <span class="math inline">\(I_\infty\)</span> is the quantity we would like to estimate, being the unbiased mutual information when having access to infinity number of experimental samples, and the coefficients <span class="math inline">\(a_i\)</span> depend on the underlying distribution of the signal and the response. This is an empirical choice to be tested. Intuitively this choice satisfies the limit that as the number of samples from the distribution grows, the empirical estimate of the mutual information <span class="math inline">\(I_{\text{biased}}\)</span> should get closer to the actual value <span class="math inline">\(I_\infty\)</span>.</p>
<p>In principle for a good number of data points the terms of higher order become negligible. So we can write the mutual information as <span id="eq:ch5_eq153"><span class="math display">\[
I_{\text{biased}} \approx I_\infty + \frac{a_1}{N} + \mathcal{O}(N^{-2}).
\qquad(153)\]</span></span> This means that if this particular arbitrary choice of functional form is a good approximation, when computing the mutual information for varying number of samples - by taking subsamples of the experimental data - we expect to find a linear relationship as a function of the inverse of these number of data points. From this linear relationship the intercept is a bias-corrected estimate of the mutual information. We can therefore bootstrap the data by taking different sample sizes and then use the Blahut-Arimoto algorithm we implemented earlier to estimate the biased channel capacity. We can then fit a line and extrapolate for when <span class="math inline">\(1/N = 0\)</span> which corresponds to our unbiased estimate of the channel capacity.</p>
<p>Let’s go through each of the steps to illustrate the method. Fig. 24 show a typical data set for a strain with an O2 binding site (<span class="math inline">\(\Delta\varepsilon_r = -13.9 \; k_BT\)</span>) and <span class="math inline">\(R = 260\)</span> repressors per cell. Each of the distributions in arbitrary units is binned into a specified number of bins to build matrix <span class="math inline">\(\mathbf{Q}\)</span>.</p>
<figure>
<img src="ch5_fig24.png" id="fig:ch5_fig24" data-short-caption="Single cell fluorescence distributions for different inducer concentrations" alt="Figure 24: Single cell fluorescence distributions for different inducer concentrations. Fluorescence distribution histogram (A) and cumulative distribution function (B) for a strain with 260 repressors per cell and a binding site with binding energy \Delta\varepsilon_r = -13.9\; k_BT. The different curves show the single cell fluorescence distributions under the 12 different IPTG concentrations used throughout this work. The triangles in (A) show the mean of each of the distributions." /><figcaption aria-hidden="true">Figure 24: <strong>Single cell fluorescence distributions for different inducer concentrations.</strong> Fluorescence distribution histogram (A) and cumulative distribution function (B) for a strain with 260 repressors per cell and a binding site with binding energy <span class="math inline">\(\Delta\varepsilon_r = -13.9\; k_BT\)</span>. The different curves show the single cell fluorescence distributions under the 12 different IPTG concentrations used throughout this work. The triangles in (A) show the mean of each of the distributions.</figcaption>
</figure>
<p>Given a specific number of bins used to construct <span class="math inline">\(\mathbf{Q}\)</span>, we subsample a fraction of the data and compute the channel capacity for such matrix using the Blahut-Arimoto algorithm. Fig. 25 shows an example where 50% of the data on each distribution from was sampled and binned into 100 equal bins. The counts on each of these bins are then normalized and used to build matrix <span class="math inline">\(\mathbf{Q}\)</span> that is then fed to the Blahut-Arimoto algorithm. We can see that for these 200 bootstrap samples the channel capacity varies by <span class="math inline">\(\approx\)</span> 0.1 bits. Not a significant variability, nevertheless we consider that it is important to bootstrap the data multiple times to get a better estimate of the channel capacity.</p>
<figure>
<img src="ch5_fig25.png" id="fig:ch5_fig25" data-short-caption="Channel capacity bootstrap for experimental data" alt="Figure 25: Channel capacity bootstrap for experimental data. Cumulative distribution function of the resulting channel capacity estimates obtained by subsampling 200 times 50% of each distribution shown in , binning it into 100 bins, and feeding the resulting \mathbf{Q} matrix to the Blahut-Arimoto algorithm." /><figcaption aria-hidden="true">Figure 25: <strong>Channel capacity bootstrap for experimental data.</strong> Cumulative distribution function of the resulting channel capacity estimates obtained by subsampling 200 times 50% of each distribution shown in , binning it into 100 bins, and feeding the resulting <span class="math inline">\(\mathbf{Q}\)</span> matrix to the Blahut-Arimoto algorithm.</figcaption>
</figure>
<p>XXX tells us that if we subsample each of the distributions from at different fractions, and plot them as a function of the inverse sample size we will find a linear relationship if the expansion of the mutual information is valid. To test this idea we repeated the bootstrap estimate of sampling 10%, 20%, and so on until taking 100% of the data. We repeated this for different number of bins since <em>a priori</em> for arbitrary units of fluorescence we do not have a way to select the optimal number of bins. Fig. 26 shows the result of these estimates. We can see that the linear relationship proposed in holds true for all number of bins selected. We also note that the value of the intercept of the linear regression varies depending on the number of bins.</p>
<figure>
<img src="ch5_fig26.png" id="fig:ch5_fig26" data-short-caption="Inverse sample size vs channel capacity" alt="Figure 26: Inverse sample size vs channel capacity. As indicated in if the channel capacity obtained for different subsample sizes of the data is plotted against the inverse sample size there must exist a linear relationship between these variables. Here we perform 15 bootstrap samples of the data from , bin these samples using different number of bins, and perform a linear regression (solid lines) between the bootstrap channel capacity estimates, and the inverse sample size." /><figcaption aria-hidden="true">Figure 26: <strong>Inverse sample size vs channel capacity.</strong> As indicated in if the channel capacity obtained for different subsample sizes of the data is plotted against the inverse sample size there must exist a linear relationship between these variables. Here we perform 15 bootstrap samples of the data from , bin these samples using different number of bins, and perform a linear regression (solid lines) between the bootstrap channel capacity estimates, and the inverse sample size.</figcaption>
</figure>
<p>To address the variability in the estimates of the unbiased channel capacity <span class="math inline">\(I_\infty\)</span> we again follow the methodology suggested in <span class="citation" data-cites="Cheong2011a"> [<a href="#ref-Cheong2011a" role="doc-biblioref">3</a>]</span>. We perform the data subsampling and computation of the channel capacity for a varying number of bins. As a control we perform the same procedure with shuffled data, where the structure that connects the fluorescence distribution to the inducer concentration input is lost. The expectation is that this control should give a channel capacity of zero if the data is not “over-binned.” Once the number of bins is too high, we would expect some structure to emerge in the data that would cause the Blahut-Arimoto algorithm to return non-zero channel capacity estimates.</p>
<p>[Fig:ch5_fig27] shows the result of the unbiased channel capacity estimates obtained for the data shown in . For the blue curve we can distinguish three phases: 1. A rapid increment from 0 bits to about 1.5 bits as the number of bins increases. 2. A flat region between <span class="math inline">\(\approx\)</span> 50 and 1000 bins. 3. A second rapid increment for large number of bins.</p>
<p>We can see that the randomized data presents two phases only: 1. A flat region where there is, as expected no information being processed since the structure of the data was lost when the data was shuffled. 2. A region with fast growth of the channel capacity as the over-binning generates separated peaks on the distribution, making it look like there is structure in the data.</p>
<p>We take the flat region of the experimental data (<span class="math inline">\(\approx\)</span> 100 bins) to be our best unbiased estimate of the channel capacity from this experimental dataset.</p>
<figure>
<img src="ch5_fig27.png" id="fig:ch5_fig27" data-short-caption="Channel capacity as a function of the number of bins" alt="Figure 27: Channel capacity as a function of the number of bins. Unbiased channel capacity estimates obtained from linear regressions as in . The blue curve show the estimates obtained from the data shown in . The orange curve is generated from estimates where the same data is shuffled, loosing the relationship between fluorescence distributions and inducer concentration." /><figcaption aria-hidden="true">Figure 27: <strong>Channel capacity as a function of the number of bins.</strong> Unbiased channel capacity estimates obtained from linear regressions as in . The blue curve show the estimates obtained from the data shown in . The orange curve is generated from estimates where the same data is shuffled, loosing the relationship between fluorescence distributions and inducer concentration.</figcaption>
</figure>
<h3 id="assumptions-involved-in-the-computation-of-the-channel-capacity">Assumptions involved in the computation of the channel capacity</h3>
<p>An interesting suggestion by Professor Gasper Tkacik was to dissect the different physical assumptions that went into the construction of the input-output function <span class="math inline">\(P(p \mid c)\)</span>, and their relevance when comparing the theoretical channel capacities with the experimental inferences. In what follows we describe the relevance of four important aspects that all affect the predictions of the information processing capacity.</p>
<h4 id="i-cell-cycle-variability.">(i) Cell cycle variability.</h4>
<p>We think that the inclusion of the gene copy number variability during the cell cycle and the non-Poissoninan protein degradation is a key component to our estimation of the input-output functions and as a consequence of the channel capacity. This variability in gene copy number is an additional source of noise that systematically decreases the ability of the system to resolve different inputs. The absence of the effects that the gene copy number variability and the protein partition has on the information processing capacity leads to an overestimate of the channel capacity as shown in [Fig:ch5_fig28]. Only when these noise sources are included in our inferences is that we get to capture the experimental channel capacities with no further fit parameters.</p>
<figure>
<img src="ch5_fig28.png" id="fig:ch5_fig28" data-short-caption="Comparison of channel capacity predictions for single- and multi-promoter models" alt="Figure 28: Comparison of channel capacity predictions for single- and multi-promoter models. Channel capacity for the multi-promoter model (solid lines) vs. the single-promoter steady state model (dot-dashed lines) as a function of repressor copy numbers for different repressor-DNA binding energies. The single-promoter model assumes Poissonian protein degradation (\gamma _p &gt; 0) and steady state, while the multi-promoter model accounts for gene copy number variability and during the cell cycle and has protein degradation as an effect due to dilution as cells grow and divide." /><figcaption aria-hidden="true">Figure 28: <strong>Comparison of channel capacity predictions for single- and multi-promoter models.</strong> Channel capacity for the multi-promoter model (solid lines) vs. the single-promoter steady state model (dot-dashed lines) as a function of repressor copy numbers for different repressor-DNA binding energies. The single-promoter model assumes Poissonian protein degradation (<span class="math inline">\(\gamma _p &gt; 0\)</span>) and steady state, while the multi-promoter model accounts for gene copy number variability and during the cell cycle and has protein degradation as an effect due to dilution as cells grow and divide.</figcaption>
</figure>
<h4 id="ii-non-gaussian-noise-distributions.">(ii) Non-Gaussian noise distributions.</h4>
<p>For the construction of the probability distributions used in the main text () we utilized the first 6 moments of the protein distribution. The maximum entropy formalism tells us that the more constraints we include in the inference, the closer the maximum entropy distribution will be to the real distribution. But <em>a priori</em> there is no way of knowing how many moments should be included in order to capture the essence of the distribution. In principle two moments could suffice to describe the entire distribution as happens with the Gaussian distribution. To compare the effect that including more or less constraints on the maximum entropy inference we constructed maximum entropy distributions using an increasing number of moments from 2 to 6. We then computed the Kullback-Leibler divergence <span class="math inline">\(D_{KL}\)</span> of the form <span id="eq:ch5_eq154"><span class="math display">\[
D_{KL}(P_6(p \mid c) || P_i(p \mid c)) =
    \sum_p P_6(p \mid c) \log_2 \frac{P_6(p \mid c)}{P_i(p \mid c)},
\qquad(154)\]</span></span> where <span class="math inline">\(P_i(p \mid c)\)</span> is the maximum entropy distribution constructed with the first <span class="math inline">\(i\)</span> moments, <span class="math inline">\(i \in \{2, 3, 4, 5, 6\}\)</span>. Since the Kullback-Leibler divergence <span class="math inline">\(D_{KL}(P || Q)\)</span> can be interpreted as the amount of information lost by assuming the incorrect distribution <span class="math inline">\(Q\)</span> when the correct distribution is <span class="math inline">\(P\)</span>, we used this metric as a way of how much information we would have lost by using less constraints compared to the six moments used in the main text.</p>
<p>Fig. 29 shows this comparison for different operators and repressor copy numbers. We can see from here that using less moments as constraints gives basically the same result. This is because most of the values of the Kullback-Leibler divergence are significantly smaller than 0.1 bits, and the entropy of these distributions is in general <span class="math inline">\(&gt; 10\)</span> bits, so we would lose less than 1% of the information contained in these distributions by utilizing only two moments as constraints. Therefore the use of non-Gaussian noise is not an important feature for our inferences.</p>
<figure>
<img src="ch5_fig29.png" id="fig:ch5_fig29" data-short-caption="Measuring the loss of information by using different number of constraints" alt="Figure 29: Measuring the loss of information by using different number of constraints. The Kullback-Leibler divergence was computed between the maximum entropy distribution constructed using the first 6 moments of the distribution and a variable number of moments." /><figcaption aria-hidden="true">Figure 29: <strong>Measuring the loss of information by using different number of constraints.</strong> The Kullback-Leibler divergence was computed between the maximum entropy distribution constructed using the first 6 moments of the distribution and a variable number of moments.</figcaption>
</figure>
<h4 id="iii-multi-state-promoter.">(iii) Multi-state promoter.</h4>
<p>This particular point is something that we are still exploring from a theoretical perspective. We have shown that in order to capture the single-molecule mRNA FISH data a single-state promoter wouldn’t suffice. This model predicts a Poisson distribution as the steady-state and the data definitely shows super Poissonian noise. Given the bursty nature of gene expression we opt to use a two-state promoter where the states reflect effective transcriptionally “active” and “inactive” states. We are currently exploring alternative formulations of this model to turn it into a single state with a geometrically distributed burst-size.</p>
<h4 id="iv-optimal-vs-log-flat-distributions.">(iv) Optimal vs Log-flat Distributions.</h4>
<p>The relevance of having use the Blahut-Arimoto algorithm to predict the maximum mutual information between input and outputs was just to understand the best case scenario. We show the comparison between theoretical and experimental input-output functions <span class="math inline">\(P(p \mid c)\)</span> in . Given the good agreement between these distributions we could compute the mutual information <span class="math inline">\(I(c; p)\)</span> for any arbitrary input distribution <span class="math inline">\(P(c)\)</span> and obtain a good agreement with the corresponding experimental mutual information.</p>
<p>The reason we opted to specifically report the mutual information at channel capacity was to put the results in a context. By reporting the upper bound in performance of these genetic circuits we can start to dissect how different molecular parameters such as repressor-DNA binding affinity or repressor copy number affect the ability of this genetic circuit to extract information from the environmental state.</p>
<h2 id="empirical-fits-to-noise-predictions">Empirical fits to noise predictions</h2>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu/chann_cap/src/theory/html/empirical_constants.html">following link</a> as an annotated Jupyter notebook)</p>
<p>In <a href="C">Fig:ch3_fig03</a> in the main text we show that our minimal model has a systematic deviation on the gene expression noise predictions compared to the experimental data. This systematics will need to be addressed on an improved version of the minimal model presented in this work. To guide the insights into the origins of this systematic deviation in this appendix we will explore empirical modifications of the model to improve the agreement between theory and experiment.</p>
<h3 id="multiplicative-factor-for-the-noise">Multiplicative factor for the noise</h3>
<p>The first option we will explore is to modify our noise predictions by a constant multiplicative factor. This means that we assume the relationship between our minimal model predictions and the data for noise in gene expression are of the from <span id="eq:ch5_eq155"><span class="math display">\[
\text{noise}_{\text{exp}} = \alpha \cdot \text{noise}_{\text{theory}},
\qquad(155)\]</span></span> where <span class="math inline">\(\alpha\)</span> is a dimensionless constant to be fit from the data. The data, especially in suggests that our predictions are within a factor of <span class="math inline">\(\approx\)</span> two from the experimental data. To further check that intuition we performed a weighted linear regression between the experimental and theoretical noise measurements. The weight for each datum was taken to be proportional to the bootstrap errors in the noise estimate, this to have poorly determined noises weigh less during the regression. The result of this regression with no intercept shows exactly that a factor of two systematically improves the theoretical vs. experimental predictions. Fig. 30 shows the improved agreement when the theoretical predictions for the noise are multiplied by <span class="math inline">\(\approx 1.5\)</span>.</p>
<figure>
<img src="ch5_fig30.png" id="fig:ch5_fig30" data-short-caption="Multiplicative factor to improve theoretical vs. experimental comparison of noise in gene expression" alt="Figure 30: Multiplicative factor to improve theoretical vs. experimental comparison of noise in gene expression. Theoretical vs. experimental noise both in linear (left) and log (right) scale. The dashed line shows the identity line of slope 1 and intercept zero. All data are colored by the corresponding value of the experimental fold-change in gene expression as indicated by the color bar. The x-axis was multiplied by a factor of \approx 1.5 as determined by a linear regression from the data in . Each datum represents a single date measurement of the corresponding strain and IPTG concentration with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples." /><figcaption aria-hidden="true">Figure 30: <strong>Multiplicative factor to improve theoretical vs. experimental comparison of noise in gene expression.</strong> Theoretical vs. experimental noise both in linear (left) and log (right) scale. The dashed line shows the identity line of slope 1 and intercept zero. All data are colored by the corresponding value of the experimental fold-change in gene expression as indicated by the color bar. The <span class="math inline">\(x\)</span>-axis was multiplied by a factor of <span class="math inline">\(\approx 1.5\)</span> as determined by a linear regression from the data in . Each datum represents a single date measurement of the corresponding strain and IPTG concentration with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples.</figcaption>
</figure>
<p>For completeness Fig. 31 shows the noise in gene expression as a function of the inducer concentration including this factor of <span class="math inline">\(\approx 1.5\)</span>. It is clear that overall a simple multiplicative factor improves the predictive power of the model.</p>
<figure>
<img src="ch5_fig31.png" id="fig:ch5_fig31" data-short-caption="Protein noise of the regulated promoter with multiplicative factor" alt="Figure 31: Protein noise of the regulated promoter with multiplicative factor. Comparison of the experimental noise for different operators ((A) O1, \Delta\varepsilon_r = -15.3 \; k_BT, (B) O2, \Delta\varepsilon_r = -13.9 \; k_BT, (C) O3, \Delta\varepsilon_r = -9.7 \; k_BT) with the theoretical predictions for the the multi-promoter model. A linear regression revealed that multiplying the theoretical noise prediction by a factor of \approx 1.5 would improve agreement between theory and data. Points represent the experimental noise as computed from single-cell fluorescence measurements of different E. coli strains under 12 different inducer concentrations. Dotted line indicates plot in linear rather than logarithmic scale. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. White-filled dots are plot at a different scale for better visualization." /><figcaption aria-hidden="true">Figure 31: <strong>Protein noise of the regulated promoter with multiplicative factor.</strong> Comparison of the experimental noise for different operators ((A) O1, <span class="math inline">\(\Delta\varepsilon_r = -15.3 \; k_BT\)</span>, (B) O2, <span class="math inline">\(\Delta\varepsilon_r = -13.9 \; k_BT\)</span>, (C) O3, <span class="math inline">\(\Delta\varepsilon_r = -9.7 \; k_BT\)</span>) with the theoretical predictions for the the multi-promoter model. A linear regression revealed that multiplying the theoretical noise prediction by a factor of <span class="math inline">\(\approx 1.5\)</span> would improve agreement between theory and data. Points represent the experimental noise as computed from single-cell fluorescence measurements of different <em>E. coli</em> strains under 12 different inducer concentrations. Dotted line indicates plot in linear rather than logarithmic scale. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. White-filled dots are plot at a different scale for better visualization.</figcaption>
</figure>
<h3 id="additive-factor-for-the-noise">Additive factor for the noise</h3>
<p>As an alternative way to empirically improve the predictions of our model we will now test the idea of an additive constant. What this means is that our minimal model underestimates the noise in gene expression as <span id="eq:ch5_eq156"><span class="math display">\[
\text{noise}_{\text{exp}} = \beta + \text{noise}_{\text{theory}},
\qquad(156)\]</span></span> where <span class="math inline">\(\beta\)</span> is an additive constant to be determined from the data. As with the multiplicative constant we performed a regression to determine this empirical additive constant comparing experimental and theoretical gene expression noise values. We use the error in the 95% bootstrap confidence interval as a weight for the linear regression. Fig. 32 shows the resulting theoretical vs. experimental noise where <span class="math inline">\(\beta \approx 0.2\)</span>. We can see a great improvement in the agreement between theory and experiment with this additive constant.</p>
<figure>
<img src="ch5_fig32.png" id="fig:ch5_fig32" data-short-caption="Additive factor to improve theoretical vs. experimental comparison of noise in gene expression" alt="Figure 32: Additive factor to improve theoretical vs. experimental comparison of noise in gene expression. Theoretical vs. experimental noise both in linear (left) and log (right) scale. The dashed line shows the identity line of slope 1 and intercept zero. All data are colored by the corresponding value of the experimental fold-change in gene expression as indicated by the color bar. A value of \approx 0.2 was added to all values in the x-axis as determined by a linear regression from the data in . Each datum represents a single date measurement of the corresponding strain and IPTG concentration with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples." /><figcaption aria-hidden="true">Figure 32: <strong>Additive factor to improve theoretical vs. experimental comparison of noise in gene expression.</strong> Theoretical vs. experimental noise both in linear (left) and log (right) scale. The dashed line shows the identity line of slope 1 and intercept zero. All data are colored by the corresponding value of the experimental fold-change in gene expression as indicated by the color bar. A value of <span class="math inline">\(\approx 0.2\)</span> was added to all values in the <span class="math inline">\(x\)</span>-axis as determined by a linear regression from the data in . Each datum represents a single date measurement of the corresponding strain and IPTG concentration with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples.</figcaption>
</figure>
<p>For completeness Fig. 33 shows the noise in gene expression as a function of the inducer concentration including this additive factor of <span class="math inline">\(\beta \approx 0.2\)</span>. If anything, the additive factor seems to improve the agreement between theory and data even more than the multiplicative factor.</p>
<figure>
<img src="ch5_fig33.png" id="fig:ch5_fig33" data-short-caption="Protein noise of the regulated promoter with additive factor" alt="Figure 33: Protein noise of the regulated promoter with additive factor. Comparison of the experimental noise for different operators ((A) O1, \Delta\varepsilon_r = -15.3 \; k_BT, (B) O2, \Delta\varepsilon_r = -13.9 \; k_BT, (C) O3, \Delta\varepsilon_r = -9.7 \; k_BT) with the theoretical predictions for the the multi-promoter model. A linear regression revealed that an additive factor of \approx 0.2 to the the theoretical noise prediction would improve agreement between theory and data. Points represent the experimental noise as computed from single-cell fluorescence measurements of different E. coli strains under 12 different inducer concentrations. Dotted line indicates plot in linear rather than logarithmic scale. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. White-filled dots are plot at a different scale for better visualization." /><figcaption aria-hidden="true">Figure 33: <strong>Protein noise of the regulated promoter with additive factor.</strong> Comparison of the experimental noise for different operators ((A) O1, <span class="math inline">\(\Delta\varepsilon_r = -15.3 \; k_BT\)</span>, (B) O2, <span class="math inline">\(\Delta\varepsilon_r = -13.9 \; k_BT\)</span>, (C) O3, <span class="math inline">\(\Delta\varepsilon_r = -9.7 \; k_BT\)</span>) with the theoretical predictions for the the multi-promoter model. A linear regression revealed that an additive factor of <span class="math inline">\(\approx 0.2\)</span> to the the theoretical noise prediction would improve agreement between theory and data. Points represent the experimental noise as computed from single-cell fluorescence measurements of different <em>E. coli</em> strains under 12 different inducer concentrations. Dotted line indicates plot in linear rather than logarithmic scale. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. White-filled dots are plot at a different scale for better visualization.</figcaption>
</figure>
<h3 id="correction-factor-for-channel-capacity-with-multiplicative-factor">Correction factor for channel capacity with multiplicative factor</h3>
<p>As seen in a constant multiplicative factor can reduce the discrepancy between the model predictions and the data with respect to the noise (standard deviation / mean) in protein copy number. To find the equivalent correction would be for the channel capacity requires gaining insights from the so-called small noise approximation <span class="citation" data-cites="Tkacik2008a"> [<a href="#ref-Tkacik2008a" role="doc-biblioref">12</a>]</span>. The small noise approximation assumes that the input-output function can be modeled as a Gaussian distribution in which the standard deviation is small. Using these assumptions one can derive a closed-form for the channel capacity. Although our data and model predictions do not satisfy the requirements for the small noise approximation, we can gain some intuition for how the channel capacity would scale given a systematic deviation in the cell-to-cell variability predictions compared with the data.</p>
<p>Using the small noise approximation one can derive the form of the input distribution at channel capacity <span class="math inline">\(P^*(c)\)</span>. To do this we use the fact that there is a deterministic relationship between the input inducer concentration <span class="math inline">\(c\)</span> and the mean output protein value <span class="math inline">\(\left\langle p \right\rangle\)</span>, therefore we can work with <span class="math inline">\(P(\left\langle p \right\rangle)\)</span> rather than <span class="math inline">\(P(c)\)</span> since the deterministic relation allows us to write <span id="eq:ch5_eq157"><span class="math display">\[
P(c) dc = P(\left\langle p \right\rangle) d\left\langle p \right\rangle.
\qquad(157)\]</span></span> Optimizing over all possible distributions <span class="math inline">\(P(\left\langle p \right\rangle)\)</span> using calculus of variations results in a distribution of the form <span id="eq:ch5_eq158"><span class="math display">\[
P^*(\left\langle p \right\rangle) = 
\frac{1}{\mathcal{Z}} \frac{1}{\sigma_p(\left\langle p \right\rangle)},
\qquad(158)\]</span></span> where <span class="math inline">\(\sigma_p(\left\langle p \right\rangle)\)</span> is the standard deviation of the protein distribution as a function of the mean protein expression, and <span class="math inline">\(\mathcal{Z}\)</span> is a normalization constant defined as <span id="eq:ch5_eq159"><span class="math display">\[
\mathcal{Z} \equiv 
\int_{\left\langle{p(c=0)}\right\rangle}
^{\left\langle{p(c\rightarrow \infty)}\right\rangle}
\frac{1}{\sigma_p(\left\langle p \right\rangle)} d\left\langle p \right\rangle.
\qquad(159)\]</span></span> Under these assumptions the small noise approximation tells us that the channel capacity is of the form <span class="citation" data-cites="Tkacik2008a"> [<a href="#ref-Tkacik2008a" role="doc-biblioref">12</a>]</span> <span id="eq:ch5_eq160"><span class="math display">\[
I = \log_2 \left( \frac{\mathcal{Z}}{\sqrt{2 \pi e}} \right).
\qquad(160)\]</span></span></p>
<p>From the theory-experiment comparison in we know that the standard deviation predicted by our model is systematically off by a factor of two compared to the experimental data, i.e. <span id="eq:ch5_eq161"><span class="math display">\[
\sigma_p^{\exp} = 2 \sigma_p^{\text{theory}}.
\qquad(161)\]</span></span> This then implies that the normalization constant <span class="math inline">\(\mathcal{Z}\)</span> between theory and experiment must follow a relationship of the form <span id="eq:ch5_eq162"><span class="math display">\[
\mathcal{Z}^{\exp} = \frac{1}{2} \mathcal{Z}^{\text{theory}}.
\qquad(162)\]</span></span> With this relationship the small noise approximation would predict that the difference between the experimental and theoretical channel capacity should be of the form <span id="eq:ch5_eq163"><span class="math display">\[
I^{\exp} = \log_2 \left( \frac{\mathcal{Z}^{\exp}}{\sqrt{2 \pi e}} \right)
= \log_2 \left( \frac{\mathcal{Z}^{\text{theory}}}{\sqrt{2 \pi e}} \right)
- \log_2(2).
\qquad(163)\]</span></span> Therefore under the small noise approximation we would expect our predictions for the channel capacity to be off by a constant of 1 bit (<span class="math inline">\(\log_2(2)\)</span>) of information. Again, the conditions for the small noise approximation do not apply to our data given the intrinsic level of cell-to-cell variability in the system, nevertheless what this analysis tells is is that we expect that an additive constant should be able to explain the discrepancy between our model predictions and the experimental channel capacity. To test this hypothesis we performed a “linear regression” between the model predictions and the experimental channel capacity with a fixed slope of 1. The intercept of this regression, -0.56 bits, indicates the systematic deviation we expect should explain the difference between our model and the data. Fig. 34 shows the comparison between the original predictions shown in (A) and the resulting predictions with this shift. Other than the data with zero channel capacity, this shift is able to correct the systematic deviation for all data. We therefore conclude that our model ends up underestimating the experimentally determined channel capacity by a constant amount of 0.43 bits.</p>
<figure>
<img src="ch5_fig34.png" id="fig:ch5_fig34" data-short-caption="Additive correction factor for channel capacity" alt="Figure 34: Additive correction factor for channel capacity. Solid lines represent the theoretical predictions of the channel capacity shown in (A). The dashed lines show the resulting predictions with a constant shift of -0.43 bits. Points represent single biological replicas of the inferred channel capacity." /><figcaption aria-hidden="true">Figure 34: <strong>Additive correction factor for channel capacity.</strong> Solid lines represent the theoretical predictions of the channel capacity shown in (A). The dashed lines show the resulting predictions with a constant shift of -0.43 bits. Points represent single biological replicas of the inferred channel capacity.</figcaption>
</figure>
<h2 id="derivation-of-the-cell-age-distribution">Derivation of the cell age distribution</h2>
<p>E. O. Powell first derive in 1956 the distribution of cell age for a cell population growing steadily in the exponential phase <span class="citation" data-cites="Powell1956"> [<a href="#ref-Powell1956" role="doc-biblioref">16</a>]</span>. This distribution is of the form <span id="eq:ch5_eq164"><span class="math display">\[
P(a) = \ln(2) \cdot 2^{1 - a},
\qquad(164)\]</span></span> where <span class="math inline">\(a \in [0, 1]\)</span> is the fraction of the cell cycle, 0 being the moment right after the mother cell divides, and 1 being the end of the cell cycle just before cell division. In this section we will reproduce and expand the details on each of the steps of the derivation.</p>
<p>For an exponentially growing bacterial culture, the cells satisfy the growth law <span id="eq:ch5_eq165"><span class="math display">\[
{\frac{dn}{dt}} = \mu n,
\qquad(165)\]</span></span> where <span class="math inline">\(n\)</span> is the number of cells and <span class="math inline">\(\mu\)</span> is the growth rate in units of time<span class="math inline">\(^{-1}\)</span>. We begin by defining <span class="math inline">\(P(a)\)</span> to be the probability density function of a cell having age <span class="math inline">\(a\)</span>. At time zero of a culture in exponential growth, i.e. the time when we start considering the growth, not the initial condition of the culture, there are <span class="math inline">\(NP(a)da\)</span> cells with age range between <span class="math inline">\([a, a + da]\)</span>. In other words, for <span class="math inline">\(N \gg 1\)</span> and <span class="math inline">\(da \ll a\)</span> <span id="eq:ch5_eq166"><span class="math display">\[
N P(a \leq x \leq a + da) \approx N P(a)da.
\qquad(166)\]</span></span> We now define <span id="eq:ch5_eq167"><span class="math display">\[
F(\tau) = \int_\tau^\infty f(\xi) d\xi,
\qquad(167)\]</span></span> as the fraction of cells whose division time is greater than <span class="math inline">\(\tau\)</span>. This is because in principle not all cells divide exactly after <span class="math inline">\(\tau\)</span> minutes, but there is a distribution function <span class="math inline">\(f(\tau)\)</span> for the division time after birth. Empirically it has been observed that a generalize Gamma distribution fits well to experimental data on cell division time, but we will worry about this specific point later on.</p>
<p>From the definition of <span class="math inline">\(F(\tau)\)</span> we can see that if a cell reaches an age <span class="math inline">\(a\)</span>, the probability of surviving to an age <span class="math inline">\(a + t\)</span> without dividing is given by <span id="eq:ch5_eq168"><span class="math display">\[
P(\text{age} = (a + t) \mid \text{age} = a) = F(a + t \mid a) =
\frac{F(a + t)}{F(a)}.
\qquad(168)\]</span></span> This result comes simply from the definition of conditional probability. Since <span class="math inline">\(F(a)\)</span> is the probability of surviving <span class="math inline">\(a\)</span> or more minutes without dividing, by the definition of conditional probability we have that <span id="eq:ch5_eq169"><span class="math display">\[
F(a + t \mid a) = \frac{F(a, a + t)}{F(a)},
\qquad(169)\]</span></span> where <span class="math inline">\(F(a, a + t)\)</span> is the joint probability of surviving <span class="math inline">\(a\)</span> minutes and <span class="math inline">\(a + t\)</span> minutes. But the probability of surviving <span class="math inline">\(a + t\)</span> minutes or more implies that the cell already survived <span class="math inline">\(a\)</span> minutes, therefore the information is redundant and we have <span id="eq:ch5_eq170"><span class="math display">\[
F(a, a + t) = F(a + t).
\qquad(170)\]</span></span> This explains XXX. From this equation we can find that out of the <span class="math inline">\(N P(a)da\)</span> cells with age <span class="math inline">\(a\)</span> only a fraction <span id="eq:ch5_eq171"><span class="math display">\[
\left[ NP(a)da \right] F(a + t \mid a) = NP(a) \frac{F(a + t)}{F(a)} da
\qquad(171)\]</span></span> will survive without dividing until time <span class="math inline">\(a + t\)</span>. During that time interval <span class="math inline">\(t\)</span> the culture has passed from <span class="math inline">\(N\)</span> cells to <span class="math inline">\(N e^{\mu t}\)</span> cells given the assumption that they are growing exponentially. The survivors <span class="math inline">\(NP(a)F(a + t \mid a)da\)</span> then represent a fraction of the total number of cells <span id="eq:ch5_eq172"><span class="math display">\[
\frac{\text{\# survivors}}{\text{\# total cells}} =
\frac{\left[ NP(a)da \right] F(a + t \mid a)}{Ne^{\mu t}} =
  P(a)\frac{F(a + t)}{F(a)}da \frac{1}{e^{\mu t}},
\qquad(172)\]</span></span> and their ages lie in the range <span class="math inline">\([a+t, a+t+da]\)</span>. Since we assume that the culture is in steady state then it follows that the fraction of cells that transitioned from age <span class="math inline">\(a\)</span> to age <span class="math inline">\(a + t\)</span> must be <span class="math inline">\(P(a + t)da\)</span>. Therefore we have a difference equation - the discrete analogous of a differential equation - of the form <span id="eq:ch5_eq173"><span class="math display">\[
P(a + t) da = P(a) \frac{F(a + t)}{F(a)}e^{-\mu t} da.
\qquad(173)\]</span></span> What this equation shows is a relationship that connects the probability of having a life time of <span class="math inline">\(a + t\)</span> with a probability of having a shorter life time <span class="math inline">\(a\)</span> and the growth of the population. If we take <span class="math inline">\(t\)</span> to be very small, specifically if we assume <span class="math inline">\(t \ll \mu^{-1}\)</span> we can Taylor expand around <span class="math inline">\(a\)</span> the following terms: <span id="eq:ch5_eq174"><span class="math display">\[
F(a + t) \approx F(a) + \frac{dF}{da} t,
\qquad(174)\]</span></span> <span id="eq:ch5_eq175"><span class="math display">\[
P(a + t) \approx P(a) + \frac{dP}{da} t,
\qquad(175)\]</span></span> and <span id="eq:ch5_eq176"><span class="math display">\[
e^{-\mu t} \approx 1 - \mu t.
\qquad(176)\]</span></span> Substituting these equations into gives <span id="eq:ch5_eq177"><span class="math display">\[
P(a) + \frac{dP}{da} t = P(a) \left( \frac{F(a) + \frac{dF}{da}t}{
  F(a)} \right) (1 - \mu t).
\qquad(177)\]</span></span> This can be rewritten as <span id="eq:ch5_eq178"><span class="math display">\[
\frac{1}{P(a)} \frac{dP}{da} =
\frac{1}{F(a)} \frac{dF}{da} - \mu - \frac{\mu t}{F(a)} \frac{dF}{da}.
\qquad(178)\]</span></span> Since we assumed <span class="math inline">\(t \ll \mu^{-1}\)</span> we then approximate the last term to be close to zero. We can then simplify this result into <span id="eq:ch5_eq179"><span class="math display">\[
\frac{1}{P(a)} \frac{dP}{da} = \frac{1}{F(a)} \frac{dF}{da} - \mu.
\qquad(179)\]</span></span> Integrating both sides of the equation with respect to <span class="math inline">\(a\)</span> gives <span id="eq:ch5_eq180"><span class="math display">\[
\ln P(a) = \ln F(a) - \mu a + C,
\qquad(180)\]</span></span> where <span class="math inline">\(C\)</span> is the integration constant. Exponentiating both sides gives <span id="eq:ch5_eq181"><span class="math display">\[
P(a) = C&#39; F(a)e^{-\mu a}.
\qquad(181)\]</span></span> Where <span class="math inline">\(C&#39; \equiv e^C\)</span>. To obtain the value of the unknown constant we recall that <span class="math inline">\(F(0) = 1\)</span> since the probability of having a life equal or longer than zero must add up to one, therefore we have that <span class="math inline">\(P(0) = C&#39;\)</span>. This gives then <span id="eq:ch5_eq182"><span class="math display">\[
P(a) = P(0) e^{-\mu a} F(a).
\qquad(182)\]</span></span> Substituting the definition of <span class="math inline">\(F(a)\)</span> gives <span id="eq:ch5_eq183"><span class="math display">\[
P(a) = P(0) e^{-\mu a} \int_a^\infty f(\xi) d\xi.
\qquad(183)\]</span></span> The last step of the derivation involves writing <span class="math inline">\(P(0)\)</span> and the growth rate <span class="math inline">\(\mu\)</span> in terms of the cell cycle length distribution <span class="math inline">\(f(\tau)\)</span>.</p>
<p>The growth rate of the population cell number (not the growth of cell mass) is defined as the number of cell doublings per unit time divided by the number of cells. This is more clear to see if we write as a finite difference <span id="eq:ch5_eq184"><span class="math display">\[
\frac{N(t + \Delta t) - N(t)}{\Delta t} = \mu N(t).
\qquad(184)\]</span></span> If the time <span class="math inline">\(\Delta t\)</span> is the time interval it takes to go from <span class="math inline">\(N\)</span> to <span class="math inline">\(2N\)</span> cells we have <span id="eq:ch5_eq185"><span class="math display">\[
\frac{2N - N}{\Delta t} = \mu N.
\qquad(185)\]</span></span> Solving for <span class="math inline">\(\mu\)</span> gives <span id="eq:ch5_eq186"><span class="math display">\[
\mu = \overbrace{\frac{2N - N}{\Delta t}}
^{\text{\# doubling events per unit time}}
\overbrace{\frac{1}{N}}^{\frac{1}{\text{population size}}}.
\qquad(186)\]</span></span> We defined <span class="math inline">\(F(a)\)</span> to be the probability of a cell reaching an age <span class="math inline">\(a\)</span> or greater. For a cell to reach an age <span class="math inline">\(a + da\)</span> we can then write <span id="eq:ch5_eq187"><span class="math display">\[
F(a + da) = \int_{a + da}^{\infty} f(\xi) d\xi
= \int_a^{\infty} f(\xi) d\xi - \int_a^{a + da} f(\xi) d\xi.
\qquad(187)\]</span></span> We can approximate the second term on the right hand side to be <span id="eq:ch5_eq188"><span class="math display">\[
\int_a^{a + da} f(\xi) d\xi \approx f(a) da,
\qquad(188)\]</span></span> for <span class="math inline">\(da \ll a\)</span>, obtaining <span id="eq:ch5_eq189"><span class="math display">\[
F(a + da) \approx F(a) - f(a)da.
\qquad(189)\]</span></span> What this means is that from the original fraction of cells <span class="math inline">\(F(a)\)</span> with age <span class="math inline">\(a\)</span> or greater a fraction <span class="math inline">\(f(a)da / F(a)\)</span> will not reach age <span class="math inline">\((a + da)\)</span> because they will divide. So out of the <span class="math inline">\(NP(a)\)</span> cells that reached exactly age <span class="math inline">\(a\)</span>, the number of doubling events on a time interval <span class="math inline">\(da\)</span> is given by <span id="eq:ch5_eq190"><span class="math display">\[
{\text{\# doublings of cells of age } a {\text{ on interval } da}} =
  \overbrace{NP(a)}^{\text{\# cells of age }a}
  \overbrace{\frac{f(a) da}{F(a)}}^{\text{fraction of doublings per unit time}}.
\qquad(190)\]</span></span> The growth rate then is just the sum (integral) of each age contribution to the total number of doublings. This is <span id="eq:ch5_eq191"><span class="math display">\[
\mu = \frac{1}{N} \int_0^\infty NP(a) \frac{f(a)da}{F(a)}.
\qquad(191)\]</span></span> Substituting gives <span id="eq:ch5_eq192"><span class="math display">\[
\mu = \int_0^\infty [P(0) e^{-\mu a} F(a)] \frac{f(a)da}{F(a)}
  = \int_0^\infty P(0) e^{-\mu a} f(a)da.
\qquad(192)\]</span></span> We now have the growth rate <span class="math inline">\(\mu\)</span> written in terms of the cell cycle length probability distribution <span class="math inline">\(f(a)\)</span> and the probability <span class="math inline">\(P(0)\)</span>. Since <span class="math inline">\(P(a)\)</span> is a probability distribution it must be normalized, i.e.  <span id="eq:ch5_eq193"><span class="math display">\[
\int_0^\infty P(a) da = 1.
\qquad(193)\]</span></span> Substituting into this normalization constraint gives <span id="eq:ch5_eq194"><span class="math display">\[
\int_0^\infty P(0) e^{-\mu a} F(a) da = 1.
\qquad(194)\]</span></span> From here we can integrate the left hand side by parts. We note that given the definition of <span class="math inline">\(F(a)\)</span>, the derivative with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(-f(a)\)</span> rather than <span class="math inline">\(f(a)\)</span>. This is because if we write the derivative of <span class="math inline">\(F(a)\)</span> we have <span id="eq:ch5_eq195"><span class="math display">\[
\frac{dF(a)}{da} \equiv \lim_{da \rightarrow 0}
  \frac{F(a + da) - F(a)}{da}.
\qquad(195)\]</span></span> Substituting the definition of <span class="math inline">\(F(a)\)</span> gives <span id="eq:ch5_eq196"><span class="math display">\[
\frac{dF(a)}{da} = \lim_{da \rightarrow 0} \frac{1}{da}
\left[\int_{a + da}^\infty f(\xi) d\xi - \int_a^\infty f(\xi) d\xi \right].
\qquad(196)\]</span></span> This difference in the integrals can be simplified to <span id="eq:ch5_eq197"><span class="math display">\[
\lim_{da \rightarrow 0} \frac{1}{da} \left[ \int_{a + da}^\infty f(\xi) d\xi -
  \int_a^\infty f(\xi) d\xi \right]\approx \frac{-f(a)da}{da} = -f(a).
\qquad(197)\]</span></span> Taking this into account we now perform the integration by parts obtaining <span id="eq:ch5_eq198"><span class="math display">\[
P(0) \left[ \frac{e^{-\mu t}}{-\mu} F(a) \right]^\infty_0
 - P(0) \int_0^\infty \frac{e^{-\mu a}}{-\mu} (-f(a)) da = 1.
\qquad(198)\]</span></span> On the first term on the left hand side we have that as <span class="math inline">\(a \rightarrow \infty\)</span>, both terms <span class="math inline">\(e^{-\mu a}\)</span> and <span class="math inline">\(F(a)\)</span> go to zero. We also have that <span class="math inline">\(e^{\mu 0} = 1\)</span> and <span class="math inline">\(F(0) = 1\)</span>. This results in <span id="eq:ch5_eq199"><span class="math display">\[
\frac{P(0)}{\mu} - P(0) \int_0^\infty \frac{e^{-\mu a}}{\mu} f(a) da = 1.
\qquad(199)\]</span></span> The second term on the left hand side is equal to since <span id="eq:ch5_eq200"><span class="math display">\[
\mu = \int_0^\infty P(0) e^{-\mu a} f(a)da \Rightarrow
  1 = \int_0^\infty P(0) \frac{e^{-\mu a}}{\mu} f(a)da.
\qquad(200)\]</span></span> This implies that on we have <span id="eq:ch5_eq201"><span class="math display">\[
\frac{P(0)}{\mu} - 1 = 1 \Rightarrow P(0) = 2 \mu.
\qquad(201)\]</span></span> With this result in hand we can rewrite as <span id="eq:ch5_eq202"><span class="math display">\[
P(a) = 2\mu e^{-\mu a} \int_a^\infty f(\xi) d\xi.
\qquad(202)\]</span></span> Also we can rewrite the result for the growth rate <span class="math inline">\(\mu\)</span> on as <span id="eq:ch5_eq203"><span class="math display">\[
\mu = 2 \mu \int_0^\infty e^{-\mu a} f(a) da \Rightarrow
  2 \int_0^\infty e^{-\mu a} f(a) da = 1.
\qquad(203)\]</span></span></p>
<p>As mentioned before the distribution <span class="math inline">\(f(a)\)</span> has been empirically fit to a generalize Gamma distribution. But if we assume that our distribution has almost negligible dispersion around the mean average doubling time <span class="math inline">\(a = \tau_d\)</span>, we can approximate <span class="math inline">\(f(a)\)</span> as <span id="eq:ch5_eq204"><span class="math display">\[
f(a) = \delta(a - \tau_d),
\qquad(204)\]</span></span> a Dirac delta function. Applying this to results in <span id="eq:ch5_eq205"><span class="math display">\[
2 \int_0^\infty e^{-\mu a} \delta(a - \tau_a) da = 1
  \Rightarrow 2 e^{-\mu \tau_d} = 1.
\qquad(205)\]</span></span> Solving for <span class="math inline">\(\mu\)</span> gives <span id="eq:ch5_eq206"><span class="math display">\[
\mu = \frac{\ln 2}{\tau_d}.
\qquad(206)\]</span></span> This delta function approximation for <span class="math inline">\(f(a)\)</span> has as a consequence that <span id="eq:ch5_eq207"><span class="math display">\[
F(a) =
  \begin{cases}
    1 \text{ for } a \in [0, \tau_d],\\
    0 \text{ for } a &gt; \tau_d.
  \end{cases}
\qquad(207)\]</span></span> Fianlly we can rewrite as <span id="eq:ch5_eq208"><span class="math display">\[
P(a) = 2 \left( \frac{\ln 2}{\tau_d} \right)
e^{- \frac{\ln 2}{\tau_d} a} \int_a^\infty \delta(\xi - \tau_d) d\xi
\Rightarrow = 2 \ln 2 \cdot 2^\frac{-a}{\tau_d}.
\qquad(208)\]</span></span> Simplifying this we obtain <span id="eq:ch5_eq209"><span class="math display">\[
P(a) =
  \begin{cases}
    \ln 2 \cdot 2^{1 - \frac{a}{\tau_d}} \text{ for } a \in [0, \tau_d],\\
    0 \text{ otherwise}.
  \end{cases}
\qquad(209)\]</span></span> This is the equation we aimed to derive. The distribution of cell ages over the cell cycle.</p>
<h2 class="unnumbered" id="references">References</h2>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-Yu2006" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">J. Yu, <em><span class="nocase">Probing Gene Expression in Live Cells, One Protein Molecule at a Time</span></em>, Science <strong>311</strong>, 1600 (2006).</div>
</div>
<div id="ref-Jaynes1957" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">E. T. Jaynes, <em><span class="nocase">Information Theory and Statistical Mechanics</span></em>, Physical Review <strong>106</strong>, 620 (1957).</div>
</div>
<div id="ref-Cheong2011a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">R. Cheong, A. Rhee, C. J. Wang, I. Nemenman, and A. Levchenko, <em><span class="nocase">Information Transduction Capacity of Noisy Biochemical Signaling Networks</span></em>, Science <strong>334</strong>, 354 (2011).</div>
</div>
<div id="ref-Frohlich2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">F. Fröhlich, P. Thomas, A. Kazeroonian, F. J. Theis, R. Grima, and J. Hasenauer, <em><span class="nocase">Inference for Stochastic Chemical Kinetics Using Moment Equations and System Size Expansion</span></em>, PLoS Computational Biology <strong>12</strong>, e1005030 (2016).</div>
</div>
<div id="ref-Schnoerr2017" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">D. Schnoerr, G. Sanguinetti, and R. Grima, <em><span class="nocase">Approximation and inference methods for stochastic biochemical kinetics—a tutorial review</span></em>, Journal of Physics A: Mathematical and Theoretical <strong>50</strong>, 093001 (2017).</div>
</div>
<div id="ref-Peterson2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">J. R. Peterson, J. A. Cole, J. Fei, T. Ha, and Z. A. Luthey-Schulten, <em><span class="nocase">Effects of DNA replication on mRNA noise</span></em>, PNAS <strong>112</strong>, 15886 (2015).</div>
</div>
<div id="ref-MacKay2003" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">D. J. MacKay, <em><span class="nocase">Information theory, inference and learning algorithms</span></em> (Cambridge university press, 2003).</div>
</div>
<div id="ref-Barnes2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">S. L. Barnes, N. M. Belliveau, W. T. Ireland, J. B. Kinney, and R. Phillips, <em><span class="nocase">Mapping DNA sequence to transcription factor binding energy in vivo</span></em>, PLoS Computational Biology <strong>15</strong>, e1006226 (2019).</div>
</div>
<div id="ref-Blahut1972" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">R. Blahut, <em><span class="nocase">Computation of channel capacity and rate-distortion functions</span></em>, IEEE Transactions on Information Theory <strong>18</strong>, 460 (1972).</div>
</div>
<div id="ref-Peccoud1995" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">J. Peccoud and B. Ycart, <em><span class="nocase">Markovian Modeling of Gene-Product Synthesis</span></em>, Theoretical Population Biology <strong>48</strong>, 222 (1995).</div>
</div>
<div id="ref-Shannon1948" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">C. E. Shannon, <em><span class="nocase">A Mathematical Theory of Communication</span></em>, Bell System Technical Journal <strong>27</strong>, 379 (1948).</div>
</div>
<div id="ref-Tkacik2008a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">G. Tkačik, C. G. Callan, and W. Bialek, <em><span class="nocase">Information capacity of genetic regulatory elements</span></em>, Physical Review E <strong>78</strong>, 011910 (2008).</div>
</div>
<div id="ref-Ale2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">A. Ale, P. Kirk, and M. P. H. Stumpf, <em><span class="nocase">A general moment expansion method for stochastic kinetic models</span></em>, The Journal of Chemical Physics <strong>138</strong>, 174101 (2013).</div>
</div>
<div id="ref-Hammar2014" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">P. Hammar, M. Walldén, D. Fange, F. Persson, Ö. Baltekin, G. Ullman, P. Leroy, and J. Elf, <em><span class="nocase">Direct measurement of transcription factor dissociation excludes a simple operator occupancy model for gene regulation</span></em>, Nature Genetics <strong>46</strong>, 405 (2014).</div>
</div>
<div id="ref-Andreychenko2017" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">A. Andreychenko, L. Bortolussi, R. Grima, P. Thomas, and V. Wolf, <em><span>Modeling Cellular Systems</span></em>, Vol. 11 (Springer International Publishing, Cham, 2017).</div>
</div>
<div id="ref-Powell1956" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">E. O. Powell, <em><span class="nocase">Growth Rate and Generation Time of Bacteria, with Special Reference to Continuous Culture</span></em>, Journal of General Microbiology <strong>15</strong>, 492 (1956).</div>
</div>
<div id="ref-Bremer1996" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">H. Bremer and P. P. Dennis, <em><span class="nocase">Modulation of Chemical Composition and Other Parameters of the Cell by Growth Rate</span></em>, (n.d.).</div>
</div>
<div id="ref-Razo-Mejia2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">M. Razo-Mejia, S. L. Barnes, N. M. Belliveau, G. Chure, T. Einav, M. Lewis, and R. Phillips, <em><span class="nocase">Tuning Transcriptional Regulation through Signaling: A Predictive Theory of Allosteric Induction</span></em>, Cell Systems <strong>6</strong>, 456 (2018).</div>
</div>
<div id="ref-Smadbeck2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">P. Smadbeck and Y. N. Kaznessis, <em><span class="nocase">A closure scheme for chemical master equations</span></em>, PNAS <strong>110</strong>, 14261 (2013).</div>
</div>
<div id="ref-Sanchez2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">A. Sanchez, S. Choubey, and J. Kondev, <em><span class="nocase">Stochastic models of transcription: From single molecules to single cells</span></em>, Methods <strong>62</strong>, 13 (2013).</div>
</div>
<div id="ref-Vilar2011" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">J. M. G. Vilar and L. Saiz, <em><span class="nocase">Control of gene expression by modulated self-assembly</span></em>, Nucleic Acids Research <strong>39</strong>, 6854 (2011).</div>
</div>
<div id="ref-Yu2006" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">J. Yu, <em><span class="nocase">Probing Gene Expression in Live Cells, One Protein Molecule at a Time</span></em>, Science <strong>311</strong>, 1600 (2006).</div>
</div>
<div id="ref-Browning2004" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">D. F. Browning and S. J. W. Busby, <em><span class="nocase">The regulation of bacterial transcription initiation</span></em>, Nature Reviews Microbiology <strong>2</strong>, 57 (2004).</div>
</div>
<div id="ref-Moran2010" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">U. Moran, R. Phillips, and R. Milo, <em><span class="nocase">SnapShot: Key Numbers in Biology</span></em>, Cell <strong>141</strong>, 1262 (2010).</div>
</div>
<div id="ref-Radzikowski2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">J. L. Radzikowski, S. Vedelaar, D. Siegel, Á. D. Ortega, A. Schmidt, and M. Heinemann, <em><span class="nocase">Bacterial persistence is an active <span class="math inline">\(\sigma\)</span> S stress response to metabolic flux limitation</span></em>, Molecular Systems Biology <strong>12</strong>, 882 (2016).</div>
</div>
<div id="ref-Phillips2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">R. Phillips, N. M. Belliveau, G. Chure, H. G. Garcia, M. Razo-Mejia, and C. Scholes, <em><span class="nocase">Figure 1 Theory Meets Figure 2 Experiments in the Study of Gene Expression</span></em>, Annual Review of Biophysics <strong>48</strong>, 121 (2019).</div>
</div>
<div id="ref-sympy" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">A. Meurer, C. P. Smith, M. Paprocki, O. Čertík, S. B. Kirpichev, M. Rocklin, A. Kumar, S. Ivanov, J. K. Moore, S. Singh, T. Rathnayake, S. Vig, B. E. Granger, R. P. Muller, F. Bonazzi, H. Gupta, S. Vats, F. Johansson, F. Pedregosa, M. J. Curry, A. R. Terrel, Š. Roučka, A. Saboo, I. Fernando, S. Kulal, R. Cimrman, and A. Scopatz, <em>SymPy: Symbolic Computing in Python</em>, PeerJ Computer Science <strong>3</strong>, e103 (2017).</div>
</div>
<div id="ref-Buchler2003" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">N. E. Buchler, U. Gerland, and T. Hwa, <em><span class="nocase">On schemes of combinatorial transcription logic.</span></em>, PNAS <strong>100</strong>, 5136 (2003).</div>
</div>
<div id="ref-Jones2014a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">D. L. Jones, R. C. Brewster, and R. Phillips, <em><span class="nocase">Promoter architecture dictates cell-to-cell variability in gene expression</span></em>, Science <strong>346</strong>, 1533 (2014).</div>
</div>
<div id="ref-Bintu2005a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">L. Bintu, N. E. Buchler, H. G. Garcia, U. Gerland, T. Hwa, J. Kondev, and R. Phillips, <em><span class="nocase">Transcriptional regulation by the numbers: models</span></em>, Current Opinion in Genetics <span>&amp;</span> Development <strong>15</strong>, 116 (2005).</div>
</div>
<div id="ref-Shahrezaei2008" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">V. Shahrezaei and P. S. Swain, <em><span class="nocase">Analytical distributions for stochastic gene expression.</span></em>, PNAS <strong>105</strong>, 17256 (2008).</div>
</div>
<div id="ref-Klumpp2008" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">S. Klumpp and T. Hwa, <em><span class="nocase">Growth-rate-dependent partitioning of RNA polymerases in bacteria.</span></em>, PNAS <strong>105</strong>, 20245 (2008).</div>
</div>
<div id="ref-Phillips2015a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">R. Phillips, <em><span class="nocase">Napoleon Is in Equilibrium</span></em>, Annual Review of Condensed Matter Physics <strong>6</strong>, 85 (2015).</div>
</div>
<div id="ref-Brewster2012" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[34] </div><div class="csl-right-inline">R. C. Brewster, D. L. Jones, and R. Phillips, <em><span class="nocase">Tuning Promoter Strength through RNA Polymerase Binding Site Design in <em>Escherichia coli</em></span></em>, PLoS Computational Biology <strong>8</strong>, e1002811 (2012).</div>
</div>
<div id="ref-Garcia2011c" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[35] </div><div class="csl-right-inline">H. G. Garcia and R. Phillips, <em><span class="nocase">Quantitative dissection of the simple repression input-output function.</span></em>, Proceedings of the National Academy of Sciences <strong>108</strong>, 12173 (2011a).</div>
</div>
<div id="ref-Brewster2014" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[36] </div><div class="csl-right-inline">R. C. Brewster, F. M. Weinert, H. G. Garcia, D. Song, M. Rydenfelt, and R. Phillips, <em><span class="nocase">The Transcription Factor Titration Effect Dictates Level of Gene Expression</span></em>, Cell <strong>156</strong>, 1312 (2014).</div>
</div>
<div id="ref-Transtrum2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[37] </div><div class="csl-right-inline">M. K. Transtrum, B. B. Machta, K. S. Brown, B. C. Daniels, C. R. Myers, and J. P. Sethna, <em><span class="nocase">Perspective: Sloppiness and emergent theories in physics, biology, and beyond.</span></em>, The Journal of Chemical Physics <strong>143</strong>, 010901 (2015).</div>
</div>
</div>
                    </div><!-- /.content -->
                </div><!-- /.col -->
                <div class="col-md-4 col-md-offset-1">
                    <div class="sections-list-wrapper">
                        <div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"><span
                                style="font-size: 14pt">Table of Contents</span>
                            <hr>
                        </div><!-- /.sections-list -->
                    </div>
                </div><!-- /.col -->
            </div><!-- /.row -->
        </div><!-- /.container -->
    </div><!-- /.section -->
    
    <div class="section section--grey">
        <div class="container">
            <div class="row">
                <div class="col-md-7">
                    <div id="disqus_thread"></div>
<script>
	/**
	 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
	 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
	 */
	 /*
	var disqus_config = function () {
	this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
		this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
	};
	*/
	(function() {
		var d = document, s = d.createElement('script');

		s.src = 'https://mrazomej-phd.disqus.com/embed.js';

		s.setAttribute('data-timestamp', +new Date());
		(d.head || d.body).appendChild(s);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

                </div><!-- /.col -->
            </div><!-- /.row -->
        </div><!-- /.container -->
    </div><!-- /.section -->
    
    <div class="js-footer-area">
        
        <nav class="page-nav">
            <div class="container">
                <div class="row">
                    <div class="col-xs-12">
                        
                        <a href="https://mrazomej.github.io/phd/chapter_04"
                            class="page-nav__item page-nav__item--prev">
                            <i class="icon icon--arrow-left"></i>
                            Chapter 4
                        </a><!-- /.page-nav__item -->
                        
                        
                        <a href="https://mrazomej.github.io/phd/chapter_06"
                            class="page-nav__item page-nav__item--next">
                            Chapter 6
                            <i class="icon icon--arrow-right"></i>
                        </a><!-- /.page-nav__item -->
                        
                    </div><!-- /.col -->
                </div><!-- /.row -->
            </div><!-- /.container -->
        </nav><!-- /.page-nav -->
        
        
        
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
						<a href="https://mrazomej.github.io/phd//" class="site-footer__logo">Manuel Razo-Mejia | PhD Thesis</a>
					
					
						<hr>
						<p class="site-footer__copyright">Copyright &copy; 2021. - Manuel Razo-Mejia <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
					<div class="col-sm-6 align-right">
						<ul class="social-list">
							
								<li>
									<a href="https://twitter.com/mrazomej" target="_blank" class="social-list__item social-list__item--twitter">
										<i class="icon icon--twitter"></i>
									</a>
								</li>
							
						</ul><!-- /.social-list -->
					</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="https://mrazomej.github.io/phd//doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="https://mrazomej.github.io/phd//doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="https://mrazomej.github.io/phd//doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="https://mrazomej.github.io/phd//doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="https://mrazomej.github.io/phd//doks-theme/assets/js/scripts.min.js"></script>


<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-109526846-4', 'auto');
	ga('send', 'pageview');
</script>



    </div><!-- /.js-footer-area -->
</body>

</html>