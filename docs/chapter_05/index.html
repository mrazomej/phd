<!DOCTYPE html>

<html>

<head>
    <meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<title>Chapter V</title>


<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "Gyre Pagella",
        webFont : "STIX-Web",
        imageFont : null
    },
	  TeX: {
		equationNumbers: {
		  autoNumber: "AMS"
		}
	  },
	  tex2jax: {
		inlineMath: [['$','$']], 
		displayMath: [['$$','$$']],
		processEscapes: true,
	  }
	});
  </script>
<style>
    .eqnos {
        display: inline-block;
        position: relative;
        width: 100%;
    }

    .eqnos br {
        display: none;
    }

    .eqnos-number {
        position: absolute;
        right: 0em;
        top: 50%;
        line-height: 0;
    }
</style>

<script>
    window.MathJax = {
        tex: {
            tags: 'all'
        }
    };
</script>
<script>
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
    });
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<link rel="icon" href="https://mrazomej.github.io/phd//favicon.ico"
    type="image/x-icon">
<link href="https://fonts.googleapis.com/css?family=Nanum+Myeongjo&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Alegreya|Lora&display=swap" rel="stylesheet">

<link rel="stylesheet"
    href="https://mrazomej.github.io/phd//doks-theme/assets/css/style.css">
</head>

<body class="grey" data-spy="scroll" data-target=".js-scrollspy">
    


	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="https://mrazomej.github.io/phd//" class="site-header__logo"><i class="icon icon--chevron-left"></i> Home</a>
					
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


    <div class="hero-subheader" style="background-image: url(https://mrazomej.github.io/phd//doks-theme/assets/images/leaf.jpg); background-position: center; object-fit: cover; width:100%;">

        <div class="container" style="background-color: rgba(0,0,0, 0.75); border:0.75px solid white;">
            <div class="row">
                <div class="col-md-7">
                    <div class="align-container" data-mh>
                        <div class="align-inner">
                            
                            <h2 class="hero-subheader__title">Chapter V</h2>
                            
                            
                            <h1 class="hero-subheader__title">Supporting Information for First-principles prediction of the information processing capacity of a simple genetic circuit
</h1>
                            
                            
                            <!-- 
									
										<a href="https://mrazomej.github.io/phd/chapter_04" class="btn btn--dark btn--rounded btn--w-icon btn--w-icon-left">
											<i class="icon icon--arrow-left"></i>	
										</a>
									
									
										<a href="https://mrazomej.github.io/phd/chapter_06" class="btn btn--dark btn--rounded btn--w-icon">
											<i class="icon icon--arrow-right"></i>
										</a>
									
								 -->
                        </div><!-- /.align-inner -->
                    </div><!-- /.align-container -->
                </div><!-- /.col -->
                
                <div class="col-md-5 col-md-offset-0 hidden-xs hidden-sm">
                    <div class="align-container" data-mh>
                        <div class="align-inner">
                            <div class="hero-subheader__author">
                                <p class="hero-subheader__author-title">
                                    Summary
                                    <i class="icon icon--chevron-down"></i>
                                </p><!-- /.hero-subheader__author-title -->
                                <p>Living organisms are constantly sensing intra and extracellular cues and responding accordingly. The quality and precision of such responses can mean the difference between surviving or not certain challenges; therefore, there is a constant selection pressure for cells to gather enough information from any stimulus in order to build an adequate response. In this context, the information that cells can obtain has a precise mathematical definition measured-—just as in computers—-in bits.
In this chapter our goal is to predict how many bits of information can a cell harboring a simple genetic circuit process. To do so, I write down a theoretical model to predict the full distribution of gene expression based on the physics of this molecular process. I calibrate our model with previous information in order to perform parameter-free predictions. To test the model, I compare the predictions with experimental single-cell gene expression data finding great agreement.
</p>
                            </div><!-- /.hero-subheader__author -->
                        </div><!-- /.align-inner -->
                    </div><!-- /.align-container -->
                </div><!-- /.col -->
                
            </div><!-- /.row -->
        </div><!-- /.container -->
    </div><!-- /.hero-subheader -->
    
    <nav class="page-nav">
        <div class="container">
            <div class="row">
                <div class="col-xs-12">
                    
                    <a href="https://mrazomej.github.io/phd/chapter_04"
                        class="page-nav__item page-nav__item--prev">
                        <i class="icon icon--arrow-left"></i>
                        Chapter 4
                    </a><!-- /.page-nav__item -->
                    
                    
                    <a href="https://mrazomej.github.io/phd/chapter_06"
                        class="page-nav__item page-nav__item--next">
                        Chapter 6
                        <i class="icon icon--arrow-right"></i>
                    </a><!-- /.page-nav__item -->
                    
                </div><!-- /.col -->
            </div><!-- /.row -->
        </div><!-- /.container -->
    </nav><!-- /.page-nav -->
    

    <div class="section">
        <div class="container">
            <div class="row">
                <div class="col-md-7">
                    <div class="content">
                        <hr/>
<p>A version of this chapter originally appeared as Razo-Mejia, M., Marzen, S., Chure, G., Taubman, R., Morrison, M., and Phillips, R. (2020). First-principles prediction of the information processing capacity of a simple genetic circuit. Physical Review E 102, 022404. DOI:https://doi:10.1103/PhysRevE.102.022404.</p>
<h2 id="abstract">Abstract</h2>
<p>Given the stochastic nature of gene expression, genetically identical cells exposed to the same environmental inputs will produce different outputs. This heterogeneity has been hypothesized to have consequences for how cells are able to survive in changing environments. Recent work has explored the use of information theory as a framework to understand the accuracy with which cells can ascertain the state of their surroundings. Yet the predictive power of these approaches is limited and has not been rigorously tested using precision measurements. To that end, we generate a minimal model for a simple genetic circuit in which all parameter values for the model come from independently published data sets. We then predict the information processing capacity of the genetic circuit for a suite of biophysical parameters such as protein copy number and protein-DNA affinity. We compare these parameter-free predictions with an experimental determination of protein expression distributions and the resulting information processing capacity of <em>E. coli</em> cells. We find that our minimal model captures the scaling of the cell-to-cell variability in the data and the inferred information processing capacity of our simple genetic circuit up to a systematic deviation.</p>
<h2 id="supp_model">Three-state promoter model for simple repression</h2>
<p>To tackle the question of how much information the simple repression motif can process, we require the joint probability distribution of mRNA and protein <span class="math inline">\(P(m, p; t)\)</span>. To obtain this distribution, we use the chemical master equation formalism as described in XXX. Specifically, we assume a three-state model, where the promoter can be found 1) in a transcriptionally active state (<span class="math inline">\(A\)</span> state), 2) in a transcriptionally inactive state without the repressor bound (<span class="math inline">\(I\)</span> state) and 3) with the repressor bound (<span class="math inline">\(R\)</span> state). (See Fig. <strong>¿fig:ch3_fig02?</strong>(A)). These three states generate a system of coupled differential equations for each of the three state distributions <span class="math inline">\(P_A(m, p)\)</span>, <span class="math inline">\(P_I(m, p)\)</span> and <span class="math inline">\(P_R(m, p)\)</span>. Given the rates shown in Fig. <strong>¿fig:ch3_fig02?</strong>(A), let us define the system of ODEs. For the transcriptionally active state, we have <span class="math display">\[
\begin{split}
    \frac{d P_A(m, p)}{dt} &amp;=
    - \overbrace{k^{(p)}_{\text{off}} P_A(m, p)}^{A \rightarrow I} % A -&gt; I
    + \overbrace{k^{(p)}_{\text{on}} P_I(m, p)}^{I \rightarrow A}\\ % I -&gt; A
    &amp;+ \overbrace{r_m P_A(m-1, p)}^{m-1 \rightarrow m} % m-1 -&gt; m
    - \overbrace{r_m P_A(m, p)}^{m \rightarrow m+1}% m -&gt; m+1
    + \overbrace{\gamma _m (m + 1) P_A(m+1 , p)}^{m+1 \rightarrow m} % m+1 -&gt; m
    - \overbrace{\gamma _m m P_A(m , p)}^{m \rightarrow m-1}\\ % m -&gt; m-1
    &amp;+ \overbrace{r_p m P_A(m, p - 1)}^{p-1 \rightarrow p} % p-1 -&gt; p
    - \overbrace{r_p m P_A(m, p)}^{p \rightarrow p+1} % p -&gt; p+1
    + \overbrace{\gamma _p (p + 1) P_A(m, p + 1)}^{p + 1 \rightarrow p} % p+1 -&gt; p
    - \overbrace{\gamma _p p P_A(m, p)}^{p \rightarrow p-1}. % p -&gt; p-1
\end{split}
\]</span> For the inactive promoter state <span class="math inline">\(I\)</span>, we have <span class="math display">\[
\begin{split}
    \frac{d P_I(m, p)}{dt} &amp;=
    \overbrace{k^{(p)}_{\text{off}} P_A(m, p)}^{A \rightarrow I} % A -&gt; I
    - \overbrace{k^{(p)}_{\text{on}} P_I(m, p)}^{I \rightarrow A} % I -&gt; A
    + \overbrace{k^{(r)}_{\text{off}} P_R(m, p)}^{R \rightarrow I} % R -&gt; I
    - \overbrace{k^{(r)}_{\text{on}} P_I(m, p)}^{I \rightarrow R}\\ % I -&gt; R
    &amp;+ \overbrace{\gamma _m (m + 1) P_I(m+1 , p)}^{m+1 \rightarrow m} % m+1 -&gt; m
    - \overbrace{\gamma _m m P_I(m , p)}^{m \rightarrow m-1}\\ % m -&gt; m-1
    &amp;+ \overbrace{r_p m P_I(m, p - 1)}^{p-1 \rightarrow p} % p-1 -&gt; p
    - \overbrace{r_p m P_I(m, p)}^{p \rightarrow p+1} % p -&gt; p+1
    + \overbrace{\gamma _p (p + 1) P_I(m, p + 1)}^{p + 1 \rightarrow p} % p+1 -&gt; p
    - \overbrace{\gamma _p p P_I(m, p)}^{p \rightarrow p-1}. % p -&gt; p-1
  \end{split}
\]</span> And finally, for the repressor bound state <span class="math inline">\(R\)</span>, we have <span class="math display">\[
\begin{split}
    \frac{d P_R(m, p)}{dt} &amp;=
    - \overbrace{k^{(r)}_{\text{off}} P_R(m, p)}^{R \rightarrow I} % R -&gt; I
    + \overbrace{k^{(r)}_{\text{on}} P_I(m, p)}^{I \rightarrow R}\\ % I -&gt; R
    &amp;+ \overbrace{\gamma _m (m + 1) P_R(m+1 , p)}^{m+1 \rightarrow m} % m+1 -&gt; m
    - \overbrace{\gamma _m m P_R(m , p)}^{m \rightarrow m-1}\\ % m -&gt; m-1
    &amp;+ \overbrace{r_p m P_R(m, p - 1)}^{p-1 \rightarrow p} % p-1 -&gt; p
    - \overbrace{r_p m P_R(m, p)}^{p \rightarrow p+1} % p -&gt; p+1
    + \overbrace{\gamma _p (p + 1) P_R(m, p + 1)}^{p + 1 \rightarrow p} % p+1 -&gt; p
    - \overbrace{\gamma _p p P_R(m, p)}^{p \rightarrow p-1}. % p -&gt; p-1
\end{split}
\]</span></p>
<p>For an unregulated promoter, i.e., a promoter in a cell that has no repressors present and therefore constitutively expresses the gene, we use a two-state model in which the state <span class="math inline">\(R\)</span> is not allowed. All the terms in the system of ODEs containing <span class="math inline">\(k^{(r)}_{\text{on}}\)</span> or <span class="math inline">\(k^{(r)}_{\text{off}}\)</span> are then set to zero.</p>
<p>As detailed in it is convenient to express this system using matrix notation <span class="citation" data-cites="Sanchez2013"> [<a href="#ref-Sanchez2013" role="doc-biblioref">1</a>]</span>. For this we define <span class="math inline">\(\mathbf{P}(m, p) = (P_A(m, p), P_I(m, p), P_R(m, p))^T\)</span>. Then the system of ODEs can be expressed as <span class="math display">\[
\begin{split}
    \frac{d \mathbf{P}(m, p)}{dt} &amp;= \mathbf{K} \mathbf{P}(m, p)
    - \mathbf{R}_m \mathbf{P}(m, p) + \mathbf{R}_m \mathbf{P}(m-1, p)
    - m \mathbf{\Gamma}_m \mathbf{P}(m, p) + (m + 1) \mathbf{\Gamma}_m \mathbf{P}(m + 1, p)\\
    &amp;- m \mathbf{R}_p \mathbf{P}(m, p) + m \mathbf{R}_p \mathbf{P}(m, p - 1)
    - p \mathbf{\Gamma}_p \mathbf{P}(m, p) + (p + 1) \mathbf{\Gamma}_p \mathbf{P}(m, p + 1),
\end{split}
\]</span> where we defined matrices representing the promoter state transition <span class="math inline">\(\mathbf{K}\)</span>, <span class="math display">\[
 \mathbf{K} \equiv
 \begin{bmatrix}
    -k^{(p)}_{\text{off}}   &amp; k^{(p)}_{\text{on}}         &amp; 0\\
    k^{(p)}_{\text{off}}    &amp; -k^{(p)}_{\text{on}} -k^{(r)}_{\text{on}}  &amp; k^{(r)}_{\text{off}}\\
    0         &amp; k^{(r)}_{\text{on}}         &amp; -k^{(r)}_{\text{off}}
 \end{bmatrix},
\]</span> mRNA production, <span class="math inline">\(\mathbf{R}_m\)</span>, and degradation, <span class="math inline">\(\mathbf{\Gamma}_m\)</span>, as <span class="math display">\[
\mathbf{R}_m \equiv
\begin{bmatrix}
    r_m   &amp; 0 &amp; 0\\
    0     &amp; 0 &amp; 0\\
    0     &amp; 0 &amp; 0\\
\end{bmatrix},
\]</span> and <span class="math display">\[
\mathbf{\Gamma}_m \equiv
  \begin{bmatrix}
    \gamma _m   &amp; 0   &amp; 0\\
    0     &amp; \gamma _m &amp; 0\\
    0     &amp; 0   &amp; \gamma _m\\
  \end{bmatrix}.
\]</span> For the protein, we also define production <span class="math inline">\(\mathbf{R}_p\)</span> and degradation <span class="math inline">\(\mathbf{\Gamma}_p\)</span> matrices as <span class="math display">\[
\mathbf{R}_p \equiv
\begin{bmatrix}
    r_p   &amp; 0   &amp; 0\\
    0     &amp; r_p &amp; 0\\
    0     &amp; 0   &amp; r_p\\
\end{bmatrix}
\]</span> and <span class="math display">\[
\mathbf{\Gamma}_p \equiv
  \begin{bmatrix}
    \gamma _p   &amp; 0   &amp; 0\\
    0     &amp; \gamma _p &amp; 0\\
    0     &amp; 0   &amp; \gamma _p\\
\end{bmatrix}.
\]</span></p>
<p>The corresponding equation for the unregulated two-state promoter takes the same form with the definition of the matrices following the same scheme without including the third row and third column, and setting <span class="math inline">\(k^{(r)}_{\text{on}}\)</span> and <span class="math inline">\(k^{(r)}_{\text{off}}\)</span> to zero.</p>
<p>A closed-form solution for this master equation might not even exist. The approximate solution of chemical master equations of this kind is an active area of research. As we will see in the two-state promoter master equation has been analytically solved for the mRNA <span class="citation" data-cites="Peccoud1995"> [<a href="#ref-Peccoud1995" role="doc-biblioref">2</a>]</span> and protein distributions <span class="citation" data-cites="Shahrezaei2008"> [<a href="#ref-Shahrezaei2008" role="doc-biblioref">3</a>]</span>. For our purposes, we will detail how to use the Maximum Entropy principle to approximate the full distribution for the two- and three-state promoter.</p>
<h2 id="parameter-inference">Parameter inference</h2>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/chemical_master_mRNA_FISH_mcmc.html">following link</a> as an annotated Jupyter notebook)</p>
<p>To generate falsifiable predictions with meaningful parameters, we infer the kinetic rates for this three-state promoter model using different data sets generated in our lab over the last decade concerning different aspects of the regulation of the simple repression motif. For example, for the unregulated promoter transition rates <span class="math inline">\(k^{(p)}_{\text{on}}\)</span> and <span class="math inline">\(k^{(p)}_{\text{off}}\)</span> and the mRNA production rate <span class="math inline">\(r_m\)</span>, we use single-molecule mRNA FISH counts from an unregulated promoter <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">4</a>]</span>. Once these parameters are fixed, we use the values to constrain the repressor rates <span class="math inline">\(k^{(r)}_{\text{on}}\)</span> and <span class="math inline">\(k^{(r)}_{\text{off}}\)</span>. These repressor rates are obtained using information from mean gene expression measurements from bulk LacZ colorimetric assays <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">5</a>]</span>. We also expand our model to include the allosteric nature of the repressor protein, taking advantage of video microscopy measurements done in the context of multiple promoter copies <span class="citation" data-cites="Brewster2014"> [<a href="#ref-Brewster2014" role="doc-biblioref">6</a>]</span> and flow-cytometry measurements of the mean response of the system to different levels of induction <span class="citation" data-cites="Razo-Mejia2018"> [<a href="#ref-Razo-Mejia2018" role="doc-biblioref">7</a>]</span>. In what follows of this section, we detail the steps taken to infer the parameter values. At each step, the values of the parameters inferred in previous steps constrain the values of the parameters that are not yet determined, building in this way a self-consistent model informed by work that spans several experimental techniques.</p>
<h3 id="unregulated-promoter-rates">Unregulated promoter rates</h3>
<p>We begin our parameter inference problem with the promoter on and off rates <span class="math inline">\(k^{(p)}_{\text{on}}\)</span> and <span class="math inline">\(k^{(p)}_{\text{off}}\)</span>, as well as the mRNA production rate <span class="math inline">\(r_m\)</span>. In this case, there are only two states available to the promoter – the inactive state <span class="math inline">\(I\)</span> and the transcriptionally active state <span class="math inline">\(A\)</span>. That means that the third ODE for <span class="math inline">\(P_R(m, p)\)</span> is removed from the system. The mRNA steady-state distribution for this particular two-state promoter model was solved analytically by Peccoud and Ycart <span class="citation" data-cites="Peccoud1995"> [<a href="#ref-Peccoud1995" role="doc-biblioref">2</a>]</span>. This distribution <span class="math inline">\(P(m) \equiv P_I(m) + P_A(m)\)</span> is of the form <span class="math display">\[
\small
  P(m \mid k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m, \gamma _m) =
  {\Gamma \left( \frac{k^{(p)}_{\text{on}}}{\gamma _m} + m \right) \over
  \Gamma(m + 1) \Gamma\left( \frac{k^{(p)}_{\text{off}}+k^{(p)}_{\text{on}}}{\gamma _m} + m \right)}
  {\Gamma\left( \frac{k^{(p)}_{\text{off}}+k^{(p)}_{\text{on}}}{\gamma _m} \right) \over
  \Gamma\left( \frac{k^{(p)}_{\text{on}}}{\gamma _m} \right)}
  \left( \frac{r_m}{\gamma _m} \right)^m
  F_1^1 \left( \frac{k^{(p)}_{\text{on}}}{\gamma _m} + m,
  \frac{k^{(p)}_{\text{off}} + k^{(p)}_{\text{on}}}{\gamma _m} + m,
  -\frac{r_m}{\gamma _m} \right),
  \label{eq:two_state_mRNA}
\]</span> where <span class="math inline">\(\Gamma(\cdot)\)</span> is the gamma function, and <span class="math inline">\(F_1^1\)</span> is the confluent hypergeometric function of the first kind. This rather complicated expression will aid us in finding parameter values for the rates. The inferred rates <span class="math inline">\(k^{(p)}_{\text{on}}\)</span>, <span class="math inline">\(k^{(p)}_{\text{off}}\)</span> and <span class="math inline">\(r_m\)</span> will be expressed in units of the mRNA degradation rate <span class="math inline">\(\gamma _m\)</span>. This is because the model in Eq. <span class="math inline">\(\ref{eq:two_state_mRNA}\)</span> is homogeneous in time, meaning that if we divide all rates by a constant, it would be equivalent to multiplying the characteristic time scale by the same constant. As we will discuss in the next section, Eq. <span class="math inline">\(\ref{eq:two_state_mRNA}\)</span> has degeneracy in the parameter values. What this means is that a change in one of the parameters, specifically <span class="math inline">\(r_m\)</span>, can be compensated by a change in another parameter, specifically <span class="math inline">\(k^{(p)}_{\text{off}}\)</span>, to obtain the same distribution. To work around this intrinsic limitation of the model, we will include information from what we know from equilibrium-based models in our inference prior.</p>
<h3 id="bayesian-parameter-inference-of-rnap-rates">Bayesian parameter inference of RNAP rates</h3>
<p>To make progress at inferring the unregulated promoter state transition rates, we make use of the single-molecule mRNA FISH data from Jones et al. <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">4</a>]</span>. Fig. 1 shows the distribution of mRNA per cell for the <em>lacUV5</em> promoter used for our inference. This promoter, being very strong, has a mean copy number of <span class="math inline">\(\langle m \rangle \approx 18\)</span> mRNA/cell.</p>
<figure>
<img src="ch5_fig01.png" id="fig:ch5_fig01" data-short-caption="lacUV5* mRNA per cell distribution" alt="Figure 1: lacUV5 mRNA per cell distribution. Data from  [4] of the unregulated lacUV5 promoter as inferred from single-molecule mRNA FISH. The Python code (ch5_fig01.py) used to generate this figure can be found on the original paper GitHub repository." /><figcaption aria-hidden="true">Figure 1: <strong><em>lacUV5</em> mRNA per cell distribution.</strong> Data from <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">4</a>]</span> of the unregulated <em>lacUV5</em> promoter as inferred from single-molecule mRNA FISH. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS01.py">(<code>ch5_fig01.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a></figcaption>
</figure>
<p>Having this data in hand, we now turn to Bayesian parameter inference. Writing Bayes theorem we have <span class="math display">\[
  P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m \mid D) = 
  \frac{P(D \mid k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m)
  P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m)}{P(D)},
  \label{eq:bayes_rnap_rates}
\]</span> where <span class="math inline">\(D\)</span> represents the data. For this case, the data consists of single-cell mRNA counts <span class="math inline">\(D = \{ m_1, m_2, \ldots, m_N \}\)</span>, where <span class="math inline">\(N\)</span> is the number of cells. We assume that each cell’s measurement is independent of the others such that we can rewrite Eq. <span class="math inline">\(\ref{eq:bayes_rnap_rates}\)</span> as <span class="math display">\[
P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m \mid \{m_i\}) \propto
  \left[\prod_{i=1}^N P(m_i \mid k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m) \right]
  P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m),
  \label{eq:bayes_sample}
\]</span> where we ignore the normalization constant <span class="math inline">\(P(D)\)</span>. The likelihood term <span class="math inline">\(P(m_i \mid k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m)\)</span> is exactly given Eq. <span class="math inline">\(\ref{eq:two_state_mRNA}\)</span> by with <span class="math inline">\(\gamma _m = 1\)</span>. Given that we have this functional form for the distribution, we can use Markov Chain Monte Carlo (MCMC) sampling to explore the 3D parameter space in order to fit <span class="math inline">\(\ref{eq:two_state_mRNA}\)</span> to the mRNA-FISH data.</p>
<h4 id="constraining-the-rates-given-prior-thermodynamic-knowledge.">Constraining the rates given prior thermodynamic knowledge.</h4>
<p>One of the Bayesian approach’s strengths is that we can include all the prior knowledge on the parameters when performing an inference <span class="citation" data-cites="MacKay2003"> [<a href="#ref-MacKay2003" role="doc-biblioref">8</a>]</span>. Basic features such as the fact that the rates have to be strictly positive constrain these parameters’ values. We know more than the simple constraint of non-negative values for the specific rates analyzed in this section. The expression of an unregulated promoter has been studied from a thermodynamic perspective <span class="citation" data-cites="Brewster2012"> [<a href="#ref-Brewster2012" role="doc-biblioref">9</a>]</span>. Given the underlying assumptions of these equilibrium models, in which the probability of finding the RNAP bound to the promoter is proportional to the transcription rate <span class="citation" data-cites="Bintu2005a"> [<a href="#ref-Bintu2005a" role="doc-biblioref">10</a>]</span>, they can only make statements about the mean expression level. Nevertheless, if both the thermodynamic and kinetic models describe the same process, the mean gene expression level predictions must agree. That means that we can use what we know about the mean gene expression and how this is related to parameters such as molecule copy numbers and binding affinities to constrain the values that the rates in question can take.</p>
<p>In the case of this two-state promoter, it can be shown that the mean number of mRNA is given by <span class="citation" data-cites="Sanchez2013"> [<a href="#ref-Sanchez2013" role="doc-biblioref">1</a>]</span> (See XXX for moment computation) <span class="math display">\[
\langle m \rangle = \frac{r_m}{\gamma _m}
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}.
  \label{eq:mean_kinetic}
\]</span> Another way of expressing this is as <span class="math inline">\(\frac{r_m}{\gamma _m} \times p_{\text{active}}^{(p)}\)</span>, where <span class="math inline">\(p_{\text{active}}^{(p)}\)</span> is the probability of the promoter being in the transcriptionally active state. The thermodynamic picture has an equivalent result where the mean number of mRNA is given by <span class="citation" data-cites="Brewster2012 Bintu2005a"> [<a href="#ref-Brewster2012" role="doc-biblioref">9</a>,<a href="#ref-Bintu2005a" role="doc-biblioref">10</a>]</span> <span class="math display">\[
\left\langle m \right\rangle = \frac{r_m}{\gamma _m}
\frac{\frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p}
}{1 + \frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p}},
  \label{eq:mean_thermo}
\]</span> where <span class="math inline">\(P\)</span> is the number of RNAP per cell, <span class="math inline">\(N_{NS}\)</span> is the number of non-specific binding sites, <span class="math inline">\(\Delta\varepsilon_p\)</span> is the RNAP binding energy in <span class="math inline">\(k_BT\)</span> units and <span class="math inline">\(\beta\equiv {(k_BT)}^{-1}\)</span>. Using Eq. <span class="math inline">\(\ref{eq:mean_kinetic}\)</span> and Eq. <span class="math inline">\(\ref{eq:mean_thermo}\)</span> we can easily see that if these frameworks are to be equivalent, then it must be true that <span class="math display">\[
\frac{k^{(p)}_{\text{on}}}{ k^{(p)}_{\text{off}}} =
\frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p},
\]</span> or equivalently <span class="math display">\[
\ln \left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) =
  -\beta\Delta\varepsilon_p + \ln P - \ln N_{NS}.
\]</span> To put numerical values into these variables, we can use information from the literature. The RNAP copy number is order <span class="math inline">\(P \approx 1000-3000\)</span> RNAP/cell for a one-hour doubling time <span class="citation" data-cites="Klumpp2008"> [<a href="#ref-Klumpp2008" role="doc-biblioref">11</a>]</span>. As for the number of non-specific binding sites and the binding energy, we have that <span class="math inline">\(N_{NS} = 4.6\times 10^6\)</span> <span class="citation" data-cites="Bintu2005a"> [<a href="#ref-Bintu2005a" role="doc-biblioref">10</a>]</span> and <span class="math inline">\(-\beta\Delta\varepsilon_p \approx 5 - 7\)</span> <span class="citation" data-cites="Brewster2012"> [<a href="#ref-Brewster2012" role="doc-biblioref">9</a>]</span>. Given these values, we define a Gaussian prior for the log ratio of these two quantities of the form <span class="math display">\[
P\left(\ln \left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) \right) \propto
\exp \left\{ - \frac{\left(\ln \left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) -
\left(-\beta\Delta\varepsilon_p + \ln P - \ln N_{NS} \right) \right)^2
}{2 \sigma^2} \right\},
  \label{eq:prior_single}
\]</span> where <span class="math inline">\(\sigma\)</span> is the variance that accounts for the uncertainty in these parameters. We include this prior as part of the prior term <span class="math inline">\(P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m)\)</span> of Eq. <span class="math inline">\(\ref{eq:bayes_sample}\)</span>. We then use MCMC to sample the posterior distribution given by Eq. <span class="math inline">\(\ref{eq:bayes_sample}\)</span>. Fig. 2 shows the MCMC samples of the posterior distribution. For the case of the <span class="math inline">\(k^{(p)}_{\text{on}}\)</span> parameter, there is a single symmetric peak. <span class="math inline">\(k^{(p)}_{\text{off}}\)</span> and <span class="math inline">\(r_m\)</span> have a rather long tail towards large values. The 2D projection of <span class="math inline">\(k^{(p)}_{\text{off}}\)</span> vs. <span class="math inline">\(r_m\)</span> shows that the model is sloppy, meaning that the two parameters are highly correlated. This feature is a common problem for many non-linear systems used in biophysics and systems biology <span class="citation" data-cites="Transtrum2015"> [<a href="#ref-Transtrum2015" role="doc-biblioref">12</a>]</span>. What this implies is that we can change the value of <span class="math inline">\(k^{(p)}_{\text{off}}\)</span>, and then compensate by a change in <span class="math inline">\(r_m\)</span> to maintain the shape of the mRNA distribution. Therefore, it is impossible for the data and the model to narrow down a single value for the parameters. Nevertheless, since we included the prior information on the rates as given by the analogous form between the equilibrium and non-equilibrium expressions for the mean mRNA level, we obtained a more constrained parameter value for the RNAP rates and the transcription rate we will take as the peak of this long-tailed distribution.</p>
<figure>
<img src="ch5_fig02.png" id="fig:ch5_fig02" data-short-caption="MCMC posterior distribution." alt="Figure 2: MCMC posterior distribution. Sampling out of Eq. \ref{eq:bayes_sample} the plot shows 2D and 1D projections of the 3D parameter space. The parameter values are (in units of the mRNA degradation rate \gamma _m) k^{(p)}_{\text{on}} = 4.3^{+1}_{-0.3}, k^{(p)}_{\text{off}} = 18.8^{+120}_{-10} and r_m = 103.8^{+423}_{-37} which are the modes of their respective distributions, where the superscripts and subscripts represent the upper and lower bounds of the 95^\text{th} percentile of the parameter value distributions. The Python code (ch5_fig02.py) used to generate this figure can be found on the original paper GitHub repository." /><figcaption aria-hidden="true">Figure 2: <strong>MCMC posterior distribution.</strong> Sampling out of Eq. <span class="math inline">\(\ref{eq:bayes_sample}\)</span> the plot shows 2D and 1D projections of the 3D parameter space. The parameter values are (in units of the mRNA degradation rate <span class="math inline">\(\gamma _m\)</span>) <span class="math inline">\(k^{(p)}_{\text{on}} = 4.3^{+1}_{-0.3}\)</span>, <span class="math inline">\(k^{(p)}_{\text{off}} = 18.8^{+120}_{-10}\)</span> and <span class="math inline">\(r_m = 103.8^{+423}_{-37}\)</span> which are the modes of their respective distributions, where the superscripts and subscripts represent the upper and lower bounds of the 95<span class="math inline">\(^\text{th}\)</span> percentile of the parameter value distributions. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS02.py">(<code>ch5_fig02.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a></figcaption>
</figure>
<p>The inferred values <span class="math inline">\(k^{(p)}_{\text{on}} = 4.3^{+1}_{-0.3}\)</span>, <span class="math inline">\(k^{(p)}_{\text{off}} = 18.8^{+120}_{-10}\)</span> and <span class="math inline">\(r_m = 103.8^{+423}_{-37}\)</span> are given in units of the mRNA degradation rate <span class="math inline">\(\gamma _m\)</span>. Given the asymmetry of the parameter distributions we report the upper and lower bound of the 95<span class="math inline">\(^\text{th}\)</span> percentile of the posterior distributions. Assuming a mean life-time for mRNA of <span class="math inline">\(\approx\)</span> 3 min (from this <a href="http://bionumbers.hms.harvard.edu/bionumber.aspx?&amp;id=107514&amp;ver=1&amp;trm=mRNA%20mean%20lifetime">link</a>) we have an mRNA degradation rate of <span class="math inline">\(\gamma _m \approx 2.84 \times 10^{-3} s^{-1}\)</span>. Using this value gives the following values for the inferred rates: <span class="math inline">\(k^{(p)}_{\text{on}} = 0.024_{-0.002}^{+0.005} s^{-1}\)</span>, <span class="math inline">\(k^{(p)}_{\text{off}} = {0.11}_ {-0.05}^{+0.66} s^{-1}\)</span>, and <span class="math inline">\(r_m = 0.3_{-0.2}^{+2.3} s^{-1}\)</span>.</p>
<p>Fig. 3 compares the experimental data from Fig. 1 with the resulting distribution obtained by substituting the most likely parameter values into Eq. <span class="math inline">\(\ref{eq:two_state_mRNA}\)</span>. As we can see, this two-state model fits the data adequately.</p>
<figure>
<img src="ch5_fig03.png" id="fig:ch5_fig03" data-short-caption="Experimental vs. theoretical distribution of mRNA per cell using parameters from Bayesian inference" alt="Figure 3: Experimental vs. theoretical distribution of mRNA per cell using parameters from Bayesian inference. Dotted line shows the result of using Eq. \ref{eq:two_state_mRNA} along with the parameters inferred for the rates. Blue bars are the same data as Fig. 1 obtained from  [4]. The Python code (ch5_fig03.py) used to generate this figure can be found on the original paper GitHub repository." /><figcaption aria-hidden="true">Figure 3: <strong>Experimental vs. theoretical distribution of mRNA per cell using parameters from Bayesian inference.</strong> Dotted line shows the result of using Eq. <span class="math inline">\(\ref{eq:two_state_mRNA}\)</span> along with the parameters inferred for the rates. Blue bars are the same data as Fig. 1 obtained from <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">4</a>]</span>. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS03.py">(<code>ch5_fig03.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a></figcaption>
</figure>
<h3 id="accounting-for-variability-in-the-number-of-promoters">Accounting for variability in the number of promoters</h3>
<p>As discussed in ref. <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">4</a>]</span> and further expanded in <span class="citation" data-cites="Peterson2015"> [<a href="#ref-Peterson2015" role="doc-biblioref">13</a>]</span> an essential source of cell-to-cell variability in gene expression in bacteria because, depending on the growth rate and the position relative to the chromosome replication origin, cells can have multiple copies of any given gene. Genes closer to the replication origin have, on average, higher gene copy numbers compared to genes at the opposite end. For the locus in which our reporter construct is located (<em>galK</em>) and the doubling time of the mRNA FISH experiments, we expect to have <span class="math inline">\(\approx\)</span> 1.66 copies of the gene <span class="citation" data-cites="Jones2014a Bremer1996"> [<a href="#ref-Jones2014a" role="doc-biblioref">4</a>,<a href="#ref-Bremer1996" role="doc-biblioref">14</a>]</span>. This implies that the cells spend 2/3 of the cell cycle with two copies of the promoter and the rest with a single copy.</p>
<p>To account for this variability in gene copy, we extend the model assuming that when cells have two copies of the promoter, the mRNA production rate is <span class="math inline">\(2 r_m\)</span> compared to the rate <span class="math inline">\(r_m\)</span> for a single promoter copy. The probability of observing a particular mRNA copy <span class="math inline">\(m\)</span> is therefore given by <span class="math display">\[
P(m) = P(m \mid \text{one promoter}) \cdot P(\text{one promoter}) +
P(m \mid \text{two promoters}) \cdot P(\text{two promoters}).
  \label{eq:prob_multipromoter}
\]</span> Both terms <span class="math inline">\(P(m \mid \text{promoter copy})\)</span> are given by Eq. <span class="math inline">\(\ref{eq:two_state_mRNA}\)</span> with the only difference being the rate <span class="math inline">\(r_m\)</span>. It is important to acknowledge that Eq. <span class="math inline">\(\ref{eq:prob_multipromoter}\)</span> assumes that once the gene is replicated, the time scale in which the mRNA count relaxes to the new steady state is much shorter than the time that the cells spend in this two promoter copies state. This approximation should be valid for a short-lived mRNA molecule, but the assumption is not applicable for proteins whose degradation rate is comparable to the cell cycle length as explored in XXX.</p>
<p>To repeat the Bayesian inference, including this variability in gene copy number, we must split the mRNA count data into two sets – cells with a single copy of the promoter and cells with two copies of the promoter. For the single-molecule mRNA FISH data, there is no labeling of the locus, making it impossible to determine the promoter’s number of copies for any given cell. We, therefore, follow Jones et al. <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">4</a>]</span> in using the cell area as a proxy for the stage in the cell cycle. They sorted cells by area in their approach, considering cells below the 33<span class="math inline">\(^\text{th}\)</span> percentile having a single promoter copy and the rest as having two copies. This approach ignores that cells are not uniformly distributed along the cell cycle. As first derived in <span class="citation" data-cites="Powell1956"> [<a href="#ref-Powell1956" role="doc-biblioref">15</a>]</span> populations of cells in a log-phase are exponentially distributed along the cell cycle. This distribution is of the form <span class="math display">\[
P(a) = (\ln 2) \cdot 2^{1 - a},
\label{eq:cell_cycle_dist}
\]</span> where <span class="math inline">\(a \in [0, 1]\)</span> is the stage of the cell cycle, with <span class="math inline">\(a = 0\)</span> being the start of the cycle and <span class="math inline">\(a = 1\)</span> being the cell division (See XXX for a derivation of Eq. <span class="math inline">\(\ref{eq:cell_cycle_dist}\)</span>). Fig. 4 shows the separation of the two groups based on the area where was used to weight the distribution along the cell cycle.</p>
<figure>
<img src="ch5_fig04.png" id="fig:ch5_fig04" data-short-caption="Separation of cells based on cell size" alt="Figure 4: Separation of cells based on cell size. Using the area as a proxy for position in the cell cycle, cells can be sorted into two groups–small cells (with one promoter copy) and large cells (with two promoter copies). The vertical black line delimits the threshold that divides both groups as weighted by Eq. \ref{eq:cell_cycle_dist}. The Python code (ch5_fig04.py) used to generate this figure can be found on the original paper GitHub repository." /><figcaption aria-hidden="true">Figure 4: <strong>Separation of cells based on cell size.</strong> Using the area as a proxy for position in the cell cycle, cells can be sorted into two groups–small cells (with one promoter copy) and large cells (with two promoter copies). The vertical black line delimits the threshold that divides both groups as weighted by Eq. <span class="math inline">\(\ref{eq:cell_cycle_dist}\)</span>. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS04.py">(<code>ch5_fig04.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a></figcaption>
</figure>
<p>A subtle but important consequence of Eq. <span class="math inline">\(\ref{eq:cell_cycle_dist}\)</span> is that computing any quantity for a single cell is not equivalent to computing the same quantity for a population of cells. For example, let us assume that we want to compute the mean mRNA copy number <span class="math inline">\(\langle m \rangle\)</span>. For a single cell, this would be of the form <span class="math display">\[
\langle m \rangle_{\text{cell}} =
\langle m \rangle_1 \cdot f + \langle m \rangle_2 \cdot (1 - f),
\]</span> where <span class="math inline">\(\langle m \rangle_i\)</span> is the mean mRNA copy number with <span class="math inline">\(i\)</span> promoter copies in the cell and <span class="math inline">\(f\)</span> is the fraction of the cell cycle that cells spend with a single copy of the promoter. For a single cell, the probability of having a single promoter copy is equivalent to this fraction <span class="math inline">\(f\)</span>. But Eq. <span class="math inline">\(\ref{eq:cell_cycle_dist}\)</span> tells us that if we sample unsynchronized cells we are not sampling uniformly across the cell cycle. Therefore for a population of cells, the mean mRNA is given by <span class="math display">\[
\langle m \rangle_{\text{population}} =
\langle m \rangle_1 \cdot \phi + \langle m \rangle_2 \cdot (1 - \phi)
  \label{eq:mean_m_pop}
\]</span> where the probability of sampling a cell with one promoter <span class="math inline">\(\phi\)</span> is given by <span class="math display">\[
\phi = \int_0^f P(a) da,
\]</span> where <span class="math inline">\(P(a)\)</span> is given by Eq. <span class="math inline">\(\ref{eq:eq:cell_cycle_dist}\)</span>. What this equation computes is the probability of sampling a cell during a stage of the cell cycle <span class="math inline">\(&lt; f\)</span> where the reporter gene hasn’t been replicated yet. Fig. 5 shows the distribution of both groups. As expected, larger cells have a higher mRNA copy number on average.</p>
<figure>
<img src="ch5_fig05.png" id="fig:ch5_fig05" data-short-caption="mRNA distribution for small and large cells." alt="Figure 5: mRNA distribution for small and large cells. (A) histogram and (B) the cumulative distribution function of the small and large cells as determined in Fig. 4. The triangles above histograms in (A) indicate the mean mRNA copy number for each group. The Python code (ch5_fig05.py) used to generate this figure can be found on the original paper GitHub repository." /><figcaption aria-hidden="true">Figure 5: <strong>mRNA distribution for small and large cells.</strong> (A) histogram and (B) the cumulative distribution function of the small and large cells as determined in Fig. 4. The triangles above histograms in (A) indicate the mean mRNA copy number for each group. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS05.py">(<code>ch5_fig05.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a></figcaption>
</figure>
<p>We modify Eq. <span class="math inline">\(\ref{eq:bayes_sample}\)</span> to account for the two separate groups of cells. Let <span class="math inline">\(N_s\)</span> be the number of cells in the small size group and <span class="math inline">\(N_l\)</span> the number of cells in the large size group. Then the posterior distribution for the parameters is of the form <span class="math display">\[
\small
P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m \mid \{m_i\}) \propto
\left[\prod_{i=1}^{N_s} P(m_i \mid k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m)\right]
\left[\prod_{j=1}^{N_l} P(m_j \mid k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, 2 r_m)\right]
P(k^{(p)}_{\text{on}}, k^{(p)}_{\text{off}}, r_m),
  \label{eq:bayes_sample_double}
\]</span> where we split the product of small and large cells.</p>
<p>For the two-promoter model, the prior shown in Eq. <span class="math inline">\(\ref{eq:prior_single}\)</span> requires a small modification. Eq. <span class="math inline">\(\ref{eq:mean_m_pop}\)</span> gives the mean mRNA copy number of a population of asynchronous cells growing at steady-state. Given that we assume that the only difference between having one vs. two promoter copies state is the change in transcription rate from <span class="math inline">\(r_m\)</span> in the single promoter case to <span class="math inline">\(2 r_m\)</span> in the two-promoter case, we can write Eq. <span class="math inline">\(\ref{eq:mean_m_pop}\)</span> as <span class="math display">\[
\langle m \rangle = \phi \cdot \frac{r_m}{\gamma _m}
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}} +
(1 -\phi) \cdot \frac{2 r_m}{\gamma _m}
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}.
\]</span> This can be simplified to <span class="math display">\[
\langle m \rangle = (2 - \phi) \frac{r_m}{\gamma _m} 
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}.
  \label{eq:mean_m_double_rates}
\]</span></p>
<p>Equating Eq. <span class="math inline">\(\ref{eq:mean_m_double_rates}\)</span> and Eq. <span class="math inline">\(\ref{eq:mean_thermo}\)</span> to again require self-consistent predictions of the mean mRNA from the equilibrium and kinetic models gives <span class="math display">\[
(2 - \phi) \frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}} =
\frac{\frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p}
}{1 + \frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p}}.
\]</span> Solving for <span class="math inline">\(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\)</span> results in <span class="math display">\[
\left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) =
\frac{\rho}{\left[ (1 + \rho)(2 - \phi) - \rho \right]},
  \label{eq:kinetic_thermo_equiv}
\]</span> where we define <span class="math inline">\(\rho \equiv \frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p}\)</span>. To simplify things further we notice that for the specified values of <span class="math inline">\(P = 1000 - 3000\)</span> per cell, <span class="math inline">\(N_{NS} = 4.6 \times 10^6\)</span> bp, and <span class="math inline">\(-\beta\Delta\varepsilon_p = 5 - 7\)</span>, we can safely assume that <span class="math inline">\(\rho \ll 1\)</span>. This simplifying assumption has been previously called the weak promoter approximation <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">5</a>]</span>. Given this we can simplify Eq. <span class="math inline">\(\ref{eq:kinetic_thermo_equiv}\)</span> as <span class="math display">\[
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}} =
\frac{1}{2 - \phi} \frac{P}{N_{NS}} e^{-\beta\Delta\varepsilon_p}.
\]</span> Taking the log of both sides gives <span class="math display">\[
\ln\left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) =
-\ln (2 - \phi) + \ln P - \ln N_{NS}
  - \beta\Delta\varepsilon_p.
\]</span> With this, we can set as before a Gaussian prior to constrain the ratio of the RNAP rates as <span class="math display">\[
P\left(\ln \left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) \right)  \propto
\exp \left\{ - \frac{\left(\ln \left(\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}}\right) -
\left[-\ln(2 - \phi) -\beta\Delta\varepsilon_p + \ln P - \ln N_{NS} \right) \right]^2
}{2 \sigma^2} \right\}.
  \label{eq:prior_double}
\]</span></p>
<p>Fig. 6 shows the result of sampling out of Eq. <span class="math inline">\(\ref{eq:bayes_sample_double}\)</span>. Again we see that the model is highly sloppy with large credible regions obtained for <span class="math inline">\(k^{(p)}_{\text{off}}\)</span> and <span class="math inline">\(r_m\)</span>. Nevertheless, again the use of the prior information allows us to get a parameter values consistent with the equilibrium picture.</p>
<figure>
<img src="ch5_fig06.png" id="fig:ch5_fig06" data-short-caption="MCMC posterior distribution for a multi-promoter model" alt="Figure 6: MCMC posterior distribution for a multi-promoter model. Sampling out of Eq. \ref{eq:bayes_sample_double} the plot shows 2D and 1D projections of the 3D parameter space. The parameter values are (in units of the mRNA degradation rate \gamma _m) k^{(p)}_{\text{on}} = 6.4^{+0.8}_{-0.4}, k^{(p)}_{\text{off}} = 132^{+737}_{-75} and r_m = 257^{+1307}_{-132} which are the modes of their respective distributions, where the superscripts and subscripts represent the upper and lower bounds of the 95^\text{th} percentile of the parameter value distributions. The sampling was bounded to values &lt; 1000 for numerical stability when computing the confluent hypergeometric function. The Python code (ch5_fig06.py) used to generate this figure can be found on the original paper GitHub repository." /><figcaption aria-hidden="true">Figure 6: <strong>MCMC posterior distribution for a multi-promoter model.</strong> Sampling out of Eq. <span class="math inline">\(\ref{eq:bayes_sample_double}\)</span> the plot shows 2D and 1D projections of the 3D parameter space. The parameter values are (in units of the mRNA degradation rate <span class="math inline">\(\gamma _m\)</span>) <span class="math inline">\(k^{(p)}_{\text{on}} = 6.4^{+0.8}_{-0.4}\)</span>, <span class="math inline">\(k^{(p)}_{\text{off}} = 132^{+737}_{-75}\)</span> and <span class="math inline">\(r_m = 257^{+1307}_{-132}\)</span> which are the modes of their respective distributions, where the superscripts and subscripts represent the upper and lower bounds of the 95<span class="math inline">\(^\text{th}\)</span> percentile of the parameter value distributions. The sampling was bounded to values <span class="math inline">\(&lt;\)</span> 1000 for numerical stability when computing the confluent hypergeometric function. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS06.py">(<code>ch5_fig06.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a></figcaption>
</figure>
<p>Using again a mRNA mean lifetime of <span class="math inline">\(\approx 3\)</span> min gives the following values for the parameters: <span class="math inline">\(k^{(p)}_{\text{on}} = {0.03}_{-0.002}^{+0.004} s^{-1}\)</span>, <span class="math inline">\(k^{(p)}_{\text{off}} = {0.7} _{-0.4}^{+4.1} s^{-1}\)</span>, and <span class="math inline">\(r_m = {1.4}_{-0.7}^{+7.3} s^{-1}\)</span>. Fig. 7 shows the result of applying Eq. <span class="math inline">\(\ref{eq:prob_multipromoter}\)</span> using these parameter values. Specifically Fig. 7(A) shows the global distribution, including cells with one and two promoters and Fig. 7(B) split the distributions within the two populations. Given that the model adequately describes both populations independently and pooled together, we confirm that using the cell area as a proxy for the stage in the cell cycle and the doubling of the transcription rate once cells have two promoters are reasonable approximations.</p>
<figure>
<img src="ch5_fig07.png" id="fig:ch5_fig07" data-short-caption="Experimental vs. theoretical distribution of mRNA per cell using parameters for multi-promoter model" alt="Figure 7: Experimental vs. theoretical distribution of mRNA per cell using parameters for multi-promoter model. (A) Solid line shows the result of using Eq. \ref{eq:prob_multipromoter} with the parameters inferred by sampling Eq. \ref{eq:bayes_sample_double}. Blue bars are the same data as Fig. 1 from  [4]. (B) Split distributions of small cells (light blue bars) and large cells (dark blue) with the corresponding theoretical predictions with transcription rate r_m (light blue line) and transcription rate 2 r_m (dark blue line). The Python code (ch5_fig07.py) used to generate this figure can be found on the original paper GitHub repository." /><figcaption aria-hidden="true">Figure 7: <strong>Experimental vs. theoretical distribution of mRNA per cell using parameters for multi-promoter model.</strong> (A) Solid line shows the result of using Eq. <span class="math inline">\(\ref{eq:prob_multipromoter}\)</span> with the parameters inferred by sampling Eq. <span class="math inline">\(\ref{eq:bayes_sample_double}\)</span>. Blue bars are the same data as Fig. 1 from <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">4</a>]</span>. (B) Split distributions of small cells (light blue bars) and large cells (dark blue) with the corresponding theoretical predictions with transcription rate <span class="math inline">\(r_m\)</span> (light blue line) and transcription rate <span class="math inline">\(2 r_m\)</span> (dark blue line). The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS07.py">(<code>ch5_fig07.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a></figcaption>
</figure>
<p>It is hard to make comparisons with literature reported values because these kinetic rates are effective parameters hiding a lot of the complexity of transcription initiation <span class="citation" data-cites="Browning2004"> [<a href="#ref-Browning2004" role="doc-biblioref">16</a>]</span>. Also, the parameters’ non-identifiability restricts our explicit comparison of the actual numerical values of the inferred rates. Nevertheless, from the model, we can see that the mean burst size for each transcription event is given by <span class="math inline">\(r_m / k^{(p)}_{\text{off}}\)</span>. We obtain a mean burst size of <span class="math inline">\(\approx 1.9\)</span> transcripts per cell from our inferred values. This mean burst size is similar to the reported burst size of 1.15 on a similar system on <em>E. coli</em> <span class="citation" data-cites="Yu2006"> [<a href="#ref-Yu2006" role="doc-biblioref">17</a>]</span>.</p>
<h3 id="repressor-rates-from-three-state-regulated-promoter.">Repressor rates from three-state regulated promoter.</h3>
<p>Having determined the unregulated promoter transition rates we now proceed to determine the repressor rates <span class="math inline">\(k^{(r)}_{\text{on}}\)</span> and <span class="math inline">\(k^{(r)}_{\text{off}}\)</span>. These rates’ values are constrained again by the correspondence between our kinetic picture and what we know from equilibrium models <span class="citation" data-cites="Phillips2015a"> [<a href="#ref-Phillips2015a" role="doc-biblioref">18</a>]</span>. For this analysis, we again exploit the feature that, at the mean, both the kinetic language and the thermodynamic language should have equivalent predictions. Over the last decade, there has been a great effort in developing equilibrium models for gene expression regulation <span class="citation" data-cites="Buchler2003 Vilar2011 Bintu2005a"> [<a href="#ref-Bintu2005a" role="doc-biblioref">10</a>,<a href="#ref-Buchler2003" role="doc-biblioref">19</a>,<a href="#ref-Vilar2011" role="doc-biblioref">20</a>]</span>. In particular, our group has extensively characterized the simple repression motif using this formalism <span class="citation" data-cites="Garcia2011c Brewster2014 Razo-Mejia2018"> [<a href="#ref-Garcia2011c" role="doc-biblioref">5</a>–<a href="#ref-Razo-Mejia2018" role="doc-biblioref">7</a>]</span>.</p>
<p>The dialogue between theory and experiments has led to simplified expressions that capture the phenomenology of the gene expression response as a function of natural variables such as molecule count and affinities between molecular players. A particularly interesting quantity for the simple repression motif used by Garcia &amp; Phillips <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">5</a>]</span> is the fold-change in gene expression, defined as <span class="math display">\[
\text{fold-change} =
\frac{\left\langle{\text{gene expression}(R \neq 0)}\right\rangle}{
\left\langle{\text{gene expression}(R = 0)}\right\rangle},
\]</span> where <span class="math inline">\(R\)</span> is the number of repressors per cell and <span class="math inline">\(\left\langle{\cdot}\right\rangle\)</span> is the population average. The fold-change is simply the mean expression level in the presence of the repressor relative to the mean expression level in the absence of regulation. In the language of statistical mechanics this quantity takes the form <span class="math display">\[
\text{fold-change} = \left( 1 + \frac{R}{N_{NS}}
e^{-\beta\Delta\varepsilon_r} \right)^{-1},
  \label{eq:fc_thermo}
\]</span> where <span class="math inline">\(\Delta\varepsilon_r\)</span> is the repressor-DNA binding energy, and as before <span class="math inline">\(N_{NS}\)</span> is the number of non-specific binding sites where the repressor can bind <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">5</a>]</span>.</p>
<p>To compute the fold-change in the chemical master equation language, we compute the first moment of the steady-state mRNA distribution <span class="math inline">\(\langle m \rangle\)</span> for both the three-state promoter (<span class="math inline">\(R \neq 0\)</span>) and the two-state promoter case (<span class="math inline">\(R=0\)</span>) (See XXX for moment derivation). The unregulated (two-state) promoter mean mRNA copy number is given by Eq. <span class="math inline">\(\ref{eq:mean_m_double_rates}\)</span>. For the regulated (three-state) promoter, we have an equivalent expression of the form <span class="math display">\[
\left\langle{m (R \neq 0)}\right\rangle = 
(2 - \phi)\frac{r_m}{\gamma _m}
\frac{k^{(r)}_{\text{off}}k^{(p)}_{\text{on}}
}{k^{(p)}_{\text{off}}k^{(r)}_{\text{off}} +
k^{(p)}_{\text{off}}k^{(r)}_{\text{on}} + 
k^{(r)}_{\text{off}}k^{(p)}_{\text{on}}}.
\]</span> Computing the fold-change then gives <span class="math display">\[
\text{fold-change} =
\frac{\left\langle{m (R \neq 0)}\right\rangle}
{\left\langle{m (R = 0)}\right\rangle} =
\frac{k^{(r)}_{\text{off}} \left( k^{(p)}_{\text{off}} + 
k^{(p)}_{\text{on}} \right)}{
k^{(p)}_{\text{off}}k^{(r)}_{\text{on}} +
k^{(r)}_{\text{off}} \left( k^{(p)}_{\text{off}} + k^{(p)}_{\text{on}} \right)},
  \label{eq:fold_change_cme}
\]</span> where the factor <span class="math inline">\((2 - \phi)\)</span> due to the multiple promoter copies, the transcription rate <span class="math inline">\(r_m\)</span> and the mRNA degradation rate <span class="math inline">\(\gamma _m\)</span> cancel out.</p>
<p>Given that the number of repressors per cell <span class="math inline">\(R\)</span> is an experimental variable that we can control, we assume that the rate at which the promoter transitions from the transcriptionally inactive state to the repressor bound state, <span class="math inline">\(k^{(r)}_{\text{on}}\)</span>, is given by the concentration of repressors <span class="math inline">\([R]\)</span> times a diffusion-limited on rate <span class="math inline">\(k_o\)</span>. For the diffusion-limited constant <span class="math inline">\(k_o\)</span> we use the value used by Jones et al. <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">4</a>]</span>. With this in hand we can rewrite Eq. <span class="math inline">\(\ref{eq:fold_change_cme}\)</span> as <span class="math display">\[
\text{fold-change} = \left( 1 + \frac{k_0 [R]}{k^{(r)}_{\text{off}}}
\frac{k^{(p)}_{\text{off}}}{k^{(p)}_{\text{on}} +
k^{(p)}_{\text{off}}} \right)^{-1}.
  \label{eq:fc_kinetic}
\]</span></p>
<p>We note that both Eq. <span class="math inline">\(\ref{eq:fc_thermo}\)</span> and Eq. <span class="math inline">\(\ref{eq:fc_kinetic}\)</span> have the same functional form. Therefore if both languages predict the same output for the mean gene expression level, it must be true that <span class="math display">\[
\frac{k_o [R]}{k^{(r)}_{\text{off}}}
\frac{k^{(p)}_{\text{off}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}} =
\frac{R}{N_{NS}} e^{-\beta\Delta\varepsilon_r}.
\]</span> Solving for <span class="math inline">\(k^{(r)}_{\text{off}}\)</span> gives <span class="math display">\[
k^{(r)}_{\text{off}} = \frac{k_o [R] N_{NS}
e^{\beta\Delta\varepsilon_r}}{R}
\frac{k^{(p)}_{\text{off}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}.
  \label{eq:kroff_complete}
\]</span></p>
<p>Since the reported value of <span class="math inline">\(k_o\)</span> is given in units of nM<span class="math inline">\(^{-1}\)</span>s<span class="math inline">\(^{-1}\)</span> for the units to cancel properly, the repressor concentration must be given in nM rather than absolute count. If we consider that the repressor concentration is equal to <span class="math display">\[
[R] = \frac{R}{V_{cell}}\cdot \frac{1}{N_A},
\]</span> where <span class="math inline">\(R\)</span> is the absolute repressor copy number per cell, <span class="math inline">\(V_{cell}\)</span> is the cell volume and <span class="math inline">\(N_A\)</span> is Avogadro’s number. The <em>E. coli</em> cell volume is 2.1 fL <span class="citation" data-cites="Radzikowski2016"> [<a href="#ref-Radzikowski2016" role="doc-biblioref">21</a>]</span>, and Avogadro’s number is <span class="math inline">\(6.022 \times 10^{23}\)</span>. If we further include the conversion factor to turn M into nM we find that <span class="math display">\[
[R] = \frac{R}{2.1 \times 10^{-15} L} \cdot \frac{1}{6.022 \times 10^{23}}
\cdot \frac{10^9 \text{ nmol}}{1 \text{ mol}} \approx 0.8 \times R.
\]</span> Using this we simplify Eq. <span class="math inline">\(\ref{eq:kroff_complete}\)</span> as <span class="math display">\[
k^{(r)}_{\text{off}} \approx 0.8 \cdot k_o \cdot
N_{NS} e^{\beta\Delta\varepsilon_r}
\cdot \frac{k^{(p)}_{\text{off}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}.
  \label{eq:kroff}
\]</span> What Eq. <span class="math inline">\(\ref{eq:kroff}\)</span> shows is the direct relationship that must be satisfied if the equilibrium model is set to be consistent with the non-equilibrium kinetic picture. Tbl. 1 summarizes the values obtained for the three operator sequences used throughout this work. To compute these numbers, the number of non-specific binding sites <span class="math inline">\(N_{NS}\)</span> was taken to be <span class="math inline">\(4.6 \times 10^6\)</span> bp, i.e., the size of the <em>E. coli</em> K12 genome.</p>
<div id="tbl:ch5_tbl01">
<table>
<caption>Table 1: <strong>Binding sites and corresponding parameters.</strong></caption>
<thead>
<tr class="header">
<th>Operator</th>
<th><span class="math inline">\(\Delta\varepsilon_r\; (k_BT)\)</span></th>
<th><span class="math inline">\(k^{(r)}_{\text{off}}\; (s^{-1})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>O1</td>
<td>-15.3</td>
<td>0.002</td>
</tr>
<tr class="even">
<td>O2</td>
<td>-13.9</td>
<td>0.008</td>
</tr>
<tr class="odd">
<td>O3</td>
<td>-9.7</td>
<td>0.55</td>
</tr>
</tbody>
</table>
</div>
<p><em>In-vivo</em> measurements of the Lac repressor off rate have been done with single-molecule resolution <span class="citation" data-cites="Hammar2014"> [<a href="#ref-Hammar2014" role="doc-biblioref">22</a>]</span>. The authors report a mean residence time of <span class="math inline">\(5.3 \pm 0.2\)</span> minutes for the repressor on an O1 operator. The corresponding rate is <span class="math inline">\(k^{(r)}_{\text{off}} \approx 0.003\)</span> <span class="math inline">\((s^{-1})\)</span>, very similar value to what we inferred from our model. In this same reference, the authors determined that, on average, the repressor takes <span class="math inline">\(30.9 \pm 0.5\)</span> seconds to bind to the operator <span class="citation" data-cites="Hammar2014"> [<a href="#ref-Hammar2014" role="doc-biblioref">22</a>]</span>. Given the kinetic model presented in Fig. <strong>¿fig:ch3_fig02?</strong>(A) this time can be converted to the probability of not being on the repressor bound state <span class="math inline">\(P_{\text{not }R}\)</span>. This is computed as <span class="math display">\[
P_{\text{not }R} = {\tau_{\text{not }R} \over
\tau_{\text{not }R} + \tau_{R}},
\]</span> where <span class="math inline">\(\tau_{\text{not }R}\)</span> is the average time that the operator is not occupied by the repressor and <span class="math inline">\(\tau_{R}\)</span> is the average time that the repressor spends bound to the operator. Substituting the numbers from <span class="citation" data-cites="Hammar2014"> [<a href="#ref-Hammar2014" role="doc-biblioref">22</a>]</span> gives <span class="math inline">\(P_{\text{not }R} \approx 0.088\)</span>. From our model we can compute the zeroth moment <span class="math inline">\(\left\langle{m^0 p^0}\right\rangle\)</span> for each of the three promoter states. This moment is equivalent to the probability of being on each of the promoter states. Upon substitution of our inferred rate parameters we can compute <span class="math inline">\(P_{\text{not }R}\)</span> as <span class="math display">\[
P_{\text{not }R} = 1 - P_R \approx 0.046,
\]</span> where <span class="math inline">\(P_R\)</span> is the probability of the promoter being bound by the repressor. The value we obtained is within a factor of two from the one reported in <span class="citation" data-cites="Hammar2014"> [<a href="#ref-Hammar2014" role="doc-biblioref">22</a>]</span>.</p>
<h2 id="computing-moments-from-the-master-equation">Computing moments from the master equation</h2>
<p>This section will compute the moment equations for the distribution <span class="math inline">\(P(m, p)\)</span>. Without loss of generality, here we will focus on the three-state regulated promoter. The computation of the two-state promoter’s moments follows the same procedure, changing only the matrices’ definition in the master equation.</p>
<h3 id="computing-moments-of-a-distribution">Computing moments of a distribution</h3>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/moment_dynamics_system.html">following link</a> as an annotated Jupyter notebook)</p>
<p>To compute any moment of our chemical master equation, let us define a vector <span class="math display">\[
\left\langle \mathbf{m^x p^y} \right\rangle \equiv
(\left\langle m^x p^y \right\rangle_A, 
\left\langle m^x p^y \right\rangle_I, \left\langle m^x p^y \right\rangle_R)^T,
\]</span> where <span class="math inline">\(\left\langle m^x p^y \right\rangle_S\)</span> is the expected value of <span class="math inline">\(m^x p^y\)</span> in state <span class="math inline">\(S \in \{A, I, R\}\)</span> with <span class="math inline">\(x, y \in \mathbb{N}\)</span>. In other words, just as we defined the vector <span class="math inline">\(\mathbf{P}(m, p)\)</span>, here we define a vector to collect the expected value of each of the promoter states. By definition, these moments <span class="math inline">\(\left\langle m^x p^y \right\rangle_S\)</span> are computed as <span class="math display">\[
    \left\langle m^x p^y \right\rangle_S \equiv 
    \sum_{m=0}^\infty \sum_{p=0}^\infty m^x p^y P_S(m, p).
    \label{eq:mom_def}
\]</span> To simplify the notation, let <span class="math inline">\(\sum_x \equiv \sum_{x=0}^\infty\)</span>. Since we are working with a system of three ODEs, one for each state, let us define the following operation: <span class="math display">\[
\left\langle \mathbf{m^x p^y} \right\rangle =
\sum_m \sum_p m^x p^y \mathbf{P}(m, p) \equiv
  \begin{bmatrix}
    \sum_m \sum_p m^x p^y P_A(m, p)\\
    \sum_m \sum_p m^x p^y P_I(m, p)\\
    \sum_m \sum_p m^x p^y P_R(m, p)\\
  \end{bmatrix}.
\]</span></p>
<p>With this in hand, we can then apply this sum over <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> to Eq. <strong>¿eq:ch3_eq09?</strong>. For the left-hand side, we have <span class="math display">\[
    \sum_m \sum_p m^x p^y \frac{d \mathbf{P}(m, p)}{dt} = 
    \frac{d}{dt}\left[ \sum_m \sum_p m^x p^y \mathbf{P}(m, p) \right],
    \label{eq:sum_mom}
\]</span> where we made use of the linearity property of the derivative to switch the order between the sum and the derivative. Notice that the right-hand side of Eq. <span class="math inline">\(\ref{eq:sum_mom}\)</span> contains the definition of a moment from Eq. <span class="math inline">\(\ref{eq:mom_def}\)</span>. That means that we can rewrite it as <span class="math display">\[
\frac{d}{dt}\left[ \sum_m \sum_p m^x p^y \mathbf{P}(m, p) \right] = 
\frac{d \mathbf{\left\langle m^x p^y \right\rangle}}{dt}.
\]</span></p>
<p>Distributing the sum on the right-hand side of Eq. <strong>¿eq:ch3_eq09?</strong> gives <span class="math display">\[
\begin{split}
    \frac{d \mathbf{\left\langle m^x p^y \right\rangle}}{dt} &amp;=
    \mathbf{K} \sum_m \sum_p m^x p^y \mathbf{P}(m, p)\\
    &amp;- \mathbf{R}_m \sum_m \sum_p m^x p^y \mathbf{P}(m, p) + 
    \mathbf{R}_m \sum_m \sum_p m^x p^y \mathbf{P}(m-1, p)\\
    &amp;- \mathbf{\Gamma}_m \sum_m \sum_p (m) m^x p^y \mathbf{P}(m, p) + 
    \mathbf{\Gamma}_m \sum_m \sum_p (m + 1) m^x p^y \mathbf{P}(m + 1, p)\\
    &amp;- \mathbf{R}_p \sum_m \sum_p (m) m^x p^y \mathbf{P}(m, p) + 
    \mathbf{R}_p \sum_m \sum_p (m) m^x p^y \mathbf{P}(m, p - 1)\\
    &amp;- \mathbf{\Gamma}_p \sum_m \sum_p (p) m^x p^y \mathbf{P}(m, p) + 
    \mathbf{\Gamma}_p \sum_m \sum_p (p + 1) m^x p^y \mathbf{P}(m, p + 1).
  \end{split}
  \label{eq:master_sum}
\]</span></p>
<p>Let’s look at each term on the right-hand side individually. For the terms in Eq. <span class="math inline">\(\ref{eq:master_sum}\)</span> involving <span class="math inline">\(\mathbf{P}(m, p)\)</span> we can again use Eq. <span class="math inline">\(\ref{eq:mom_def}\)</span> to rewrite them in a more compact form. This means that we can rewrite the state transition term as <span class="math display">\[
\mathbf{K} \sum_m \sum_p m^x p^y \mathbf{P}(m, p) = 
\mathbf{K} \mathbf{\left\langle m^x p^y \right\rangle}.
\]</span> The mRNA production term involving <span class="math inline">\(\mathbf{P}(m, p)\)</span> can be rewritten as <span class="math display">\[
\mathbf{R}_m \sum_m \sum_p m^x p^y \mathbf{P}(m, p) = 
\mathbf{R}_m \mathbf{\left\langle m^x p^y \right\rangle}.
\]</span> In the same way, the mRNA degradation term gives <span class="math display">\[
\mathbf{\Gamma}_m \sum_m \sum_p (m) m^x p^y \mathbf{P}(m, p) = 
\mathbf{\Gamma}_m \mathbf{\left\langle{m^{(x + 1)} p^y}\right\rangle}.
\]</span> For the protein production and degradation terms involving <span class="math inline">\(\mathbf{P}(m, p)\)</span> we have <span class="math display">\[
\mathbf{R}_p \sum_m \sum_p (m) m^x p^y \mathbf{P}(m, p) = 
\mathbf{R}_p \mathbf{\left\langle{m^{(x + 1)} p^y}\right\rangle},
\]</span> and <span class="math display">\[
\mathbf{\Gamma}_p \sum_m \sum_p (p) m^x p^y \mathbf{P}(m, p) = 
\mathbf{\Gamma}_p \mathbf{\left\langle{m^x p^{(y + 1)}}\right\rangle},
\]</span> respectively.</p>
<p>For the terms of the sum in Eq. <span class="math inline">\(\ref{eq:master_sum}\)</span> involving <span class="math inline">\(\mathbf{P}(m \pm 1, p)\)</span> or <span class="math inline">\(\mathbf{P}(m, p \pm 1)\)</span> we can reindex the sum to work around this mismatch. To be more specific, let’s again look at each term case by case. For the mRNA production term involving <span class="math inline">\(\mathbf{P}(m-1, p)\)</span> we define <span class="math inline">\(m&#39; \equiv m - 1\)</span>. Using this, we write <span class="math display">\[
\mathbf{R}_m \sum_m \sum_p m^x p^y \mathbf{P}(m-1, p) =
\mathbf{R}_m \sum_{m&#39; = -1}^\infty \sum_p (m&#39; + 1)^x p^y \mathbf{P}(m&#39;, p).
\]</span> Since having negative numbers of mRNA or protein doesn’t make physical sense, we have that <span class="math inline">\(\mathbf{P}(-1, p) = 0\)</span>. Therefore we can rewrite the sum starting from 0 rather than from -1, obtaining <span class="math display">\[
\mathbf{R}_m \sum_{m&#39; = -1}^\infty \sum_p (m&#39; + 1)^x p^y \mathbf{P}(m&#39;, p) =
\mathbf{R}_m \sum_{m&#39;=0}^\infty \sum_p (m&#39; + 1)^x p^y \mathbf{P}(m&#39;, p).
    \label{eq:reindex}
\]</span> Recall that our distribution <span class="math inline">\(\mathbf{P}(m, p)\)</span> takes <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> as numerical inputs and returns a probability associated with such a molecule count. Nevertheless, <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> themselves are dimensionless quantities that serve as indices of how many molecules are in the cell. The distribution is the same whether the variable is called <span class="math inline">\(m\)</span> or <span class="math inline">\(m&#39;\)</span>; for a specific number, let’s say <span class="math inline">\(m = 5\)</span>, or <span class="math inline">\(m&#39; = 5\)</span>, <span class="math inline">\(\mathbf{P}(5, p)\)</span> will return the same result. This means that the variable name is arbitrary, and the right-hand side of Eq. <span class="math inline">\(\ref{eq:reindex}\)</span> can be written as <span class="math display">\[
\mathbf{R}_m \sum_{m&#39;=0}^\infty \sum_p (m&#39; + 1)^x p^y \mathbf{P}(m&#39;, p) =
\mathbf{R}_m \mathbf{\left\langle{(m+1)^x p^y}\right\rangle},
\]</span> since the left-hand side corresponds to the definition of a moment.</p>
<p>For the mRNA degradation term involving <span class="math inline">\(\mathbf{P}(m + 1, p)\)</span> we follow a similar procedure in which we define <span class="math inline">\(m&#39; = m + 1\)</span> to obtain <span class="math display">\[
\mathbf{\Gamma}_m \sum_m \sum_p (m + 1) m^x p^y \mathbf{P}(m + 1, p) =
\mathbf{\Gamma}_m \sum_{m&#39; = 1}^\infty \sum_p m&#39; 
(m&#39; - 1)^x p^y \mathbf{P}(m&#39;, p).
\]</span> Since the term on the right-hand side of the equation is multiplied by <span class="math inline">\(m&#39;\)</span>, starting the sum over <span class="math inline">\(m&#39;\)</span> from 0 rather than from 1 will not affect the result since this factor will not contribute to the total sum. Nevertheless, this is useful since our definition of a moment from Eq. <span class="math inline">\(\ref{eq:mom_def}\)</span> requires the sum to start at zero. This means that we can rewrite this term as <span class="math display">\[
\mathbf{\Gamma}_m \sum_{m&#39; = 1}^\infty m&#39; \sum_p 
(m&#39; - 1)^x p^y \mathbf{P}(m&#39;, p) =
\mathbf{\Gamma}_m \sum_{m&#39; = 0}^\infty m&#39; \sum_p 
(m&#39; - 1)^x p^y \mathbf{P}(m&#39;, p).
\]</span> Here again, we can change the arbitrary label <span class="math inline">\(m&#39;\)</span> back to <span class="math inline">\(m\)</span>, obtaining <span class="math display">\[
\mathbf{\Gamma}_m \sum_{m&#39; = 0}^\infty m&#39; \sum_p 
(m&#39; - 1)^x p^y \mathbf{P}(m&#39;, p) =
\mathbf{\Gamma}_m \mathbf{\left\langle{m (m - 1)^x p^y}\right\rangle}.
\]</span></p>
<p>The protein production term involving <span class="math inline">\(\mathbf{P}(m, p - 1)\)</span> can be reindexed by defining <span class="math inline">\(p&#39; \equiv p - 1\)</span>. This gives <span class="math display">\[
\mathbf{R}_p \sum_m \sum_p (m) m^x p^y \mathbf{P}(m, p - 1) =
\mathbf{R}_p \sum_m \sum_{p&#39;=-1}^\infty 
m^{(x + 1)} (p + 1)^y \mathbf{P}(m, p&#39;).
\]</span> We again use the fact that negative molecule copy numbers are assigned with probability zero to begin the sum from 0 rather than -1 and the arbitrary nature of the label <span class="math inline">\(p&#39;\)</span> to write <span class="math display">\[
\mathbf{R}_p \sum_m \sum_{p&#39;=0}^\infty m^{(x + 1)} (p + 1)^y \mathbf{P}(m, p&#39;) =
\mathbf{R}_p \mathbf{\left\langle{m^{(x + 1)} (p + 1)^y}\right\rangle}.
\]</span> Finally, we take care of the protein degradation term involving <span class="math inline">\(\mathbf{P}(m, p + 1)\)</span>. As before, we define <span class="math inline">\(p&#39; = p + 1\)</span> and substitute this to obtain <span class="math display">\[
\mathbf{\Gamma}_p \sum_m \sum_p (p + 1) m^x p^y \mathbf{P}(m, p + 1) =
\mathbf{\Gamma}_p \sum_m \sum_{p&#39;=1}^\infty 
(p&#39;) m^x (p&#39; - 1)^y \mathbf{P}(m, p&#39;).
\]</span> Just as with the mRNA degradation term, having a term <span class="math inline">\(p&#39;\)</span> inside the sum allows us to start the sum over <span class="math inline">\(p&#39;\)</span> from 0 rather than 1. We can therefore write <span class="math display">\[
\mathbf{\Gamma}_p \sum_m \sum_{p&#39;=0}^\infty 
(p&#39;) m^x (p&#39; - 1)^y \mathbf{P}(m, p&#39;) =
\mathbf{\Gamma}_p \mathbf{\left\langle{m^x p (p - 1)^y}\right\rangle}.
\]</span></p>
<p>Putting all these terms together, we can write the general moment ODE. This is of the form <span class="math display">\[
\begin{split}
    \frac{d\mathbf{\left\langle m^x p^y \right\rangle}}{dt} &amp;=
    \mathbf{K} \mathbf{\left\langle m^x p^y \right\rangle}
    \text{  (promoter state transition)}\\
    &amp;- \mathbf{R}_m \mathbf{\left\langle m^x p^y \right\rangle} +
    \mathbf{R}_m \mathbf{\left\langle{(m+1)^x p^y}\right\rangle}
    \text{  (mRNA production)}\\
    &amp;- \mathbf{\Gamma}_m \mathbf{\left\langle{m^{(x + 1)} p^y}\right\rangle} + 
    \mathbf{\Gamma}_m \mathbf{\left\langle{m (m - 1)^x p^y}\right\rangle}
    \text{  (mRNA degradation)}\\
    &amp;- \mathbf{R}_p \mathbf{\left\langle{m^{(x + 1)} p^y}\right\rangle} + 
    \mathbf{R}_p \mathbf{\left\langle{m^{(x + 1)} (p + 1)^y}\right\rangle}
    \text{  (protein production)}\\
    &amp;- \mathbf{\Gamma}_p \mathbf{\left\langle{m^x p^{(y + 1)}}\right\rangle} + 
    \mathbf{\Gamma}_p \mathbf{\left\langle{m^x p (p - 1)^y}\right\rangle}
    \text{  (protein degradation)}.
  \end{split}
  \label{eq:mom_ode}
\]</span></p>
<h3 id="moment-closure-of-the-simple-repression-distribution">Moment closure of the simple-repression distribution</h3>
<p>A very interesting and useful feature of Eq. <span class="math inline">\(\ref{eq:mom_ode}\)</span> is that for a given value of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> the moment <span class="math inline">\(\mathbf{\left\langle m^x p^y \right\rangle}\)</span> is only a function of lower moments. Specifically <span class="math inline">\(\mathbf{\left\langle m^x p^y \right\rangle}\)</span> is a function of moments <span class="math inline">\(\mathbf{\left\langle{m^{x&#39;} p^{y&#39;}}\right\rangle}\)</span> that satisfy two conditions:</p>
<p><span class="math display">\[
    \begin{split}
        &amp;1) y&#39; \leq y,\\
    &amp;2) x&#39; + y&#39; \leq x + y.
    \end{split}
    \label{eq:mom_conditions}
\]</span></p>
<p>To prove this we rewrite Eq. <span class="math inline">\(\ref{eq:mom_ode}\)</span> as <span class="math display">\[
\begin{split}
    \frac{d\mathbf{\left\langle m^x p^y \right\rangle}}{dt} &amp;=
    \mathbf{K} \mathbf{\left\langle m^x p^y \right\rangle}\\
    &amp;+ \mathbf{R}_m 
    \mathbf{\left\langle{p^y \left[ (m + 1)^x -m^x \right]}\right\rangle}\\
    &amp;+ \mathbf{\Gamma}_m 
    \mathbf{\left\langle{m p^y \left[ (m - 1)^x - m^x \right]}\right\rangle}\\
    &amp;+ \mathbf{R}_p 
    \mathbf{\left\langle{m^{(x + 1)}
    \left[ (p + 1)^y - p^y \right]}\right\rangle}\\
    &amp;+ \mathbf{\Gamma}_p 
    \mathbf{\left\langle{m^x p \left[ (p - 1)^y - p^y \right]}\right\rangle},
\end{split}
    \label{eq:mom_ode_factorized}
\]</span> where the factorization is valid given the linearity of expected values. The objective is to find the highest moment for each term once the relevant binomial, such as <span class="math inline">\((m-1)^x\)</span>, is expanded. Take, for example, a simple case in which we want to find the second moment of the mRNA distribution. We then set <span class="math inline">\(x = 2\)</span> and <span class="math inline">\(y = 0\)</span>. Eq. <span class="math inline">\(\ref{eq:mom_ode_factorized}\)</span> then becomes <span class="math display">\[
\begin{split}
    \frac{\mathbf{\left\langle{m^2 p^0}\right\rangle}}{dt} &amp;=
    \mathbf{K} \mathbf{\left\langle{m^2 p^0}\right\rangle}\\
    &amp;+ \mathbf{R}_m 
    \mathbf{\left\langle{p^0 \left[ (m + 1)^2 - m^2 \right]}\right\rangle}\\
    &amp;+ \mathbf{\Gamma}_m
    \mathbf{\left\langle{m p^0 \left[ (m - 1)^2 - m^2 \right]}\right\rangle}\\
    &amp;+ \mathbf{R}_p 
    \mathbf{\left\langle{m^{(2 + 1)} 
    \left[ (p + 1)^0 - p^0 \right]}\right\rangle}\\
    &amp;+ \mathbf{\Gamma}_p 
    \mathbf{\left\langle{m^2 p \left[ (p - 1)^0 - p^0 \right]}\right\rangle}.
\end{split}
\]</span> Simplifying this equation gives <span class="math display">\[
    \frac{d \mathbf{\left\langle{m^2}\right\rangle}}{dt} =
    \mathbf{K} 
    \mathbf{\left\langle{m^2}\right\rangle}
    + \mathbf{R}_m 
    \mathbf{\left\langle{\left[ 2m + 1 \right]}\right\rangle}
    + \mathbf{\Gamma}_m 
    \mathbf{\left\langle{\left[- 2m^2 + m \right]}\right\rangle}.
    \label{eq:second_mom_mRNA}
\]</span></p>
<p>Eq. <span class="math inline">\(\ref{eq:second_mom_mRNA}\)</span> satisfies both of our conditions. Since we set <span class="math inline">\(y\)</span> to be zero, none of the terms depend on any moment that involves the protein number. Therefore <span class="math inline">\(y&#39; \leq y\)</span> is satisfied. Also, the highest moment in Eq. <span class="math inline">\(\ref{eq:second_mom_mRNA}\)</span> also satisfies <span class="math inline">\(x&#39; + y&#39; \leq x + y\)</span> since the second moment of mRNA doesn’t depend on any moment higher than <span class="math inline">\(\mathbf{\left\langle{m^2}\right\rangle}\)</span>. To demonstrate that this is true for any <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, we now rewrite Eq. <span class="math inline">\(\ref{eq:mom_ode_factorized}\)</span>, making use of the binomial expansion <span class="math display">\[
(z \pm 1)^n = \sum_{k=0}^n {n \choose k} (\pm 1)^{k} z^{n-k}.
\]</span> Just as before, let’s look at each term individually. For the mRNA production term we have <span class="math display">\[
\mathbf{R}_m 
\mathbf{\left\langle{p^y \left[ (m + 1)^x -m^x \right]}\right\rangle} =
\mathbf{R}_m 
\mathbf{\left\langle{p^y 
\left[ \sum_{k=0}^x {x \choose k} m^{x-k} - m^x \right]}\right\rangle}.
\]</span> When <span class="math inline">\(k = 0\)</span>, the term inside the sum on the right-hand side cancels with the other <span class="math inline">\(m^x\)</span> so that we can simplify to <span class="math display">\[
\mathbf{R}_m 
\mathbf{\left\langle{p^y \left[ (m + 1)^x -m^x \right]}\right\rangle} =
\mathbf{R}_m 
\mathbf{\left\langle{p^y 
\left[ \sum_{k=1}^x {x \choose k} m^{x-k} \right]}\right\rangle}.
\]</span> Once the sum is expanded we can see that the highest moment in this sum is given by <span class="math inline">\(\mathbf{\left\langle{m^{(x-1)} p^y}\right\rangle}\)</span> which satisfies both of the conditions on Eq. <span class="math inline">\(\ref{eq:mom_conditions}\)</span>.</p>
<p>For the mRNA degradation term, we similarly have <span class="math display">\[
\mathbf{\Gamma}_m 
\mathbf{\left\langle{m p^y \left[ (m - 1)^x - m^x \right]}\right\rangle} =
\mathbf{\Gamma}_m 
\mathbf{\left\langle{m p^y 
\left[ \sum_{k=0}^x {x \choose k}(-1)^k m^{x-k} - m^x \right]}\right\rangle}.
\]</span> Simplifying terms we obtain <span class="math display">\[
\mathbf{\Gamma}_m 
\mathbf{\left\langle{m p^y \left[ \sum_{k=0}^x {x \choose k}(-1)^k m^{x-k} -
m^x \right]}\right\rangle} =
\mathbf{\Gamma}_m 
\mathbf{\left\langle{p^y 
\left[ \sum_{k=1}^x {x \choose k}(-1)^k m^{x+1-k} \right]}\right\rangle}.
\]</span> The largest moment in this case is <span class="math inline">\(\mathbf{\left\langle{m^x p^y}\right\rangle}\)</span>, which again satisfies the conditions on Eq. <span class="math inline">\(\ref{eq:mom_conditions}\)</span>.</p>
<p>The protein production term gives <span class="math display">\[
\mathbf{R}_p 
\mathbf{\left\langle{m^{(x + 1)} \left[ (p + 1)^y - p^y \right]}\right\rangle} =
\mathbf{R}_p \mathbf{\left\langle{m^{(x + 1)}
\left[ \sum_{k=0}^y {y \choose k} (-1)^k p^{y-k} - p^y \right]}\right\rangle}.
\]</span> Upon simplification, we obtain <span class="math display">\[
\mathbf{R}_p 
\mathbf{\left\langle{m^{(x + 1)} 
\left[ \sum_{k=0}^y {y \choose k} (-1)^k p^{y-k} - p^y \right]}\right\rangle} =
\mathbf{R}_p 
\mathbf{\left\langle{m^{(x + 1)} 
\left[ \sum_{k=1}^y {y \choose k}(-1)^k p^{y-k} \right]}\right\rangle}.
\]</span> Here the largest moment is given by <span class="math inline">\(\mathbf{\left\langle{m^{x+1} p^{y-1}}\right\rangle}\)</span>, that again satisfies both of our conditions. For the last term, for protein degradation, we have <span class="math display">\[
\mathbf{R}_p 
\mathbf{\left\langle{m^{(x + 1)} \left[ (p + 1)^y - p^y \right]}\right\rangle} =
\mathbf{R}_p 
\mathbf{\left\langle{m^{(x + 1)} \left[ \sum_{k=1}^y {y \choose k} (-1^k) p^{y - k}
  \right]}\right\rangle}.
\]</span> The largest moment involved in this term is therefore <span class="math inline">\(\mathbf{\left\langle{m^x p^{y-1}}\right\rangle}\)</span>. With this, we show that the four terms involved in our general moment equation depend only on lower moments that satisfy XXX.</p>
<p>As a reminder, we showed in this section that the kinetic model introduced in Fig. <strong>¿fig:ch3_fig02?</strong>(A) has no moment-closure problem. In other words, moments of the joint mRNA and protein distribution can be computed from knowledge of lower moments. This allows us to cleanly integrate the distribution moment dynamics as cells progress through the cell cycle.</p>
<h3 id="computing-single-promoter-steady-state-moments">Computing single promoter steady-state moments</h3>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/chemical_master_steady_state_moments_general.html">following link</a> as an annotated Jupyter notebook)</p>
<p>As discussed in XXX, one of the main factors contributing to cell-to-cell variability in gene expression is the change in gene copy number during the cell cycle as cells replicate their genome before cell division. Our minimal model accounts for this variability by considering the time trajectory of the distribution moments given by Eq. <span class="math inline">\(\ref{eq:mom_ode_factorized}\)</span>. These predictions will be contrasted with the predictions from a kinetic model that doesn’t account for gene copy numbers changes during the cell cycle in XXX.</p>
<p>Suppose we do not account for the change in gene copy number during the cell cycle or the partition of proteins during division. In that case, the dynamics of the moments of the distribution described in this section will reach a steady state. To compute the kinetic model’s steady-state moments with a single gene across the cell cycle, we use the moment closure property of our master equation. By equating Eq. <span class="math inline">\(\ref{eq:mom_ode_factorized}\)</span> to zero for a given <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, we can solve the resulting linear system and obtain a solution for <span class="math inline">\(\mathbf{\left\langle m^x p^y \right\rangle}\)</span> at steady state as a function of moments <span class="math inline">\(\mathbf{\left\langle{m^{x&#39;} p^{y&#39;}}\right\rangle}\)</span> that satisfy Eq. <span class="math inline">\(\ref{eq:mom_conditions}\)</span>. Then, by solving for the zero<span class="math inline">\(^\text{th}\)</span> moment <span class="math inline">\(\mathbf{\left\langle{m^0 p^0}\right\rangle}\)</span> subject to the constraint that the probability of the promoter being in any state should add up to one, we can substitute back all of the solutions in terms of moments <span class="math inline">\(\mathbf{\left\langle{m^{x&#39;} p^{y&#39;}}\right\rangle}\)</span> with solutions in terms of the rates shown in Fig. <strong>¿fig:ch3_fig02?</strong>. In other words, through an iterative process, we can get at the value of any moment of the distribution. We start by solving for the zero<span class="math inline">\(^\text{th}\)</span> moment. Since all higher moments depend on lower moments, we can use the solution of the zero<span class="math inline">\(^\text{th}\)</span> moment to compute the first mRNA moment. This solution is then used for higher moments in a hierarchical iterative process.</p>
<h2 id="accounting-for-the-variability-in-gene-copy-number-during-the-cell-cycle">Accounting for the variability in gene copy number during the cell cycle</h2>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu/chann_cap/src/theory/html/moment_dynamics_cell_division.html">following link</a> as an annotated Jupyter notebook)</p>
<p>When growing in rich media, bacteria can double every <span class="math inline">\(\approx\)</span> 20 minutes. With two replication forks, each traveling at <span class="math inline">\(\approx\)</span> 1000 bp per second, and a genome of <span class="math inline">\(\approx\)</span> 5 Mbp for <em>E. coli</em> <span class="citation" data-cites="Moran2010"> [<a href="#ref-Moran2010" role="doc-biblioref">23</a>]</span>, a cell would need <span class="math inline">\(\approx\)</span> 40 minutes to replicate its genome. The apparent paradox of growth rates faster than one division per 40 minutes is solved because cells have multiple replisomes, i.e., molecular machines that replicate the genome running in parallel. Cells can have up to 8 copies of the genome being replicated simultaneously, depending on the growth rate <span class="citation" data-cites="Bremer1996"> [<a href="#ref-Bremer1996" role="doc-biblioref">14</a>]</span>.</p>
<p>This observation implies that during the cell cycle, gene copy number varies. This variation depends on the growth rate and the relative position of the gene with respect to the replication origin, having genes close to the replication origin spending more time with multiple copies compare to genes closer to the replication termination site. This change in gene dosage has a direct effect on the cell-to-cell variability in gene expression <span class="citation" data-cites="Jones2014a Peterson2015"> [<a href="#ref-Jones2014a" role="doc-biblioref">4</a>,<a href="#ref-Peterson2015" role="doc-biblioref">13</a>]</span>.</p>
<h3 id="numerical-integration-of-moment-equations">Numerical integration of moment equations</h3>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/moment_dynamics_cell_division.html">following link</a> as an annotated Jupyter notebook)</p>
<p>For our specific locus (<em>galK</em>) and a doubling time of <span class="math inline">\(\approx\)</span> 60 min for our experimental conditions, cells have on average 1.66 copies of the reporter gene during the cell cycle <span class="citation" data-cites="Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">4</a>]</span>. What this means is that cells spend 60% of the time having one copy of the gene and 40% of the time with two copies. To account for this variability in gene copy number across the cell cycle, we numerically integrate the moment equations derived in for a time <span class="math inline">\(t = [0, t_s]\)</span> with an mRNA production rate <span class="math inline">\(r_m\)</span>, where <span class="math inline">\(t_s\)</span> is the time point at which the replication fork reaches our specific locus. For the remaining time before the cell division <span class="math inline">\(t = [t_s, t_d]\)</span> that the cell spends with two promoters, we assume that the only parameter that changes is the mRNA production rate from <span class="math inline">\(r_m\)</span> to <span class="math inline">\(2 r_m\)</span>. This simplifying assumption ignores potential changes in protein translation rate <span class="math inline">\(r_p\)</span> or changes in the repressor copy number that would be reflected in changes on the repressor on rate <span class="math inline">\(k^{(r)}_{\text{on}}\)</span>.</p>
<h3 id="computing-distribution-moments-after-cell-division">Computing distribution moments after cell division</h3>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/binomial_moments.html">following link</a> as an annotated Jupyter notebook)</p>
<p>We have already solved a general form for the dynamics of the moments of the distribution, i.e., we wrote differential equations for the moments <span class="math inline">\(\frac{d\left\langle{m^x p^y}\right\rangle}{dt}\)</span>. Given that we know all parameters for our model, we can numerically integrate these equations to compute how the moments of the distribution evolve as cells progress through their cell cycle. Once the cell reaches a time <span class="math inline">\(t_d\)</span> when is going to divide the mRNA and proteins that we are interested in undergo a binomial partitioning between the two daughter cells. In other words, each molecule flips a coin and decides whether to go to either daughter. The question then becomes given that we have a value for the moment <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_d}\)</span> at a time before the cell division, what would the value of this moment be after the cell division takes place <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_o}\)</span>?</p>
<p>The probability distribution of mRNA and protein after the cell division <span class="math inline">\(P_{t_o}(m, p)\)</span> must satisfy <span class="math display">\[
    P_{t_o}(m, p) = \sum_{m&#39;=m}^\infty \sum_{p&#39;=p}^\infty 
    P(m, p \mid m&#39;, p&#39;) P_{t_d}(m&#39;, p&#39;),
    \label{eq_dist_post_div}
\]</span> where we are summing over all the possibilities of having <span class="math inline">\(m&#39;\)</span> mRNA and <span class="math inline">\(p&#39;\)</span> proteins before cell division. Note that the sums start at <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span>; this is because for a cell to have these copy numbers before cell division, it is a requirement that the mother cell had at least such copy number since we are not assuming that there is any production at the instantaneous cell division time. Since we assume that the partition of mRNA is independent of the partition of protein, the conditional probability <span class="math inline">\(P(m, p \mid m&#39;, p&#39;)\)</span> is given by a product of two binomial distributions, one for the mRNA and one for the protein, i.e. <span class="math display">\[
P(m, p \mid m&#39;, p&#39;) = {m&#39; \choose m} \left( \frac{1}{2} \right)^{m&#39;} \cdot
                      {p&#39; \choose p} \left( \frac{1}{2} \right)^{p&#39;}.
    \label{eq_binom_prod}
\]</span> Because of this product of binomial probabilities are allowed to extend the sum from Eq. <span class="math inline">\(\ref{eq_dist_post_div}\)</span> to start at <span class="math inline">\(m&#39;=0\)</span> and <span class="math inline">\(p&#39;=0\)</span> as <span class="math display">\[
P_{t_o}(m, p) = \sum_{m&#39;=0}^\infty \sum_{p&#39;=0}^\infty 
                  P(m, p \mid m&#39;, p&#39;) P_{t_d}(m&#39;, p&#39;),
\]</span> since the product of the binomial distributions in Eq. <span class="math inline">\(\ref{eq_binom_prod}\)</span> is zero for all <span class="math inline">\(m&#39; &lt; m\)</span> and/or <span class="math inline">\(p&#39; &lt; 0\)</span>. So from now on in this section we will assume that a sum of the form <span class="math inline">\(\sum_x \equiv \sum_{x=0}^\infty\)</span> to simplify notation.</p>
<p>We can then compute the distribution moments after the cell division <span class="math inline">\(\left\langle{m^x p^y}\right\rangle_{t_o}\)</span> as <span class="math display">\[
\left\langle m^x p^y \right\rangle_{t_o} = 
\sum_m \sum_p m^x p^y P_{t_o}(m, p),
\]</span> for all <span class="math inline">\(x, y \in \mathbb{N}\)</span>. Substituting Eq. <span class="math inline">\(\ref{eq_dist_post_div}\)</span> results in <span class="math display">\[
\left\langle m^x p^y \right\rangle_{t_o} = \sum_m \sum_p m^x p^y
\sum_{m&#39;} \sum_{p&#39;} P(m, p \mid m&#39;, p&#39;) P_{t_d}(m&#39;, p&#39;).
\]</span> We can rearrange the sums to be <span class="math display">\[
\left\langle m^x p^y \right\rangle_{t_o} = \sum_{m&#39;} \sum_{p&#39;} P_{t_d}(m&#39;, p&#39;)
                     \sum_m \sum_p m^x p^y P(m, p \mid m&#39;, p&#39;).
\]</span> The fact that Eq. <span class="math inline">\(\ref{eq_binom_prod}\)</span> is the product of two independent events allows us to rewrite the joint probability <span class="math inline">\(P(m, p \mid m&#39;, p&#39;)\)</span> as <span class="math display">\[
P(m, p \mid m&#39;, p&#39;) = P(m \mid m&#39;) \cdot P(p \mid p&#39;).
\]</span> With this, we can then write the moment <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_o}\)</span> as <span class="math display">\[
\left\langle m^x p^y \right\rangle_{t_o} = \sum_{m&#39;} \sum_{p&#39;} P_{t_d}(m&#39;, p&#39;)
                     \sum_m  m^x  P(m \mid m&#39;)
                     \sum_p p^y P(p \mid p&#39;).
\]</span> Notice that both terms summing over <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> are the conditional expected values, i.e. <span class="math display">\[
\sum_z  z^x  P(z \mid z&#39;) \equiv \left\langle{z^x \mid z&#39;}\right\rangle, \; 
{\text{ for } z\in \{m, p \}}.
\]</span> These conditional expected values are the expected values of a binomial random variable <span class="math inline">\(z \sim \text{Bin}(z&#39;, 1/2)\)</span>, which can be easily computed, as we will show later in this section. We then rewrite the expected values after the cell division in terms of these moments of a binomial distribution <span class="math display">\[
    \left\langle m^x p^y \right\rangle_{t_o} = \sum_{m&#39;} \sum_{p&#39;} 
    \left\langle{m^x \mid m&#39;}\right\rangle 
    \left\langle{p^y \mid p&#39;}\right\rangle
    P_{t_d}(m&#39;, p&#39;).
    \label{eq_general_binom_mom}
\]</span></p>
<p>To see how this general formula for the moments after the cell division works, let’s compute the mean protein per cell after the cell division <span class="math inline">\(\left\langle{p}\right\rangle_{t_o}\)</span>. That is setting <span class="math inline">\(x = 0\)</span>, and <span class="math inline">\(y = 1\)</span>. This results in <span class="math display">\[
\left\langle{p}\right\rangle_{t_o} = 
\sum_{m&#39;} \sum_{p&#39;} 
\left\langle{m^0 \mid m&#39;}\right\rangle \left\langle{p \mid p&#39;}\right\rangle
P_{t_d}(m&#39;, p&#39;).
\]</span> The zeroth moment <span class="math inline">\(\left\langle{m^0 \mid m&#39;}\right\rangle\)</span> by definition must be one since we have <span class="math display">\[
\left\langle{m^0 \mid m&#39;}\right\rangle = 
\sum_m m^0 P(m \mid m&#39;) = 
\sum_m P(m \mid m&#39;) = 1,
\]</span> since the probability distribution must be normalized. This leaves us then with <span class="math display">\[
\left\langle{p}\right\rangle_{t_o} = 
\sum_{m&#39;} \sum_{p&#39;} P_{t_d}(m&#39;, p&#39;) \left\langle p \mid p&#39; \right\rangle.
\]</span> If we take the sum over <span class="math inline">\(m&#39;\)</span> we simply compute the marginal probability distribution <span class="math inline">\(\sum_{m&#39;} P_{t_d}(m&#39;, p&#39;) = P_{t_d}(p&#39;)\)</span>, then we have <span class="math display">\[
\left\langle p \right\rangle_{t_o} = 
\sum_{p&#39;} \left\langle p \mid p&#39; \right\rangle P_{t_d}(p&#39;).
\]</span> For the particular case of the first moment of the binomial distribution with parameters <span class="math inline">\(p&#39;\)</span> and <span class="math inline">\(1/2\)</span> we know that <span class="math display">\[
\left\langle p \mid p&#39; \right\rangle = \frac{p&#39;}{2}.
\]</span> Therefore the moment after division is equal to <span class="math display">\[
\left\langle p \right\rangle_{t_o} = 
\sum_{p&#39;} \frac{p&#39;}{2} P_{t_d}(p&#39;)
= \frac{1}{2} \sum_{p&#39;} p&#39; P_{t_d}(p&#39;).
\]</span> Notice that this is just 1/2 of the expected value of <span class="math inline">\(p&#39;\)</span> averaging over the distribution before cell division, i.e. <span class="math display">\[
\left\langle p \right\rangle_{t_o} = 
\frac{\left\langle{p&#39;}\right\rangle_{t_d}}{2},
\]</span> where <span class="math inline">\(\left\langle{\cdot}\right\rangle_{t_d}\)</span> highlights that is the moment of the distribution prior to the cell division. This result makes perfect sense. What this is saying is that the mean protein copy number right after the cell divides is half of the mean protein copy number just before the cell division. That is exactly we would expect. So in principle to know the first moment of either the mRNA distribution <span class="math inline">\(\langle m \rangle_{t_o}\)</span> or the protein distribution <span class="math inline">\(\langle m \rangle_{t_o}\)</span> right after cell division it suffices to multiply the moments before the cell division <span class="math inline">\(\langle m \rangle_{t_d}\)</span> or <span class="math inline">\(\left\langle p \right\rangle_{t_d}\)</span> by 1/2. Let’s now explore how this generalizes to any other moment <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_o}\)</span>.</p>
<h4 id="computing-the-moments-of-a-binomial-distribution">Computing the moments of a binomial distribution</h4>
<p>Last section’s result was dependent on us knowing the functional form of the first moment of the binomial distribution. For higher moments, we need some systematic way to compute such moments. Luckily for us, we can do so by using the so-called moment generating function (MGF). The MGF of a random variable <span class="math inline">\(X\)</span> is defined as <span class="math display">\[
M_X(t) = \left\langle{e^{tX}}\right\rangle,
\]</span> where <span class="math inline">\(t\)</span> is a dummy variable. Once we know the MGF we can obtain any moment of the distribution by simply computing <span class="math display">\[
    \left\langle{X^n}\right\rangle = 
    \left. \frac{d^n}{dt^n} M_X(t) \right\vert_{t=0},
    \label{eq_mgf_def}
\]</span> i.e., taking the <span class="math inline">\(n\)</span>-th derivative of the MGF returns the <span class="math inline">\(n\)</span>-th moment of the distribution. For the particular case of the binomial distribution <span class="math inline">\(X \sim \text{Bin}(N, q)\)</span> it can be shown that the MGF is of the form <span class="math display">\[
M_X(t) = \left[ (1 - q) + qe^{t} \right]^N.
\]</span> As an example, let’s compute the first moment of this binomially distributed variable. For this, the first derivative of the MGF results in <span class="math display">\[
\frac{d M_X(t)}{dt} = N [(1 - q) + qe^t]^{N - 1} q e^t.
\]</span> We just need to follow Eq. <span class="math inline">\(\ref{eq_mgf_def}\)</span> and set <span class="math inline">\(t = 0\)</span> to obtain the first moment <span class="math display">\[
    \left. \frac{d M_X(t)}{dt} \right\vert_{t=0} = N q,
    \label{eq_mgf_mean}
\]</span> which is precisely the expected value of a binomially distributed random variable.</p>
<p>So according to Eq. <span class="math inline">\(\ref{eq_general_binom_mom}\)</span> to compute any moment <span class="math inline">\(\left\langle{m^x p^y}\right\rangle\)</span> after cell division we can just take the <span class="math inline">\(x\)</span>-th derivative and the <span class="math inline">\(y\)</span>-th derivative of the binomial MGF to obtain <span class="math inline">\(\left\langle{m^x \mid m&#39;}\right\rangle\)</span> and <span class="math inline">\(\left\langle{p^y \mid p&#39;}\right\rangle\)</span>, respectively, and take the expected value of the result. Let’s follow on detail the specific case for the moment <span class="math inline">\(\left\langle{m p}\right\rangle\)</span>. When computing the moment after cell division <span class="math inline">\(\left\langle{mp}\right\rangle_{t_o}\)</span> which is of the form <span class="math display">\[
\left\langle{mp}\right\rangle_{t_o} = 
\sum_{m&#39;} \sum{p&#39;} 
\left\langle{m \mid m&#39;}\right\rangle \left\langle p \mid p&#39; \right\rangle 
P_{t_d}(m&#39;, p&#39;),
\]</span> the product <span class="math inline">\(\left\langle{m \mid m&#39;}\right\rangle \left\langle p \mid p&#39; \right\rangle\)</span> is then <span class="math display">\[
\left\langle{m \mid m&#39;}\right\rangle \left\langle p \mid p&#39; \right\rangle =
\frac{m&#39;}{2} \cdot \frac{p&#39;}{2},
\]</span> where we used the result in Eq. <span class="math inline">\(\ref{eq_mgf_mean}\)</span>, substituting <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> for <span class="math inline">\(X\)</span>, respectively, and <span class="math inline">\(q\)</span> for 1/2. Substituting this result into the moment gives <span class="math display">\[
\left\langle{mp}\right\rangle_{t_o} = 
\sum_{m&#39;} \sum_{p&#39;} \frac{m&#39; p&#39;}{4} P_{t_d}(m&#39;, p&#39;) 
= \frac{\left\langle{m&#39; p&#39;}\right\rangle_{t_d}}{4}.
\]</span> Therefore to compute the moment after cell division <span class="math inline">\(\left\langle{mp}\right\rangle_{t_o}\)</span> we simply have to divide by 4 the corresponding equivalent moment before the cell division.</p>
<p>Not all moments after cell division depend only on the equivalent moment before cell division. For example if we compute the third moment of the protein distribution <span class="math inline">\(\left\langle{p^3}\right\rangle_{t_o}\)</span>, we find <span class="math display">\[
\left\langle{p^3}\right\rangle_{t_o} = 
\frac{\left\langle{p^3}\right\rangle_{t_d}}{8} + 
\frac{3 \left\langle{p^2}\right\rangle_{t_d}}{8}.
\]</span> For this particular case, the third moment of the protein distribution depends on the third moment and the second moment before the cell division. In general all moments after cell division <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_o}\)</span> linearly depend on moments before cell division. Furthermore, there is “moment closure” for this specific case in the sense that all moments after cell division depend on lower moments before cell division. To generalize these results to all the moments computed in this work, let us then define a vector to collect all moments before the cell division up the <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_d}\)</span> moment, i.e. <span class="math display">\[
\mathbf{\left\langle m^x p^y \right\rangle}_{t_d} = \left(
\left\langle{m^0 p^0}\right\rangle_{t_d}, \left\langle{m^1}\right\rangle_{t_d},
\ldots , \left\langle m^x p^y \right\rangle_{t_d}
\right).
\]</span> Then any moment after cell division <span class="math inline">\(\left\langle{m^{x&#39;} p^{y&#39;}}\right\rangle_{t_o}\)</span> for <span class="math inline">\(x&#39; \leq x\)</span> and <span class="math inline">\(y&#39; \leq y\)</span> can be computed as <span class="math display">\[
\left\langle{m^{x&#39;} p^{y&#39;}}\right\rangle_{t_o} = 
\mathbf{z}_{x&#39;y&#39;} \cdot \mathbf{\left\langle m^x p^y \right\rangle}_{t_d},
\]</span> where we define the vector <span class="math inline">\(\mathbf{z}_{x&#39;y&#39;}\)</span> as the vector containing all the coefficients that we obtain with the product of the two binomial distributions. For example for the case of the third protein moment <span class="math inline">\(\left\langle{p^3}\right\rangle_{t_o}\)</span> the vector <span class="math inline">\(\mathbf{z}_{x&#39;y&#39;}\)</span> would have zeros for all entries except for the corresponding entry for <span class="math inline">\(\left\langle{p^2}\right\rangle_{t_d}\)</span> and for <span class="math inline">\(\left\langle{p^3}\right\rangle_{t_d}\)</span>, where it would have <span class="math inline">\(3/8\)</span> and <span class="math inline">\(1/8\)</span> accordingly.</p>
<p>If we want then to compute all the moments after the cell division up to <span class="math inline">\(\left\langle m^x p^y \right\rangle_{t_o}\)</span> let us define an equivalent vector <span class="math display">\[
\mathbf{\left\langle m^x p^y \right\rangle}_{t_o} = \left(
\left\langle{m^0 p^0}\right\rangle_{t_o}, \left\langle{m^1}\right\rangle_{t_o}, 
\ldots , \left\langle m^x p^y \right\rangle_{t_o}
\right).
\]</span> Then we need to build a square matrix <span class="math inline">\(\mathbf{Z}\)</span> such that each row of the matrix contains the corresponding vector <span class="math inline">\(\mathbf{z}_{x&#39; y&#39;}\)</span> for each of the moments. Having this matrix, we would simply compute the moments after the cell division as <span class="math display">\[
\mathbf{\left\langle{m^x p^x}\right\rangle}_{t_o} = 
\mathbf{Z} \cdot \mathbf{\left\langle{m^x p^x}\right\rangle}_{t_d}.
\]</span> In other words, the matrix <span class="math inline">\(\mathbf{Z}\)</span> will contain all the coefficients that we need to multiply by the moments before the cell division to obtain the moments after cell division. The matrix <span class="math inline">\(\mathbf{Z}\)</span> was then generated automatically using Python’s analytical math library <code>sympy</code> <span class="citation" data-cites="sympy"> [<a href="#ref-sympy" role="doc-biblioref">24</a>]</span>.</p>
<p>Fig. 8 (adapted from Fig. <strong>¿fig:ch3_fig03?</strong>(B)) shows how the first moment of both mRNA and protein changes over several cell cycles. The mRNA quickly relaxes to the steady-state corresponding to the parameters for both a single and two promoter copies. This is expected since the parameters for the mRNA production was determined in the first place under this assumption (See ). We note that there is no apparent delay before reaching steady-state of the mean mRNA count after the cell divides. This is because the mean mRNA count for the two promoters copies state is precisely twice the expected mRNA count for the single promoter state (See XXX). Therefore once the mean mRNA count is halved after the cell division, it is already at the steady-state value for the single promoter case. On the other hand, given that the degradation rate determines the relaxation time to steady-state, the mean protein count does not reach its corresponding steady-state value for either promoter copy number state. Interestingly once a couple of cell cycles have passed, the first moment has a repetitive trajectory over cell cycles. We have observed this experimentally by tracking cells as they grow under the microscope. Comparing cells at the beginning of the cell cycle with the daughter cells that appear after cell division showed that on average, all cells have the same amount of protein at the start of the cell cycle (See Fig. 18 of <span class="citation" data-cites="Phillips2019"> [<a href="#ref-Phillips2019" role="doc-biblioref">25</a>]</span>), suggesting that this dynamical steady state takes place <em>in vivo</em>.</p>
<figure>
<img src="ch5_fig08.png" id="fig:ch5_fig08" data-short-caption="First and second moment dynamics over the cell cycle" alt="Figure 8: First and second moment dynamics over the cell cycle. Mean \pm standard deviation mRNA (upper panel) and mean \pm standard deviation protein copy number (lower panel) as the cell cycle progresses. The dark shaded region delimits the fraction of the cell cycle that cells spend with a single copy of the promoter. The light shaded region delimits the fraction of the cell cycle that cells spend with two copies of the promoter. For a 100 min doubling time at the galK locus cells spend 60% of the time with one copy of the promoter and the rest with two copies. The Python code (ch5_fig08.py) used to generate this figure can be found on the original paper GitHub repository." /><figcaption aria-hidden="true">Figure 8: <strong>First and second moment dynamics over the cell cycle.</strong> Mean <span class="math inline">\(\pm\)</span> standard deviation mRNA (upper panel) and mean <span class="math inline">\(\pm\)</span> standard deviation protein copy number (lower panel) as the cell cycle progresses. The dark shaded region delimits the fraction of the cell cycle that cells spend with a single copy of the promoter. The light shaded region delimits the fraction of the cell cycle that cells spend with two copies of the promoter. For a 100 min doubling time at the <em>galK</em> locus cells spend 60% of the time with one copy of the promoter and the rest with two copies. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS08.py">(<code>ch5_fig08.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a></figcaption>
</figure>
<p>When measuring gene expression levels experimentally from an asynchronous culture, cells are sampled from any time point across their cell cycles. This means that the moments determined experimentally correspond to an average over the cell cycle. In the following section, we discuss how to account for the fact that cells are not uniformly distributed across the cell cycle to compute these averages.</p>
<h3 id="exponentially-distributed-ages">Exponentially distributed ages</h3>
<p>As mentioned in XXX, cells in exponential growth have exponentially distributed ages across the cell cycle, having more young cells compared to old ones. Specifically, the probability of a cell being at any time point in the cell cycle is given by <span class="citation" data-cites="Powell1956"> [<a href="#ref-Powell1956" role="doc-biblioref">15</a>]</span> <span class="math display">\[
    P(a) = (\ln 2) \cdot 2^{1 - a},
    \label{seq_age_prob}
\]</span> where <span class="math inline">\(a \in [0, 1]\)</span> is the stage of the cell cycle, with <span class="math inline">\(a = 0\)</span> being the start of the cycle and <span class="math inline">\(a = 1\)</span> being the cell division. In XXX we reproduce this derivation. It is a surprising result, but it can be intuitively thought as follows: If the culture is growing exponentially, that means that all the time, there is an increasing number of cells. That means, for example, that if in a time interval <span class="math inline">\(\Delta t\)</span> <span class="math inline">\(N\)</span> “old” cells divided, these produced <span class="math inline">\(2N\)</span> “young” cells. So at any point, there are always more younger than older cells.</p>
<p>Our numerical integration of the moment equations gave us a time evolution of the moments as cells progress through the cell cycle. Since experimentally we sample asynchronous cells that follow Eq. <span class="math inline">\(\ref{seq_age_prob}\)</span>, each time point along the moment dynamic must be weighted by the probability of having sampled a cell at such a specific time point of the cell cycle. Without loss of generality, let’s focus on the first mRNA moment <span class="math inline">\(\left\langle{m(t)}\right\rangle\)</span> (the same can be applied to all other moments). As mentioned before, to calculate the first moment across the entire cell cycle, we must weigh each time point by the corresponding probability that a cell is found in such a point of its cell cycle. This translates to computing the integral <span class="math display">\[
\langle m \rangle_c = 
\int_{\text{beginning cell cycle}}^{\text{end cell cycle}}
                       \left\langle{m(t)}\right\rangle P(t) dt,
\]</span> where <span class="math inline">\(\langle m \rangle_c\)</span> is the mean mRNA copy number averaged over the entire cell cycle trajectory, and <span class="math inline">\(P(t)\)</span> is the probability of a cell being at a time <span class="math inline">\(t\)</span> of its cell cycle.</p>
<p>If we set the time in units of the cell cycle length, we can use Eq. <span class="math inline">\(\ref{seq_age_prob}\)</span> and compute instead <span class="math display">\[
    \langle m \rangle = \int_0^1 \left\langle{m(a)}\right\rangle P(a) da,
    \label{seq_moment_avg}
\]</span> where <span class="math inline">\(P(a)\)</span> is given by Eq. <span class="math inline">\(\ref{seq_age_prob}\)</span>.</p>
<p>What Eq. <span class="math inline">\(\ref{seq_moment_avg}\)</span> implies is that to compute the first moment (or any moment of the distribution), we must weigh each point in the moment dynamics by the corresponding probability of a cell being at that point along its cell cycle. That is why when computing a moment, we take the time trajectory of a single cell cycle as the ones shown in Fig. 8 and compute the average using Eq. <span class="math inline">\(\ref{seq_age_prob}\)</span> to weigh each time point. We perform this integral numerically for all moments using Simpson’s rule.</p>
<h3 id="reproducing-the-equilibrium-picture">Reproducing the equilibrium picture</h3>
<p>Given the large variability of the first moments depicted in Fig. 8 it is worth considering why a simplistic equilibrium picture has shown to be very successful in predicting the mean expression level under diverse conditions <span class="citation" data-cites="Garcia2011c Brewster2014 Barnes2019 Razo-Mejia2018"> [<a href="#ref-Garcia2011c" role="doc-biblioref">5</a>–<a href="#ref-Razo-Mejia2018" role="doc-biblioref">7</a>,<a href="#ref-Barnes2019" role="doc-biblioref">26</a>]</span>. This section compares the simple repression thermodynamic model with this dynamical picture of the cell cycle. But before diving into this comparison, it is worth recapping the assumptions that go into the equilibrium model.</p>
<h4 id="steady-state-under-the-thermodynamic-model">Steady-state under the thermodynamic model</h4>
<p>Given the construction of the thermodynamic model of gene regulation for which the probability of the promoter microstates rather than the probability of mRNA or protein counts are accounted for; we can only describe the first moment’s dynamics using this theoretical framework <span class="citation" data-cites="Phillips2015a"> [<a href="#ref-Phillips2015a" role="doc-biblioref">18</a>]</span>. Again let’s only focus on the mRNA first moment <span class="math inline">\(\langle m \rangle\)</span>. The same principles apply if we consider the protein first moment. We can write a dynamical system of the form <span class="math display">\[
\frac{d \langle m \rangle}{dt} = 
r_m \cdot p_{\text{bound}} - \gamma _m \langle m \rangle,
\]</span> where <span class="math inline">\(r_m\)</span> and <span class="math inline">\(\gamma _m\)</span> are the mRNA production and degradation rates respectively, and <span class="math inline">\(p_{\text{bound}}\)</span> is the probability of finding the RNAP bound to the promoter <span class="citation" data-cites="Bintu2005a"> [<a href="#ref-Bintu2005a" role="doc-biblioref">10</a>]</span>. This dynamical system is predicted to have a single stable fixed point that we can find by computing the steady state. When we solve for the mean mRNA copy number at steady state <span class="math inline">\(\langle m \rangle_{ss}\)</span> we find <span class="math display">\[
\langle m \rangle_{ss} = \frac{r_m}{\gamma _m} p_{\text{bound}}.
\]</span></p>
<p>Since we assume that the only effect that the repressor has over the promoter’s regulation is the exclusion of the RNAP from binding to the promoter, we assume that only <span class="math inline">\(p_{\text{bound}}\)</span> depends on the repressor copy number <span class="math inline">\(R\)</span>. Therefore when computing the fold-change in gene expression, we are left with <span class="math display">\[
\text{fold-change} = \frac{\left\langle{m (R \neq 0)}\right\rangle_{ss}}{
\left\langle{m (R = 0)}\right\rangle_{ss}}
= \frac{p_{\text{bound}} (R \neq 0)}{p_{\text{bound}} (R = 0)}.
\]</span> As derived in <span class="citation" data-cites="Garcia2011c"> [<a href="#ref-Garcia2011c" role="doc-biblioref">5</a>]</span> this can be written in the language of equilibrium statistical mechanics as <span class="math display">\[
    \text{fold-change} = 
    \left(1 + \frac{R}{N_{NS}}e^{-\beta \Delta\varepsilon_r}  \right)^{-1},
    \label{seq_fold_change_thermo}
\]</span> where <span class="math inline">\(\beta \equiv (k_BT)^{-1}\)</span>, <span class="math inline">\(\Delta\varepsilon_r\)</span> is the repressor-DNA binding energy, and <span class="math inline">\(N_{NS}\)</span> is the number of non-specific binding sites where the repressor can bind.</p>
<p>To arrive at Eq. <span class="math inline">\(\ref{seq_fold_change_thermo}\)</span> we ignore the physiological changes that occur during the cell cycle; one of the most important being the variability in gene copy number that we are exploring in this section. It is therefore worth thinking about whether or not the dynamical picture exemplified in Fig. 8 can be reconciled with the predictions made by Eq. <span class="math inline">\(\ref{seq_fold_change_thermo}\)</span> both at the mRNA and protein level.</p>
<p>Fig. 9 compares the predictions of both theoretical frameworks for varying repressor copy numbers and repressor-DNA affinities. The solid lines are directly computed from Eq. <span class="math inline">\(\ref{seq_fold_change_thermo}\)</span>. The hollow triangles and the solid circles represent the fold-change in mRNA and protein, respectively, as computed from the moment dynamics. To compute the fold-change from the kinetic picture, we first numerically integrate the moment dynamics for both the two- and the three-state promoter (See Fig. 8 for the unregulated case) and then average the time series accounting for the probability of cells being sampled at each stage of the cell cycle as defined in Eq. <span class="math inline">\(\ref{seq_moment_avg}\)</span>. The small systematic deviations between both models come partly from the simplifying assumption that the repressor copy number, and therefore the repressor on rate <span class="math inline">\(k^{(r)}_{\text{on}}\)</span> remains constant during the cell cycle. In principle, the gene producing the repressor protein itself is also subjected to the same duplication during the cell cycle, changing, therefore, the mean repressor copy number for both stages.</p>
<figure>
<img src="ch5_fig09.png" id="fig:ch5_fig09" data-short-caption="Comparison of the equilibrium and kinetic repressor titration predictions" alt="Figure 9: Comparison of the equilibrium and kinetic reressor titration predictions. The equilibrium model (solid lines) and the kinetic model with variation over the cell cycle (solid circles and white triangles) predictions are compared for varying repressor copy numbers and operator binding energy. The equilibrium model is directly computed from Eq. \ref{seq_fold_change_thermo} while the kinetic model is computed by numerically integrating the moment equations over several cell cycles, and then averaging over the extent of the cell cycle as defined in Eq. \ref{seq_moment_avg} . The Python code (ch5_fig09.py) used to generate this figure can be found on the original paper GitHub repository." /><figcaption aria-hidden="true">Figure 9: <strong>Comparison of the equilibrium and kinetic reressor titration predictions.</strong> The equilibrium model (solid lines) and the kinetic model with variation over the cell cycle (solid circles and white triangles) predictions are compared for varying repressor copy numbers and operator binding energy. The equilibrium model is directly computed from Eq. <span class="math inline">\(\ref{seq_fold_change_thermo}\)</span> while the kinetic model is computed by numerically integrating the moment equations over several cell cycles, and then averaging over the extent of the cell cycle as defined in Eq. <span class="math inline">\(\ref{seq_moment_avg}\)</span> . The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS09.py">(<code>ch5_fig09.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a></figcaption>
</figure>
<p>For completeness Fig. 10 compares the kinetic and equilibrium models for the extended model of <span class="citation" data-cites="Razo-Mejia2018"> [<a href="#ref-Razo-Mejia2018" role="doc-biblioref">7</a>]</span> in which the inducer concentration enters into the equation. The solid line is directly computed from Eq. 5 of <span class="citation" data-cites="Razo-Mejia2018"> [<a href="#ref-Razo-Mejia2018" role="doc-biblioref">7</a>]</span>. The hollow triangles and solid points follow the same procedure as for Fig. 9, where the only effect that the inducer is assumed to have in the kinetics is an effective change in the number of active repressors, affecting, therefore, <span class="math inline">\(k^{(r)}_{\text{on}}\)</span>.</p>
<figure>
<img src="ch5_fig10.png" id="fig:ch5_fig10" data-short-caption="Comparison of the equilibrium and kinetic inducer titration predictions" alt="Figure 10: Comparison of the equilibrium and kinetic inducer titration predictions. The equilibrium model (solid lines) and the kinetic model with variation over the cell cycle (solid circles and white triangles) predictions are compared for varying repressor copy numbers and inducer concentrations. The equilibrium model is directly computed as Eq. 5 of reference  [7] with repressor-DNA binding energy \Delta\varepsilon_r = -13.5 \; k_BT while the kinetic model is computed by numerically integrating the moment dynamics over several cell cycles, and then averaging over the extent of a single cell cycle as defined in Eq. \ref{seq_moment_avg} The Python code (ch5_fig10.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 10: <strong>Comparison of the equilibrium and kinetic inducer titration predictions.</strong> The equilibrium model (solid lines) and the kinetic model with variation over the cell cycle (solid circles and white triangles) predictions are compared for varying repressor copy numbers and inducer concentrations. The equilibrium model is directly computed as Eq. 5 of reference <span class="citation" data-cites="Razo-Mejia2018"> [<a href="#ref-Razo-Mejia2018" role="doc-biblioref">7</a>]</span> with repressor-DNA binding energy <span class="math inline">\(\Delta\varepsilon_r = -13.5 \; k_BT\)</span> while the kinetic model is computed by numerically integrating the moment dynamics over several cell cycles, and then averaging over the extent of a single cell cycle as defined in Eq. <span class="math inline">\(\ref{seq_moment_avg}\)</span> The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS10.py">(<code>ch5_fig10.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h3 id="comparison-between-single--and-multi-promoter-kinetic-model">Comparison between single- and multi-promoter kinetic model</h3>
<p>After these calculations, it is worth questioning whether this change in gene dosage is drastically different from the more straightforward picture of a kinetic model that ignores the gene copy number variability during the cell cycle. To this end, we systematically computed the average moments for varying repressor copy numbers and repressor-DNA affinities. We then compare these results with the moments obtained from a single-promoter model and their corresponding parameters. The derivation of the steady-state moments of the distribution for the single-promoter model is detailed in XXX.</p>
<p>Fig. 9 and Fig. 10 both suggest that since the dynamic multi-promoter model can reproduce the results of the equilibrium model at the first-moment level, it must then also be able to reproduce the results of the single-promoter model at this level (See XXX). The interesting comparison comes with higher moments. A useful metric to consider for gene expression variability is the noise in gene expression <span class="citation" data-cites="Shahrezaei2008"> [<a href="#ref-Shahrezaei2008" role="doc-biblioref">3</a>]</span>. This quantity, defined as the standard deviation divided by the mean, is a dimensionless metric of how much variability there is with respect to the mean of a distribution. As we will show below, this quantity differs from the also commonly used metric known as the Fano factor (variance/mean). For experimentally determined expression levels in arbitrary fluorescent units, the noise is a dimensionless quantity while the Fano factor is not.</p>
<p>Fig. 11 shows the comparison of the predicted protein noise between the single- (dashed lines) and the multi-promoter model (solid lines) for different operators and repressor copy numbers. A striking difference between both is that the single-promoter model predicts that as the inducer concentration increases, the standard deviation grows much slower than the mean, giving a very small noise. In comparison the multi-promoter model has a much higher floor for the lowest value of the noise, reflecting the expected result that the variability in gene copy number across the cell cycle should increase the cell-to-cell variability in gene expression <span class="citation" data-cites="Peterson2015 Jones2014a"> [<a href="#ref-Jones2014a" role="doc-biblioref">4</a>,<a href="#ref-Peterson2015" role="doc-biblioref">13</a>]</span></p>
<figure>
<img src="ch5_fig11.png" id="fig:ch5_fig11" data-short-caption="Comparison of the predicted protein noise between a single- and a multi-promoter kinetic model" alt="Figure 11: Comparison of the predicted protein noise between a single- and a multi-promoter kinetic model. Comparison of the noise (standard deviation/mean) between a kinetic model that considers a single promoter at all times (dashed line) and the multi-promoter model developed in this section (solid line) for different repressor operators. (A) Operator O1, \Delta\varepsilon_r = -15.3 \; k_BT, (B) O2, \Delta\varepsilon_r = -13.9 \; k_BT, (C) O3, \Delta\varepsilon_r = -9.7 \; k_BT. The Python code (ch5_fig11.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 11: <strong>Comparison of the predicted protein noise between a single- and a multi-promoter kinetic model.</strong> Comparison of the noise (standard deviation/mean) between a kinetic model that considers a single promoter at all times (dashed line) and the multi-promoter model developed in this section (solid line) for different repressor operators. (A) Operator O1, <span class="math inline">\(\Delta\varepsilon_r = -15.3 \; k_BT\)</span>, (B) O2, <span class="math inline">\(\Delta\varepsilon_r = -13.9 \; k_BT\)</span>, (C) O3, <span class="math inline">\(\Delta\varepsilon_r = -9.7 \; k_BT\)</span>. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS11.py">(<code>ch5_fig11.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h3 id="comparison-with-experimental-data">Comparison with experimental data</h3>
<p>Having shown that the kinetic model presented in this section can not only reproduce the results from the equilibrium picture at the mean level (See Fig. 9 and Fig. 10), but make predictions for the cell-to-cell variability as quantified by the noise (See Fig. 11), we can assess whether or not this model can predict experimental measurements of the noise. For this, we take the single-cell intensity measurements (See Methods) to compute the noise at the protein level.</p>
<p>This metric differs from the Fano factor since for arbitrary fluorescent units, the noise is a dimensionless quantity. To see why consider that the noise is defined as <span class="math display">\[
\text{noise} \equiv \frac{\sqrt{\left\langle p^2 \right\rangle -
                        \left\langle p \right\rangle^2}}
                        {\left\langle p \right\rangle}.
    \label{seq_noise_protein}
\]</span> We assume that the intensity level of a cell <span class="math inline">\(I\)</span> is linearly proportional to the absolute protein count, i.e. <span class="math display">\[
    I = \alpha p,
    \label{seq_calibration_factor}
\]</span> where <span class="math inline">\(\alpha\)</span> is the proportionality constant between arbitrary units and absolute protein number <span class="math inline">\(p\)</span>. Substituting this definition on Eq. <span class="math inline">\(\ref{seq_noise_protein}\)</span> gives <span class="math display">\[
\text{noise} = \frac{\sqrt{\left\langle{(\alpha I)^2}\right\rangle - 
\left\langle{\alpha I}\right\rangle^2}}{
\left\langle{\alpha I}\right\rangle}.
\]</span></p>
<p>Since <span class="math inline">\(\alpha\)</span> is a constant, it can be taken out of the average operator <span class="math inline">\(\left\langle{\cdot}\right\rangle\)</span>, obtaining <span class="math display">\[
\text{noise} = \frac{\sqrt{\alpha^2 \left(\left\langle{I^2}\right\rangle -
\left\langle{I}\right\rangle^2 \right)}}{
\alpha \left\langle{I}\right\rangle}
= \frac{\sqrt{\left(\left\langle{I^2}\right\rangle - 
\left\langle{I}\right\rangle^2 \right)}}{
\left\langle{I}\right\rangle}.
\]</span></p>
<p>Notice that in Eq. <span class="math inline">\(\ref{seq_calibration_factor}\)</span> the linear proportionality between intensity and protein count has no intercept. This ignores the autofluorescence that cells without reporter would generate. To account for this, in practice, we compute <span class="math display">\[
\text{noise} = 
\frac{\sqrt{\left(\left\langle{(I - \left\langle
{I_\text{auto}}\right\rangle)^2}\right\rangle -
\left\langle{I - \left\langle{I_\text{auto}}\right\rangle}\right\rangle^2
\right)}}{
\left\langle{I - \left\langle{I_\text{auto}}\right\rangle}\right\rangle}.
\]</span> where <span class="math inline">\(I\)</span> is the intensity of the strain of interest and <span class="math inline">\(\left\langle{I_\text{auto}}\right\rangle\)</span> is the mean autofluorescence intensity, obtained from a strain that does not carry the fluorescent reporter gene.</p>
<p>Fig. 12 shows the comparison between theoretical predictions and experimental measurements for the unregulated promoter. The reason we split the data by operator despite the fact that since these are unregulated promoters, they should, in principle, have identical expression profiles is to make sure that this is the case precisely. We have found in the past that sequences downstream of the RNAP binding site can affect the expression level of constitutively expressed genes. We can see that both models, the single-promoter (gray dotted line) and the multi-promoter (black dashed line) underestimates the experimental noise to different degrees. The single-promoter model does a worse job predicting the experimental data since it doesn’t account for the differences in gene dosage during the cell cycle. But still, we can see that accounting for this variability takes us to within a factor of two of the experimentally determined noise for these unregulated strains.</p>
<figure>
<img src="ch5_fig12.png" id="fig:ch5_fig12" data-short-caption="Protein noise of the unregulated promoter" alt="Figure 12: Protein noise of the unregulated promoter. Comparison of the experimental noise for different operators with the theoretical predictions for the single-promoter (gray dotted line) and the multi-promoter model (black dashed line). Each datum represents a single date measurement of the corresponding \Delta lacI strain with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. The Python code (ch5_fig12.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 12: <strong>Protein noise of the unregulated promoter.</strong> Comparison of the experimental noise for different operators with the theoretical predictions for the single-promoter (gray dotted line) and the multi-promoter model (black dashed line). Each datum represents a single date measurement of the corresponding <span class="math inline">\(\Delta lacI\)</span> strain with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS12.py">(<code>ch5_fig12.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<p>To further test the model predictive power, we compare the predictions for the three-state regulated promoter. Fig. 13 shows the theoretical predictions for the single- and multi-promoter model for varying repressor copy numbers and repressor-DNA binding affinities as a function of the inducer concentration. Again, we can see that our zero-parameter fits systematically underestimate the noise for all strains and all inducer concentrations. We highlight that the <span class="math inline">\(y\)</span>-axis is shown in a log-scale to emphasize this deviation more, but, as we will show in the next section, our predictions still fall within a factor of two from the experimental data.</p>
<figure>
<img src="ch5_fig13.png" id="fig:ch5_fig13" data-short-caption="Protein noise of the regulated promoter" alt="Figure 13: Protein noise of the regulated promoter. Comparison of the experimental noise for different operators ((A) O1, \Delta\varepsilon_r = -15.3 \; k_BT, (B) O2, \Delta\varepsilon_r = -13.9 \; k_BT, (C) O3, \Delta\varepsilon_r = -9.7 \; k_BT) with the theoretical predictions for the single-promoter (dashed lines) and the multi-promoter model (solid lines). Points represent the experimental noise as computed from single-cell fluorescence measurements of different E. coli strains under 12 different inducer concentrations. The dotted line indicates the plot in linear rather than logarithmic scale. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. White-filled dots are plot at a different scale for better visualization. The Python code (ch5_fig13.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 13: <strong>Protein noise of the regulated promoter.</strong> Comparison of the experimental noise for different operators ((A) O1, <span class="math inline">\(\Delta\varepsilon_r = -15.3 \; k_BT\)</span>, (B) O2, <span class="math inline">\(\Delta\varepsilon_r = -13.9 \; k_BT\)</span>, (C) O3, <span class="math inline">\(\Delta\varepsilon_r = -9.7 \; k_BT\)</span>) with the theoretical predictions for the single-promoter (dashed lines) and the multi-promoter model (solid lines). Points represent the experimental noise as computed from single-cell fluorescence measurements of different <em>E. coli</em> strains under 12 different inducer concentrations. The dotted line indicates the plot in linear rather than logarithmic scale. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. White-filled dots are plot at a different scale for better visualization. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS13.py">(<code>ch5_fig13.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h4 id="systematic-deviation-of-the-noise-in-gene-expression">Systematic deviation of the noise in gene expression</h4>
<p>Fig. 12 and Fig. 13 highlight that our model underestimates the cell-to-cell variability as measured by the noise. To further explore this systematic deviation Fig. 14 shows the theoretical vs. experimental noise both in linear and log scale. As we can see, the data is systematically above the identity line. Their corresponding experimental fold-change values color the data. The data with the largest deviations from the identity line also corresponds to the data with the largest error bars and the smallest fold-change. This is because measurements with very small fold-changes correspond to intensities very close to the autofluorescence background. Therefore minimal changes when computing the noise are amplified given the ratio of std/mean. In XXX, we will explore empirical ways to improve the agreement between our minimal and experimental data to guide future efforts to improve the minimal.</p>
<figure>
<img src="ch5_fig14.png" id="fig:ch5_fig14" data-short-caption="Systematic comparison of theoretical vs. experimental noise in gene expression" alt="Figure 14: Systematic comparison of theoretical vs. experimental noise in gene expression. Theoretical vs. experimental noise both in linear (left) and log (right) scale. The dashed line shows the identity line of slope one and intercept zero. All data are colored by the experimental fold-change’s corresponding value in gene expression as indicated by the color bar. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. The Python code (ch5_fig14.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 14: <strong>Systematic comparison of theoretical vs. experimental noise in gene expression.</strong> Theoretical vs. experimental noise both in linear (left) and log (right) scale. The dashed line shows the identity line of slope one and intercept zero. All data are colored by the experimental fold-change’s corresponding value in gene expression as indicated by the color bar. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS14.py">(<code>ch5_fig14.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h2 id="maximum-entropy-approximation-of-distributions">Maximum entropy approximation of distributions</h2>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/MaxEnt_approx_joint.html">following link</a> as an annotated Jupyter notebook)</p>
<p>On the one hand, chemical master equations like the one here represent a hard mathematical challenge. As presented in Peccoud and Ycart derived a closed-form solution for the two-state promoter at the mRNA level <span class="citation" data-cites="Peccoud1995"> [<a href="#ref-Peccoud1995" role="doc-biblioref">2</a>]</span>. In an impressive display of mathematical skills, Shahrezaei and Swain were able to derive an approximate solution for the one- (not considered in this work) and two-state promoter master equation at the protein level <span class="citation" data-cites="Shahrezaei2008"> [<a href="#ref-Shahrezaei2008" role="doc-biblioref">3</a>]</span>. Nevertheless, both of these solutions do not give instantaneous insights about the distributions as they involve complicated terms such as confluent hypergeometric functions.</p>
<p>On the other hand, there has been a great deal of work to generate methods that can approximate the solution of these discrete state Markovian models <span class="citation" data-cites="Ale2013 Andreychenko2017 Frohlich2016 Schnoerr2017 Smadbeck2013"> [<a href="#ref-Ale2013" role="doc-biblioref">27</a>–<a href="#ref-Smadbeck2013" role="doc-biblioref">31</a>]</span>. In particular, for master equations like the one that concerns us here, whose moments can be easily computed, the moment expansion method provides a simple method to approximate the full joint distribution of mRNA and protein <span class="citation" data-cites="Smadbeck2013"> [<a href="#ref-Smadbeck2013" role="doc-biblioref">31</a>]</span>. This section will explain the principles behind this method and show the implementation for our particular case study.</p>
<h3 id="the-maxent-principle">The MaxEnt principle</h3>
<p>The principle of maximum entropy (MaxEnt) first proposed by E. T. Jaynes in 1957 tackles the question of given limited information what is the least biased inference one can make about a particular probability distribution <span class="citation" data-cites="Jaynes1957"> [<a href="#ref-Jaynes1957" role="doc-biblioref">32</a>]</span>. In particular, Jaynes used this principle to show the correspondence between statistical mechanics and information theory, demonstrating, for example, that the Boltzmann distribution is the probability distribution that maximizes Shannon’s entropy subject to a constraint that the average energy of the system is fixed.</p>
<p>To illustrate the principle let us focus on a univariate distribution <span class="math inline">\(P_X(x)\)</span>. The <span class="math inline">\(n^{\text{th}}\)</span> moment of the distribution for a discrete set of possible values of <span class="math inline">\(x\)</span> is given by <span class="math display">\[
    \left\langle{x^n}\right\rangle \equiv \sum_x x^n P_X(x).
    \label{eq:mom_ref}
\]</span></p>
<p>Now assume that we have knowledge of the first <span class="math inline">\(m\)</span> moments <span class="math inline">\(\mathbf{\left\langle{x}\right\rangle}_m = (\left\langle{x}\right\rangle, \left\langle{x^2}\right\rangle, \ldots, \left\langle{x^m}\right\rangle )\)</span>. The question is then how can we use this information to build an estimator <span class="math inline">\(P_H(x \mid \mathbf{\left\langle{x}\right\rangle}_m)\)</span> of the distribution such that <span class="math display">\[
\lim_{m \rightarrow \infty} 
P_H(x \mid \mathbf{\left\langle{x}\right\rangle}_m) \rightarrow P_X(x),
\]</span> i.e. that the more moments we add to our approximation, the more the estimator distribution converges to the real distribution.</p>
<p>The MaxEnt principle tells us that our best guess for this estimator is to build it based on maximizing the Shannon entropy, constrained by the information we have about these <span class="math inline">\(m\)</span> moments. Shannon’s entropy’s maximization guarantees that we are the least committed possible to information that we do not possess. The Shannon entropy for a univariate discrete distribution is given by <span class="citation" data-cites="Shannon1948"> [<a href="#ref-Shannon1948" role="doc-biblioref">33</a>]</span> <span class="math display">\[
H(x) \equiv - \sum_x P_X(x) \log P_X(x).
\]</span></p>
<p>For an optimization problem subject to constraints, we make use of the method of the Lagrange multipliers. For this, we define the constraint equation <span class="math inline">\(\mathcal{L}(x)\)</span> as <span class="math display">\[
    \mathcal{L}(x) \equiv H(x) - \sum_{i=0}^m
    \left[ \lambda_i \left( \left\langle{x^i}\right\rangle -
    \sum_x x^i P_X(x) \right) \right],
    \label{eq:constraint_eq}
\]</span> where <span class="math inline">\(\lambda_i\)</span> is the Lagrange multiplier associated with the <span class="math inline">\(i^\text{th}\)</span> moment. The inclusion of the zeroth moment is an additional constraint to guarantee the normalization of the resulting distribution. Since <span class="math inline">\(P_X(x)\)</span> has a finite set of discrete values, when taking the derivative of the constraint equation with respect to <span class="math inline">\(P_X(x)\)</span>, we chose a particular value of <span class="math inline">\(X = x\)</span>. Therefore from the sum over all possible <span class="math inline">\(x\)</span> values, only a single term survives. With this in mind, we take the derivative of the constraint equation, obtaining <span class="math display">\[
\frac{d\mathcal{L}}{d P_X(x)} = -\log P_X(x) - 1 -
\sum_{i=0}^m \lambda_i x^i.
\]</span></p>
<p>Equating this derivative to zero and solving for the distribution (that we now start calling <span class="math inline">\(P_H(x)\)</span>, our MaxEnt estimator) gives <span class="math display">\[
P_H(x) = \exp \left(- 1 - \sum_{i=0}^m \lambda_i x^i \right)
= \frac{1}{\mathcal{Z}}
\exp \left( - \sum_{i=1}^m \lambda_i x^i \right),
\label{eq:maxEnt}
\]</span> where <span class="math inline">\(\mathcal{Z}\)</span> is the normalization constant that can be obtained by substituting this solution into the normalization constraint. This results in <span class="math display">\[
\mathcal{Z} \equiv \exp\left( 1 + \lambda_0 \right) =
\sum_x \exp \left( - \sum_{i=1}^m \lambda_i x^i \right).
\]</span></p>
<p>Eq. <span class="math inline">\(\ref{eq:maxEnt}\)</span> is the general form of the MaxEnt distribution for a univariate distribution. The computational challenge then consists of finding numerical values for the Lagrange multipliers <span class="math inline">\(\{ \lambda_i \}\)</span> such that <span class="math inline">\(P_H(x)\)</span> satisfies our constraints. In other words, the Lagrange multipliers weigh the contribution of each term in the exponent such that when computing any of the moments, we recover the value of our constraint. Mathematically what this means is that <span class="math inline">\(P_H(x)\)</span> must satisfy <span class="math display">\[
\sum_x x^n P_H(x) =
\sum_x \frac{x^n}{\mathcal{Z}}
\exp \left( - \sum_{i=1}^m \lambda_i x^i \right) = 
\left\langle{x^n}\right\rangle.
\]</span></p>
<p>As an example of applying the MaxEnt principle, let us use a six-face die’s classic problem. If we are only told that after a large number of die rolls, the mean value of the face is <span class="math inline">\(\left\langle{x}\right\rangle = 4.5\)</span> (note that a fair die has a mean of <span class="math inline">\(3.5\)</span>), what would the least biased guess for the distribution look like? The MaxEnt principle tells us that our best guess would be of the form <span class="math display">\[
P_H(x) = \frac{1}{\mathcal{Z}} \exp \left( \lambda x \right).
\]</span> Using any numerical minimization package, we can easily find the value of the Lagrange multiplier <span class="math inline">\(\lambda\)</span> that satisfies our constraint. Fig. 15 shows two examples of distributions that satisfy the constraint. Panel (A) shows a distribution consistent with the 4.5 average where both 4 and 5 are equally likely. Nevertheless, in the information we got about the nature of the die, it was never stated that some of the faces were forbidden. In that sense, the distribution is committing to information about the process that we do not possess. Panel (B), by contrast, shows the MaxEnt distribution that satisfies this constraint. Since this distribution maximizes Shannon’s entropy, it is guaranteed to be the least biased distribution given the available information.</p>
<figure>
<img src="ch5_fig15.png" id="fig:ch5_fig15" data-short-caption="Maximum entropy distribution of six-face die" alt="Figure 15: Maximum entropy distribution of six-face die. (A)biased distribution consistent with the constraint \left\langle{x}\right\rangle = 4.5. (B) MaxEnt distribution also consistent with the constraint. The Python code (ch5_fig15.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 15: <strong>Maximum entropy distribution of six-face die.</strong> (A)biased distribution consistent with the constraint <span class="math inline">\(\left\langle{x}\right\rangle = 4.5\)</span>. (B) MaxEnt distribution also consistent with the constraint. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS15.py">(<code>ch5_fig15.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h4 id="the-mrna-and-protein-joint-distribution">The mRNA and protein joint distribution</h4>
<p>The MaxEnt principle can easily be extended to multivariate distributions. For our particular case, we are interested in the mRNA and protein joint distribution <span class="math inline">\(P(m, p)\)</span>. The definition of a moment <span class="math inline">\(\left\langle m^x p^y \right\rangle\)</span> is a natural extension of Eq. <span class="math inline">\(\ref{eq:mom_ref}\)</span> of the form <span class="math display">\[
\left\langle m^x p^y \right\rangle = \sum_m \sum_p m^x p^y P(m, p).
\]</span></p>
<p>As a consequence, the MaxEnt joint distribution <span class="math inline">\(P_H(m, p)\)</span> is of the form <span class="math display">\[
P_H(m, p) = \frac{1}{\mathcal{Z}}
              \exp \left( - \sum_{(x,y)} \lambda_{(x,y)} m^x p^y \right),
\label{eq:maxEnt_joint}
\]</span> where <span class="math inline">\(\lambda_{(x,y)}\)</span> is the Lagrange multiplier associated with the moment <span class="math inline">\(\left\langle m^x p^y \right\rangle\)</span>, and again <span class="math inline">\(\mathcal{Z}\)</span> is the normalization constant, given by <span class="math display">\[
\mathcal{Z} = \sum_m \sum_p
              \exp \left( - \sum_{(x, y)} \lambda_{(x, y)} m^x p^y \right).
\]</span> Note that the sum in the exponent is taken over all available <span class="math inline">\((x, y)\)</span> pairs that define the moment constraints for the distribution.</p>
<h3 id="the-bretthorst-rescaling-algorithm">The Bretthorst rescaling algorithm</h3>
<p>The Lagrange multipliers’ determination suffers from a numerical underflow and overflow problem due to the difference in magnitude between the constraints. This becomes a problem when higher moments are taken into account. The resulting numerical values for the Lagrange multipliers end up being separated by several orders of magnitude. For routines such as Newton-Raphson or other minimization algorithms that can be used to find these Lagrange multipliers, these different scales become problematic.</p>
<p>To get around this problem, we implemented a variation to the algorithm due to G. Larry Bretthorst, E.T. Jaynes’ last student. With a straightforward argument, we can show that linearly rescaling the constraints, the Lagrange multipliers, and the “rules” for computing each of the moments, i.e., each of the individual products that go into the moment calculation, should converge to the same MaxEnt distribution. In order to see this let’s consider again a univariate distribution <span class="math inline">\(P_X(x)\)</span> that we are trying to reconstruct given the first two moments <span class="math inline">\(\left\langle{x}\right\rangle\)</span>, and <span class="math inline">\(\left\langle{x^2}\right\rangle\)</span>. The MaxEnt distribution can be written as <span class="math display">\[
P_H(x) = \frac{1}{\mathcal{Z}}
  \exp \left(- \lambda_1 x - \lambda_2 x^2 \right) =
  \frac{1}{\mathcal{Z}}
  \exp \left(- \lambda_1 x \right) \exp \left( - \lambda_2 x^2 \right).
\]</span> We can always rescale the terms in any way and obtain the same result. Assume that, for some reason, we want to rescale the quadratic terms by a factor <span class="math inline">\(a\)</span>. We can define a new Lagrange multiplier <span class="math inline">\(\lambda_2&#39; \equiv \frac{\lambda_2}{a}\)</span> that compensates for the rescaling of the terms, obtaining <span class="math display">\[
P_H(x) = \frac{1}{\mathcal{Z}}
  \exp \left(- \lambda_1 x \right) \exp \left( - \lambda_2&#39; ax^2 \right).
\]</span> Computationally it might be more efficient to find the numerical value of <span class="math inline">\(\lambda_2&#39;\)</span> rather than <span class="math inline">\(\lambda_2\)</span> maybe because it is of the same order of magnitude as <span class="math inline">\(\lambda_1\)</span>. Then we can always multiply <span class="math inline">\(\lambda_2&#39;\)</span> by <span class="math inline">\(a\)</span> to obtain back the constraint for our quadratic term. This means that we can always rescale the MaxEnt problem to make it numerically more stable, then we can rescale it back to obtain the value of the Lagrange multipliers. The key to the Bretthorst algorithm lies in selecting what rescaling factor to choose to make the numerical inference more efficient.</p>
<p>Bretthorst’s algorithm goes even further by further transforming the constraints and the variables to make the constraints orthogonal, making the computation much more effective. We now explain the algorithm’s implementation for our joint distribution of interest <span class="math inline">\(P(m, p)\)</span>.</p>
<h4 id="algorithm-implementation">Algorithm implementation</h4>
<p>Let the <span class="math inline">\(M \times N\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> contain all the factors used to compute the moments that serve as constraints, where each entry is of the form <span class="math display">\[
A_{ij} = m_i^{x_j} \cdot p_i^{y_j}.
\label{eq:maxent_rules}
\]</span> In other words, recall that to obtain any moment <span class="math inline">\(\left\langle m^x p^y \right\rangle\)</span> we compute <span class="math display">\[
\left\langle m^x p^y \right\rangle = \sum_m \sum_p m^x p^y P(m, x).
\]</span> If we have <span class="math inline">\(M\)</span> possible <span class="math inline">\((m, p)\)</span> pairs in our truncated sample space (because we can’t include the sample space up to infinity) <span class="math inline">\(\{(m, p)_1, (m, p)_2, \ldots (m, p)_N \}\)</span>, and we have <span class="math inline">\(N\)</span> exponent pairs <span class="math inline">\((x, y)\)</span> corresponding to the <span class="math inline">\(N\)</span> moments used to constraint the maximum entropy distribution <span class="math inline">\(\{(x, y)_1, (x, y)_2, \ldots, (x, y)_N \}\)</span>, then matrix <span class="math inline">\(\mathbf{A}\)</span> contains all the possible <span class="math inline">\(M\)</span> by <span class="math inline">\(N\)</span> terms of the form described in Eq. <span class="math inline">\(\ref{eq:maxent_rules}\)</span>. Let also <span class="math inline">\(\mathbf{v}\)</span> be a vector of length <span class="math inline">\(N\)</span> containing all the constraints with each entry of the form <span class="math display">\[
v_j = \left\langle{m^{x_j} p^{y_j}}\right\rangle,
\]</span> i.e. the information that we have about the distribution. That means that the constraint equation <span class="math inline">\(\mathcal{L}\)</span> to be used for this problem takes the form <span class="math display">\[
\mathcal{L} = -\sum_i P_i \ln P_i + \lambda_0 \left( 1 - \sum_i P_i \right)
  + \sum_{j&gt;0} \lambda_j \left( v_j - \sum_i A_{ij} P_i \right),
\]</span> where <span class="math inline">\(\lambda_0\)</span> is the Lagrange multiplier associated with the normalization constraint, and <span class="math inline">\(\lambda_j\)</span> is the Lagrange multiplier associated with the <span class="math inline">\(j^\text{th}\)</span> constraint. This constraint equation is equivalent to Eq. <span class="math inline">\(\ref{eq:constraint_eq}\)</span>, but now all the details of how to compute the moments are specified in matrix <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>With this notation in hand, we now proceed to rescale the problem. The first step consists of rescaling the terms to compute the entries of the matrix <span class="math inline">\(\mathbf{A}\)</span>. As mentioned before, this is the crucial feature of the Bretthorst algorithm; the particular choice of rescaling factor used in the algorithm empirically promotes that the rescaled Lagrange multipliers are of the same order of magnitude. The rescaling takes the form <span class="math display">\[
A_{ij}&#39; = \frac{A_{ij}}{G_j},
\]</span> where <span class="math inline">\(G_j\)</span> serves to rescale the moments, providing numerical stability to the inference problem. Bretthorst proposes an empirical rescaling that satisfies <span class="math display">\[
G_j^2 = \sum_i A_{ij}^2,
\]</span> or in terms of our particular problem <span class="math display">\[
G_j^2 = \sum_m \sum_p \left( m^{x_j} p^{y_j} \right)^2.
\]</span> What this indicates is that each pair <span class="math inline">\(m_i^{x_j} p_i^{y_j}\)</span> is normalized by the square root of the sum of all pairs of the same form squared.</p>
<p>Since we rescale the factors involved in computing the constraints, the constraints must also be rescaled simply as <span class="math display">\[
v_j&#39; = \left\langle{m^{x_j} p^{y_j}}\right\rangle&#39; = 
\frac{\left\langle{m^{x_j} p^{y_j}}\right\rangle}{G_j}.
\]</span> The Lagrange multipliers must compensate for this rescaling since the probability must add up to the same value at the end of the day. Therefore we rescale the <span class="math inline">\(\lambda_j\)</span> terms as <span class="math display">\[
\lambda_j&#39; = \lambda_j G_j,
\]</span> such that any <span class="math inline">\(\lambda_j A_{ij} = \lambda_j&#39; A_{ij}&#39;\)</span>. If this empirical value for the rescaling factor makes the rescaled Lagrange multipliers <span class="math inline">\(\lambda_j&#39;\)</span> be of the same order of magnitude, this by itself would already improve the algorithm convergence. Bretthorst proposes another linear transformation to make the optimization routine even more efficient. For this, we generate orthogonal constraints that make Newton-Raphson and similar algorithms converge faster. The transformation is as follows <span class="math display">\[
A_{ik}&#39;&#39; = \sum_j {e}_{jk} A_{ij}&#39;,
\]</span> for the entires of matrix <span class="math inline">\(\mathbf{A}\)</span>, and <span class="math display">\[
v_k&#39;&#39; = \sum_j {e}_{jk} u_j&#39;,
\]</span> for entires of the constraint vector <span class="math inline">\(\mathbf{v}\)</span>, finally <span class="math display">\[
\lambda_k&#39;&#39; = \sum_j {e}_{jk} \beta_j,
\]</span> for the Lagrange multipliers. Here <span class="math inline">\({e}_{jk}\)</span> is the <span class="math inline">\(j^\text{th}\)</span> component of the <span class="math inline">\(k^\text{th}\)</span> eigenvector of the matrix <span class="math inline">\(\mathbf{E}\)</span> with entries <span class="math display">\[
{E}_{kj} = \sum_i {A}_{ik}&#39; {A}_{ij}&#39;.
\]</span> This transformation guarantees that the matrix <span class="math inline">\(\mathbf{A}&#39;&#39;\)</span> has the property <span class="math display">\[
\sum_i A_{ij}&#39;&#39; A_{jk}&#39;&#39; = \beta_j \delta_{jk},
\]</span> where <span class="math inline">\(\beta_j\)</span> is the <span class="math inline">\(j^\text{th}\)</span> eigenvalue of the matrix <span class="math inline">\(\mathbf{E}\)</span> and <span class="math inline">\(\delta_{jk}\)</span> is the Kronecker delta function. What this means is that, as desired, the constraints are orthogonal to each other, improving the algorithm convergence speed.</p>
<h3 id="predicting-distributions-for-simple-repression-constructs">Predicting distributions for simple repression constructs</h3>
<p>Having explained the theoretical background and the practical difficulties and a workaround strategy proposed by Bretthorst, we implemented the inference using the moments obtained from averaging over the variability along the cell cycle (See XXX). Fig. 16 and Fig. 17 present these inferences for both mRNA and protein levels respectively for different values of the repressor-DNA binding energy and repressor copy numbers per cell. From these plots, we can easily appreciate that even though the mean of each distribution changes as the induction level changes, there is a lot of overlap between distributions. This, as a consequence, means that at the single-cell level, cells cannot perfectly resolve between different inputs.</p>
<figure>
<img src="ch5_fig16.png" id="fig:ch5_fig16" data-short-caption="Maximum entropy mRNA distributions for simple repression constructs" alt="Figure 16: Maximum entropy mRNA distributions for simple repression constructs. mRNA distributions for different biophysical parameters. From left to right, the repressor-DNA affinity decreases as defined by the three lacI operators O1 (-15.3 \; k_BT), O2 (-13.9 \; k_BT), and O3 (-9.7 \; k_BT). From top to bottom, the mean repressor copy number per cell increases. The curves on each plot represent different IPTG concentrations. Each distribution was fitted using the first three moments of the mRNA distribution. The Python code (ch5_fig16.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 16: <strong>Maximum entropy mRNA distributions for simple repression constructs.</strong> mRNA distributions for different biophysical parameters. From left to right, the repressor-DNA affinity decreases as defined by the three lacI operators O1 (<span class="math inline">\(-15.3 \; k_BT\)</span>), O2 (<span class="math inline">\(-13.9 \; k_BT\)</span>), and O3 (<span class="math inline">\(-9.7 \; k_BT\)</span>). From top to bottom, the mean repressor copy number per cell increases. The curves on each plot represent different IPTG concentrations. Each distribution was fitted using the first three moments of the mRNA distribution. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS16.py">(<code>ch5_fig16.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<figure>
<img src="ch5_fig17.png" id="fig:ch5_fig17" data-short-caption="Maximum entropy protein distributions for simple repression constructs" alt="Figure 17: Maximum entropy protein distributions for simple repression constructs. Protein distributions for different biophysical parameters. From left to right, the repressor-DNA affinity decreases as defined by the three lacI operators O1 (-15.3 \; k_BT), O2 (-13.9 \; k_BT), and O3 (-9.7 \; k_BT). From top to bottom, the mean repressor copy number per cell increases. The curves on each plot represent different IPTG concentrations. Each distribution was fitted using the first six moments of the protein distribution. The Python code (ch5_fig17.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 17: <strong>Maximum entropy protein distributions for simple repression constructs.</strong> Protein distributions for different biophysical parameters. From left to right, the repressor-DNA affinity decreases as defined by the three lacI operators O1 (<span class="math inline">\(-15.3 \; k_BT\)</span>), O2 (<span class="math inline">\(-13.9 \; k_BT\)</span>), and O3 (<span class="math inline">\(-9.7 \; k_BT\)</span>). From top to bottom, the mean repressor copy number per cell increases. The curves on each plot represent different IPTG concentrations. Each distribution was fitted using the first six moments of the protein distribution. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS17.py">(<code>ch5_fig17.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h3 id="comparison-with-experimental-data-1">Comparison with experimental data</h3>
<p>Now that we have reconstructed an approximation of the probability distribution <span class="math inline">\(P(m, p)\)</span> we can compare this with our experimental measurements. But just as detailed in the single-cell microscopy, measurements are given in arbitrary units of fluorescence. Therefore we cannot directly compare our predicted protein distributions with these values. To get around this issue, we use the fact that the fold-change in gene expression that we defined as the ratio of the gene expression level in the presence of the repressor and the expression level of a knockout strain is a non-dimensional quantity. Therefore we normalize all of our single-cell measurements by the mean fluorescence value of the <span class="math inline">\(\Delta lacI\)</span> strain with the proper background fluorescence subtracted as explained in the noise measurements. In the case of the theoretical predictions of the protein distribution, we also normalize each protein value by the predicted mean protein level <span class="math inline">\(\left\langle p \right\rangle\)</span>, having now non-dimensional scales that can be directly compared. Fig. 18 shows the experimental (color curves) and theoretical (dark dashed line) cumulative distribution functions for the three <span class="math inline">\(\Delta lacI\)</span> strains. As in Fig. 12, we do not expect differences between the operators, but we explicitly plot them separately to ensure that this is the case. We can see right away that as we would expect, given the limitations of the model to predict the noise and skewness of the distribution accurately, the model doesn’t accurately predict the data. Our model predicts a narrower distribution compared to what we measured with single-cell microscopy.</p>
<figure>
<img src="ch5_fig18.png" id="fig:ch5_fig18" data-short-caption="Experiment vs. theory comparison for $\Delta lacI$ strain" alt="Figure 18: Experiment vs. theory comparison for \Delta lacI strain. Example fold-change empirical cumulative distribution functions (ECDF) for strains with no repressors and different operators. The color curves represent single-cell microscopy measurements while the dashed black lines represent the theoretical distributions as reconstructed by the maximum entropy principle. The theoretical distributions were fitted using the first six moments of the protein distribution. The Python code (ch5_fig18.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 18: <strong>Experiment vs. theory comparison for <span class="math inline">\(\Delta lacI\)</span> strain.</strong> Example fold-change empirical cumulative distribution functions (ECDF) for strains with no repressors and different operators. The color curves represent single-cell microscopy measurements while the dashed black lines represent the theoretical distributions as reconstructed by the maximum entropy principle. The theoretical distributions were fitted using the first six moments of the protein distribution. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS18.py">(<code>ch5_fig18.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<p>The same narrower prediction applies to the regulated promoters. Fig. 19, shows the theory-experiment comparison of the cumulative distribution functions for different repressor binding sites (different figures), repressor copy numbers (rows), and inducer concentrations (columns). In general, the predictions are systematically narrower compared to the actual experimental data.</p>
<figure>
<img src="ch5_fig19.png" id="fig:ch5_fig19" data-short-caption="Experiment vs. theory comparison for regulated promoters" alt="Figure 19: Experiment vs. theory comparison for regulated promoters. Example fold-change empirical cumulative distribution functions (ECDF) for regulated strains with the three operators (different colors) as a function of repressor copy numbers (rows) and inducer concentrations (columns). The color curves represent single-cell microscopy measurements while the dashed black lines represent the theoretical distributions as reconstructed by the maximum entropy principle. The theoretical distributions were fitted using the first six moments of the protein distribution. The Python code (ch5_fig19.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 19: <strong>Experiment vs. theory comparison for regulated promoters.</strong> Example fold-change empirical cumulative distribution functions (ECDF) for regulated strains with the three operators (different colors) as a function of repressor copy numbers (rows) and inducer concentrations (columns). The color curves represent single-cell microscopy measurements while the dashed black lines represent the theoretical distributions as reconstructed by the maximum entropy principle. The theoretical distributions were fitted using the first six moments of the protein distribution. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS19.py">(<code>ch5_fig19.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h2 id="gillespie-simulation-of-the-master-equation">Gillespie simulation of the master equation</h2>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/gillespie_simulation.html">following link</a> as an annotated Jupyter notebook)</p>
<p>So far, we have generated a way to compute an approximated form of the joint distribution of protein and mRNA <span class="math inline">\(P(m, p)\)</span> as a function of the moments of the distribution <span class="math inline">\(\left\langle m^x p^y \right\rangle\)</span>. This is a non-conventional form to work with the resulting distribution of the master equation. A more conventional approach to work with master equations whose closed-form solutions are not known or not computable is to use stochastic simulations, commonly known as Gillespie simulations. To benchmark our approach’s performance based on distribution moments and maximum entropy, we implemented the Gillespie algorithm. Our implementation, as detailed in the corresponding Jupyter notebook, makes use of just-in-time compilation as implemented with the Python package <a href="http://numba.pydata.org">numba</a>.</p>
<h3 id="mrna-distribution-with-gillespie-simulations">mRNA distribution with Gillespie simulations</h3>
<p>To confirm that the Gillespie simulation’s implementation was correct, we perform the simulation at the mRNA level, for which the closed-form solution of the steady-state distribution is known as detailed in XXX. Fig. 20 shows example trajectories of mRNA counts. Each of these trajectories was computed over several cell cycles, where the cell division was implemented, generating a binomially distributed random variable that depended on the last mRNA count before the division event.</p>
<figure>
<img src="ch5_fig20.png" id="fig:ch5_fig20" data-short-caption="Stochastic trajectories of mRNA counts" alt="Figure 20: Stochastic trajectories of mRNA counts. 100 stochastic trajectories generated with the Gillespie algorithm for mRNA counts over time for a two-state unregulated promoter. Cells spend a fraction of the cell cycle with a single copy of the promoter (light brown) and the rest of the cell cycle with two copies (light yellow). When trajectories reach a new cell cycle, the mRNA counts undergo a binomial partitioning to simulate the cell division. The Python code (ch5_fig20.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 20: <strong>Stochastic trajectories of mRNA counts.</strong> 100 stochastic trajectories generated with the Gillespie algorithm for mRNA counts over time for a two-state unregulated promoter. Cells spend a fraction of the cell cycle with a single copy of the promoter (light brown) and the rest of the cell cycle with two copies (light yellow). When trajectories reach a new cell cycle, the mRNA counts undergo a binomial partitioning to simulate the cell division. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS20.py">(<code>ch5_fig20.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<p>To check the implementation of our stochastic algorithm, we generated several of these stochastic trajectories to reconstruct the mRNA steady-state distribution. These reconstructed distributions for a single- and double-copy of the promoter can be compared with Eq. <span class="math inline">\(\ref{eq:two_state_mRNA}\)</span>–the steady-state distribution for the two-state promoter. Fig. 21 shows the excellent agreement between the stochastic simulation and the analytical result, confirming that our implementation of the Gillespie simulation is correct.</p>
<figure>
<img src="ch5_fig21.png" id="fig:ch5_fig21" data-short-caption="Comparison of analytical and simulated mRNA distribution" alt="Figure 21: Comparison of analytical and simulated mRNA distribution. Solid lines show the steady-state mRNA distributions for one copy (light blue) and two copies of the promoter (dark blue) as defined by Eq. \ref{eq:two_state_mRNA}. Shaded regions represent the corresponding distribution obtained using 2500 stochastic mRNA trajectories and taking the last cell-cycle to approximate the distribution. The Python code (ch5_fig21.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 21: <strong>Comparison of analytical and simulated mRNA distribution.</strong> Solid lines show the steady-state mRNA distributions for one copy (light blue) and two copies of the promoter (dark blue) as defined by Eq. <span class="math inline">\(\ref{eq:two_state_mRNA}\)</span>. Shaded regions represent the corresponding distribution obtained using 2500 stochastic mRNA trajectories and taking the last cell-cycle to approximate the distribution. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS21.py">(<code>ch5_fig21.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h3 id="protein-distribution-with-gillespie-simulations">Protein distribution with Gillespie simulations</h3>
<p>Having confirmed that our implementation of the Gillespie algorithm that includes the binomial partitioning of molecules reproduces analytical results, we extended the implementation to include protein counts. Fig. 22 shows representative trajectories for both mRNA and protein counts over several cell cycles. Especially for the protein, we can see that it takes several cell cycles for counts to converge to the dynamical steady-state observed with the deterministic moment equations. Once this steady-state is reached, the ensemble of trajectories between cell cycles looks very similar.</p>
<figure>
<img src="ch5_fig22.png" id="fig:ch5_fig22" data-short-caption="Stochastic trajectories of mRNA and protein counts" alt="Figure 22: Stochastic trajectories of mRNA and protein counts. 2500 protein counts over time for a two-state unregulated promoter. Cells spend a fraction of the cell cycle with a single copy of the promoter (light brown) and the rest of the cell cycle with two copies (light yellow). When trajectories reach a new cell cycle, the molecule counts undergo a binomial partitioning to simulate the cell division. The Python code (ch5_fig22.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 22: <strong>Stochastic trajectories of mRNA and protein counts.</strong> 2500 protein counts over time for a two-state unregulated promoter. Cells spend a fraction of the cell cycle with a single copy of the promoter (light brown) and the rest of the cell cycle with two copies (light yellow). When trajectories reach a new cell cycle, the molecule counts undergo a binomial partitioning to simulate the cell division. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS22.py">(<code>ch5_fig22.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<p>From these trajectories, we can compute the steady-state protein distribution, taking into account the cell-age distribution as detailed in XXX. Fig. 23 shows the comparison between this distribution and the one generated using the maximum entropy algorithm. Although the notorious differences between the distributions, the Gillespie simulation and the maximum entropy results are indistinguishable in terms of the mean, variance, and skewness of the distribution. We remind the reader that the maximum entropy is an approximation of the distribution that gets better the more moments we add. We, therefore, claim that the approximation works sufficiently well for our purpose. The enormous advantage of the maximum entropy approach comes from the computation time. For the number of distributions needed for our calculations, the Gillespie algorithm proved to be a very inefficient method given the ample sample space. Our maximum entropy approach reduces the computation time by several orders of magnitude, allowing us to explore different regulatory models’ parameters extensively.</p>
<figure>
<img src="ch5_fig23.png" id="fig:ch5_fig23" data-short-caption="Comparison of protein distributions" alt="Figure 23: Comparison of protein distributions. Comparison of the protein distribution generated with Gillespie stochastic simulations (blue curve) and the maximum entropy approach (orange curve). The upper panel shows the probability mass function. The lower panel compares the cumulative distribution functions. The Python code (ch5_fig23.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 23: <strong>Comparison of protein distributions.</strong> Comparison of the protein distribution generated with Gillespie stochastic simulations (blue curve) and the maximum entropy approach (orange curve). The upper panel shows the probability mass function. The lower panel compares the cumulative distribution functions. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS23.py">(<code>ch5_fig23.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h2 id="computational-determination-of-the-channel-capacity">Computational determination of the channel capacity</h2>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu//chann_cap/software/blahut_algorithm_channel_capacity.html">following link</a> as an annotated Jupyter notebook)</p>
<p>This section details the computation of the channel capacity of the simple genetic circuit shown in Fig. <strong>¿fig:ch3_fig05?</strong>. As detailed in XXX the channel capacity is defined as the mutual information between input <span class="math inline">\(c\)</span> and output <span class="math inline">\(p\)</span> maximized over all possible input distributions <span class="math inline">\(P(c)\)</span> <span class="citation" data-cites="Shannon1948"> [<a href="#ref-Shannon1948" role="doc-biblioref">33</a>]</span>. In principle, there is an infinite number of input distributions, so the task of finding <span class="math inline">\(\hat{P}(c)\)</span>, the input distribution at channel capacity, requires an algorithmic approach that guarantees the convergence to this distribution. Tkačik, Callan, and Bialek developed an analytical approximation to find the <span class="math inline">\(\hat{P}(c)\)</span> distribution <span class="citation" data-cites="Tkacik2008a"> [<a href="#ref-Tkacik2008a" role="doc-biblioref">34</a>]</span>. The validity of their so-called small noise approximation requires the standard deviation of the output distribution <span class="math inline">\(P(p \mid c)\)</span> to be much smaller than the distribution domain. For our particular case, such a condition is not satisfied given the spread of the inferred protein distributions shown in Fig. <strong>¿fig:ch3_fig04?</strong>.</p>
<p>Fortunately, a numerical algorithm can approximate <span class="math inline">\(\hat{P}(c)\)</span> for discrete distributions. In 1972 Richard Blahut and Suguru Arimoto independently came up with an algorithm mathematically shown to converge to <span class="math inline">\(\hat{P}(c)\)</span> <span class="citation" data-cites="Blahut1972"> [<a href="#ref-Blahut1972" role="doc-biblioref">35</a>]</span>. To compute both the theoretical and the experimental channel capacity shown in Fig. <strong>¿fig:ch3_fig05?</strong>, we implemented Blahut’s algorithm. In the following section, we detail the definitions needed for the algorithm. Then we describe how to compute the experimental channel capacity when the bins of the distribution are not clear given the arbitrary intrinsic nature of microscopy fluorescence measurements.</p>
<h3 id="blahuts-algorithm">Blahut’s algorithm</h3>
<p>Following <span class="citation" data-cites="Blahut1972"> [<a href="#ref-Blahut1972" role="doc-biblioref">35</a>]</span> we implemented the algorithm to compute the channel capacity. We define <span class="math inline">\(\mathbf{p_c}\)</span> to be an array containing the probability of each of the input inducer concentrations (twelve concentrations, See Methods). Each entry <span class="math inline">\(j\)</span> of the array is then of the form <span class="math display">\[
p_c^{(j)} = P(c = c_j),
\]</span> with <span class="math inline">\(j \in \{1, 2, \ldots, 12 \}\)</span>. The objective of the algorithm is to find the entries <span class="math inline">\(p_c^{(j)}\)</span> that maximize the mutual information between inputs and outputs. We also define <span class="math inline">\(\mathbf{Q}\)</span> to be a <span class="math inline">\(\vert \mathbf{p_c} \vert\)</span> by <span class="math inline">\(\vert \mathbf{p_{p \mid c}} \vert\)</span> matrix, where <span class="math inline">\(\vert \cdot \vert\)</span> specifies the length of the array, and <span class="math inline">\(\mathbf{p_{p \mid c}}\)</span> is an array containing the probability distribution of an output given a specific value of the input. In other words, the matrix <span class="math inline">\(\mathbf{Q}\)</span> recollects all of the individual output distribution arrays <span class="math inline">\(\mathbf{p_{p \mid c}}\)</span> into a single object. Then each entry of the matrix <span class="math inline">\(\mathbf{Q}\)</span> is of the form <span class="math display">\[
Q^{(i, j)} = P(p = p_i \mid c = c_j).
\]</span></p>
<p>For the case of the theoretical predictions of the channel capacity (Solid lines in Fig. <strong>¿fig:ch3_fig05?</strong>), the entries of the matrix <span class="math inline">\(\mathbf{Q}\)</span> are given by the inferred maximum entropy distributions as shown in Fig. <strong>¿fig:ch3_fig04?</strong>. In the next section, we will discuss how to define this matrix for the single-cell fluorescence measurements. Having defined these matrices, we proceed to implement the algorithm shown in Figure 1 of <span class="citation" data-cites="Blahut1972"> [<a href="#ref-Blahut1972" role="doc-biblioref">35</a>]</span>.</p>
<h3 id="channel-capacity-from-arbitrary-units-of-fluorescence">Channel capacity from arbitrary units of fluorescence</h3>
<p>A difficulty when computing the channel capacity between inputs and outputs from experimental data is that ideally, we would like to compute <span class="math display">\[
C(g; c) \equiv \sup_{P(c)} I(g; c),
\]</span> where <span class="math inline">\(g\)</span> is the gene expression level, and <span class="math inline">\(c\)</span> is the inducer concentration. But in reality we are computing <span class="math display">\[
C(f(g); c) \equiv \sup_{P(c)} I(f(g); c),
\]</span> where <span class="math inline">\(f(g)\)</span> is a function of gene expression that has to do with our mapping from the YFP copy number to some arbitrary fluorescent value as computed from the images were taken with the microscope. The data processing inequality, as derived by Shannon himself, tells us that for a Markov chain of the form <span class="math inline">\(c \rightarrow g \rightarrow f(g)\)</span> it must be true that <span class="citation" data-cites="Shannon1948"> [<a href="#ref-Shannon1948" role="doc-biblioref">33</a>]</span> <span class="math display">\[
I(g; c) \geq I(f(g); c),
\]</span> meaning that information can only be lost when mapping from the real relationship between gene expression and inducer concentration to a fluorescence value.</p>
<p>On top of that, given the limited number of samples that we have access to when computing the channel capacity, there is a bias in our estimate given this undersampling. The definition of accurate, unbiased descriptors of mutual information is still an area of active research. For our purposes, we will use the method described in <span class="citation" data-cites="Cheong2011a"> [<a href="#ref-Cheong2011a" role="doc-biblioref">36</a>]</span>. The basic idea of the method is to write the mutual information as a series expansion in terms of inverse powers of the sample size, i.e. <span class="math display">\[
I_{\text{biased}} = I_\infty + \frac{a_1}{N} + \frac{a_2}{N^2} + \cdots,
\]</span> where <span class="math inline">\(I_{\text{biased}}\)</span> is the biased estimate of the mutual information as computed from experimental data, <span class="math inline">\(I_\infty\)</span> is the quantity we would like to estimate, being the unbiased mutual information when having access to an infinity number of experimental samples and the coefficients <span class="math inline">\(a_i\)</span> depend on the underlying distribution of the signal and the response. This is an empirical choice to be tested. Intuitively this choice satisfies the limit that as the number of samples from the distribution grows, the empirical estimate of the mutual information <span class="math inline">\(I_{\text{biased}}\)</span> should get closer to the actual value <span class="math inline">\(I_\infty\)</span>.</p>
<p>In principle, for a good number of data points, the terms of higher-order become negligible. So we can write the mutual information as <span class="math display">\[
I_{\text{biased}} \approx I_\infty + \frac{a_1}{N} + \mathcal{O}(N^{-2}).
\label{eq:mutual_biased}
\]</span> This means that if this particular arbitrary choice of functional form is a good approximation, when computing the mutual information for varying numbers of samples - by taking subsamples of the experimental data - we expect to find a linear relationship as a function of the inverse of these number of data points. From this linear relationship, the intercept is a bias-corrected estimate of the mutual information. Therefore, we can bootstrap the data by taking different sample sizes and then use the Blahut-Arimoto algorithm we implemented earlier to estimate the biased channel capacity. We can then fit a line and extrapolate when <span class="math inline">\(1/N = 0\)</span>, which corresponds to our unbiased estimate of the channel capacity.</p>
<p>Let’s go through each of the steps to illustrate the method. Fig. 24 show a typical data set for a strain with an O2 binding site (<span class="math inline">\(\Delta\varepsilon_r = -13.9 \; k_BT\)</span>) and <span class="math inline">\(R = 260\)</span> repressors per cell. Each of the distributions in arbitrary units is binned into a specified number of bins to build matrix <span class="math inline">\(\mathbf{Q}\)</span>.</p>
<figure>
<img src="ch5_fig24.png" id="fig:ch5_fig24" data-short-caption="Single-cell fluorescence distributions for different inducer concentrations" alt="Figure 24: Single-cell fluorescence distributions for different inducer concentrations. Fluorescence distribution histogram (A) and cumulative distribution function (B) for a strain with 260 repressors per cell and a binding site with binding energy \Delta\varepsilon_r = -13.9\; k_BT. The different curves show the single-cell fluorescence distributions under the 12 different IPTG concentrations used throughout this work. The triangles in (A) show the mean of each of the distributions. The Python code (ch5_fig24.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 24: <strong>Single-cell fluorescence distributions for different inducer concentrations.</strong> Fluorescence distribution histogram (A) and cumulative distribution function (B) for a strain with 260 repressors per cell and a binding site with binding energy <span class="math inline">\(\Delta\varepsilon_r = -13.9\; k_BT\)</span>. The different curves show the single-cell fluorescence distributions under the 12 different IPTG concentrations used throughout this work. The triangles in (A) show the mean of each of the distributions. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS24.py">(<code>ch5_fig24.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<p>Given a specific number of bins used to construct <span class="math inline">\(\mathbf{Q}\)</span>, we subsample a fraction of the data and compute the channel capacity for such matrix using the Blahut-Arimoto algorithm. Fig. 25 shows an example where 50% of the data on each distribution from Fig. 24 was sampled and binned into 100 equal bins. The counts on each of these bins are then normalized and used to build matrix <span class="math inline">\(\mathbf{Q}\)</span> that is then fed to the Blahut-Arimoto algorithm. We can see that for these 200 bootstrap samples, the channel capacity varies by <span class="math inline">\(\approx\)</span> 0.1 bits. Not a significant variability; nevertheless, we consider it essential to bootstrap the data multiple times to estimate the channel capacity better.</p>
<figure>
<img src="ch5_fig25.png" id="fig:ch5_fig25" data-short-caption="Channel capacity bootstrap for experimental data" alt="Figure 25: Channel capacity bootstrap for experimental data. The cumulative distribution function of the resulting channel capacity estimates obtained by subsampling 200 times 50% of each distribution shown in Fig. 24, binning it into 100 bins, and feeding the resulting \mathbf{Q} matrix to the Blahut-Arimoto algorithm. The Python code (ch5_fig25.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 25: <strong>Channel capacity bootstrap for experimental data.</strong> The cumulative distribution function of the resulting channel capacity estimates obtained by subsampling 200 times 50% of each distribution shown in Fig. 24, binning it into 100 bins, and feeding the resulting <span class="math inline">\(\mathbf{Q}\)</span> matrix to the Blahut-Arimoto algorithm. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS25.py">(<code>ch5_fig25.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<p>Eq. <span class="math inline">\(\ref{eq:mutual_biased}\)</span> tells us that if we subsample each of the distributions from Fig. 24 at different fractions and plot them as a function of the inverse sample size, we will find a linear relationship if the expansion of the mutual information is valid. To test this idea, we repeated the bootstrap estimate of Fig. 25 sampling 10%, 20%, and so on until taking 100% of the data. We repeated this for different number of bins since <em>a priori</em> for arbitrary units of fluorescence, we do not have a way to select the optimal number of bins. Fig. 26 shows the result of these estimates. We can see that the linear relationship proposed in Eq. <span class="math inline">\(\ref{eq:mutual_biased}\)</span> holds for all number of bins selected. We also note that the value of the linear regression intercept varies depending on the number of bins.</p>
<figure>
<img src="ch5_fig26.png" id="fig:ch5_fig26" data-short-caption="Inverse sample size vs. channel capacity" alt="Figure 26: Inverse sample size vs. channel capacity. As indicated in Eq. \ref{eq:mutual_biased} if the channel capacity obtained for different subsample sizes of the data are plotted against the inverse sample size there must exist a linear relationship between these variables. Here we perform 15 bootstrap samples of the data from Fig. 24, bin these samples using different number of bins, and perform a linear regression (solid lines) between the bootstrap channel capacity estimates, and the inverse sample size. The Python code (ch5_fig26.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 26: <strong>Inverse sample size vs. channel capacity.</strong> As indicated in Eq. <span class="math inline">\(\ref{eq:mutual_biased}\)</span> if the channel capacity obtained for different subsample sizes of the data are plotted against the inverse sample size there must exist a linear relationship between these variables. Here we perform 15 bootstrap samples of the data from Fig. 24, bin these samples using different number of bins, and perform a linear regression (solid lines) between the bootstrap channel capacity estimates, and the inverse sample size. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS26.py">(<code>ch5_fig26.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<p>To address the variability in the estimates of the unbiased channel capacity <span class="math inline">\(I_\infty\)</span> we again follow the methodology suggested in <span class="citation" data-cites="Cheong2011a"> [<a href="#ref-Cheong2011a" role="doc-biblioref">36</a>]</span>. We perform the data subsampling and computation of the channel capacity for a varying number of bins. As a control, we perform the same procedure with shuffled data, where the structure that connects the fluorescence distribution to the inducer concentration input is lost. The expectation is that this control should give a channel capacity of zero if the data is not “over-binned.” Once the number of bins is too high, we would expect some structure to emerge in the data that would cause the Blahut-Arimoto algorithm to return non-zero channel capacity estimates.</p>
<p>Fig. 27 shows the result of the unbiased channel capacity estimates obtained for the data shown in Fig. 24. For the blue curve we can distinguish three phases: 1. A rapid increment from 0 bits to about 1.5 bits as the number of bins increases. 2. A flat region between <span class="math inline">\(\approx\)</span> 50 and 1000 bins. 3. A second rapid increment for a large number of bins.</p>
<p>We can see that the randomized data presents two phases only: 1. A flat region where there is, as expected, no information being processed since the structure of the data was lost when the data was shuffled. 2. A region with a fast growth of the channel capacity as the over-binning generates separated peaks on the distribution, making it look like there is a structure in the data.</p>
<p>We take the flat region of the experimental data (<span class="math inline">\(\approx\)</span> 100 bins) to be our best unbiased estimate of the channel capacity from this experimental dataset.</p>
<figure>
<img src="ch5_fig27.png" id="fig:ch5_fig27" data-short-caption="Channel capacity as a function of the number of bins" alt="Figure 27: Channel capacity as a function of the number of bins. Unbiased channel capacity estimates we obtained from linear regressions as in Fig. 26. The blue curve shows the estimates obtained from the data shown in Fig. 24. The orange curve is generated from estimates where the same data is shuffled, losing the relationship between fluorescence distributions and inducer concentration. The Python code (ch5_fig27.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 27: <strong>Channel capacity as a function of the number of bins.</strong> Unbiased channel capacity estimates we obtained from linear regressions as in Fig. 26. The blue curve shows the estimates obtained from the data shown in Fig. 24. The orange curve is generated from estimates where the same data is shuffled, losing the relationship between fluorescence distributions and inducer concentration. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS27.py">(<code>ch5_fig27.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h3 id="assumptions-involved-in-the-computation-of-the-channel-capacity">Assumptions involved in the computation of the channel capacity</h3>
<p>An interesting suggestion by Professor Gasper Tkacik was to dissect the different physical assumptions that went into the construction of the input-output function <span class="math inline">\(P(p \mid c)\)</span>, and their relevance when comparing the theoretical channel capacities with the experimental inferences. In what follows we describe the relevance of four important aspects that all affect the predictions of the information processing capacity.</p>
<h4 id="i-cell-cycle-variability.">(i) Cell cycle variability.</h4>
<p>We think that the inclusion of the gene copy number variability during the cell cycle and non-Poissoninan protein degradation is crucial to our estimation of the input-output functions and channel capacity. This variability in gene copy number is an additional source of noise that systematically decreases the system’s ability to resolve different inputs. The absence of the effects that the gene copy number variability and the protein partition have on the information processing capacity leads to an overestimate of the channel capacity, as shown in Fig. 28. When these noise sources are included in our inferences, we capture the experimental channel capacities with no additional fit parameters.</p>
<figure>
<img src="ch5_fig28.png" id="fig:ch5_fig28" data-short-caption="Comparison of channel capacity predictions for single- and multi-promoter models" alt="Figure 28: Comparison of channel capacity predictions for single- and multi-promoter models. Channel capacity for the multi-promoter model (solid lines) vs. the single-promoter steady-state model (dot-dashed lines) as a function of repressor copy numbers for different repressor-DNA binding energies. The single-promoter model assumes Poissonian protein degradation (\gamma _p &gt; 0) and steady-state, while the multi-promoter model accounts for gene copy number variability and during the cell cycle and has protein degradation as an effect due to dilution as cells grow and divide. The Python code (ch5_fig28.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 28: <strong>Comparison of channel capacity predictions for single- and multi-promoter models.</strong> Channel capacity for the multi-promoter model (solid lines) vs. the single-promoter steady-state model (dot-dashed lines) as a function of repressor copy numbers for different repressor-DNA binding energies. The single-promoter model assumes Poissonian protein degradation (<span class="math inline">\(\gamma _p &gt; 0\)</span>) and steady-state, while the multi-promoter model accounts for gene copy number variability and during the cell cycle and has protein degradation as an effect due to dilution as cells grow and divide. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS28.py">(<code>ch5_fig28.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h4 id="ii-non-gaussian-noise-distributions.">(ii) Non-Gaussian noise distributions.</h4>
<p>For the construction of the probability distributions used in the main text (Fig. <strong>¿fig:ch3_fig04?</strong>) we utilized the first six moments of the protein distribution. The maximum entropy formalism tells us that the more constraints we include in the inference, the closer the maximum entropy distribution will be to the real distribution. But <em>a priori</em> there is no way of knowing how many moments should be included to capture the distribution’s essence. In principle, two moments could suffice to describe the entire distribution as happens with the Gaussian distribution. To compare the effect that including more or fewer constraints on the maximum entropy inference, we constructed maximum entropy distributions using an increasing number of moments from 2 to 6. We then computed the Kullback-Leibler divergence <span class="math inline">\(D_{KL}\)</span> of the form <span class="math display">\[
D_{KL}(P_6(p \mid c) || P_i(p \mid c)) =
    \sum_p P_6(p \mid c) \log_2 \frac{P_6(p \mid c)}{P_i(p \mid c)},
\]</span> where <span class="math inline">\(P_i(p \mid c)\)</span> is the maximum entropy distribution constructed with the first <span class="math inline">\(i\)</span> moments, <span class="math inline">\(i \in \{2, 3, 4, 5, 6\}\)</span>. Since the Kullback-Leibler divergence <span class="math inline">\(D_{KL}(P || Q)\)</span> can be interpreted as the amount of information lost by assuming the incorrect distribution <span class="math inline">\(Q\)</span> when the correct distribution is <span class="math inline">\(P\)</span>, we used this metric as a way of how much information we would have lost by using less constraints compared to the six moments used in the main text.</p>
<p>Fig. 29 shows this comparison for different operators and repressor copy numbers. We can see from here that using fewer moments as constraints gives the same result. This is because most of the values of the Kullback-Leibler divergence is significantly smaller than 0.1 bits. The entropy of these distributions is, in general, <span class="math inline">\(&gt; 10\)</span> bits, so we would lose less than 1% of the information contained in these distributions by utilizing only two moments as constraints. Therefore the use of non-Gaussian noise is not an essential feature for our inferences.</p>
<figure>
<img src="ch5_fig29.png" id="fig:ch5_fig29" data-short-caption="Measuring the loss of information by using a different number of constraints" alt="Figure 29: Measuring the loss of information by using a different number of constraints. The Kullback-Leibler divergence was computed between the maximum entropy distribution constructed using the first six moments of the distribution and a variable number of moments. The Python code (ch5_fig29.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 29: <strong>Measuring the loss of information by using a different number of constraints.</strong> The Kullback-Leibler divergence was computed between the maximum entropy distribution constructed using the first six moments of the distribution and a variable number of moments. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS29.py">(<code>ch5_fig29.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h4 id="iii-multi-state-promoter.">(iii) Multi-state promoter.</h4>
<p>This particular point is something that we are still exploring from a theoretical perspective. We have shown that to capture the single-molecule mRNA FISH data, a single-state promoter wouldn’t suffice. This model predicts a Poisson distribution as the steady-state and the data shows super Poissonian noise. Given the bursty nature of gene expression, we opt to use a two-state promoter to reflect effective transcriptionally “active” and “inactive” states. We are currently exploring alternative formulations of this model to turn it into a single state with a geometrically distributed burst size.</p>
<h4 id="iv-optimal-vs-log-flat-distributions.">(iv) Optimal vs Log-flat Distributions.</h4>
<p>The relevance of having to use the Blahut-Arimoto algorithm to predict the maximum mutual information between input and outputs was to understand the best-case scenario. We show the comparison between theoretical and experimental input-output functions <span class="math inline">\(P(p \mid c)\)</span> in Fig. 19. Given the good agreement between these distributions, we could compute the mutual information <span class="math inline">\(I(c; p)\)</span> for any arbitrary input distribution <span class="math inline">\(P(c)\)</span> and obtain a good agreement with the corresponding experimental mutual information.</p>
<p>The reason we opted to report the mutual information at the channel capacity was to put the results in context. By reporting the upper bound in performance of these genetic circuits, we can start to dissect how different molecular parameters such as repressor-DNA binding affinity or repressor copy number affect the ability of this genetic circuit to extract information from the environmental state.</p>
<h2 id="empirical-fits-to-noise-predictions">Empirical fits to noise predictions</h2>
<p>(Note: The Python code used for the calculations presented in this section can be found in the <a href="https://www.rpgroup.caltech.edu/chann_cap/src/theory/html/empirical_constants.html">following link</a> as an annotated Jupyter notebook)</p>
<p>In <a href="C">Fig:ch3_fig03</a> in the main text, we show that our minimal model has a systematic deviation on the gene expression noise predictions compared to the experimental data. This systematics will need to be addressed on an improved version of the minimal model presented in this work. To guide the insights into the origins of this systematic deviation in this appendix, we will explore the model’s empirical modifications to improve the agreement between theory and experiment.</p>
<h3 id="multiplicative-factor-for-the-noise">Multiplicative factor for the noise</h3>
<p>The first option we will explore is to modify our noise predictions by a constant multiplicative factor. This means that we assume the relationship between our minimal model predictions and the data for noise in gene expression are of the form <span class="math display">\[
\text{noise}_{\text{exp}} = \alpha \cdot \text{noise}_{\text{theory}},
\]</span> where <span class="math inline">\(\alpha\)</span> is a dimensionless constant to be fit from the data. The data, especially in Fig. 12 suggests that our predictions are within a factor of <span class="math inline">\(\approx\)</span> two from the experimental data. To further check that intuition, we performed a weighted linear regression between the experimental and theoretical noise measurements. The weight for each datum was proportional to the bootstrap errors in the noise estimate; this to have poorly determined noises weigh less during the regression. This regression with no intercept shows that a factor of two systematically improves the theoretical vs. experimental predictions. Fig. 30 shows the improved agreement when the noise’s theoretical predictions are multiplied by <span class="math inline">\(\approx 1.5\)</span>.</p>
<figure>
<img src="ch5_fig30.png" id="fig:ch5_fig30" data-short-caption="Multiplicative factor in improving theoretical vs. experimental comparison of noise in gene expression" alt="Figure 30: Multiplicative factor in improving theoretical vs. experimental comparison of noise in gene expression. Theoretical vs. experimental noise both in linear (left) and log (right) scale. The dashed line shows the identity line of slope 1 and intercept zero. All data are colored by the experimental fold-change’s corresponding value in gene expression as indicated by the color bar. The x-axis was multiplied by a factor of \approx 1.5 as determined by linear regression from the data in Fig. 11. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. The Python code (ch5_fig30.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 30: <strong>Multiplicative factor in improving theoretical vs. experimental comparison of noise in gene expression.</strong> Theoretical vs. experimental noise both in linear (left) and log (right) scale. The dashed line shows the identity line of slope 1 and intercept zero. All data are colored by the experimental fold-change’s corresponding value in gene expression as indicated by the color bar. The <span class="math inline">\(x\)</span>-axis was multiplied by a factor of <span class="math inline">\(\approx 1.5\)</span> as determined by linear regression from the data in Fig. 11. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS30.py">(<code>ch5_fig30.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<p>For completeness Fig. 31 shows the noise in gene expression as a function of the inducer concentration, including this factor of <span class="math inline">\(\approx 1.5\)</span>. Overall a simple multiplicative factor improves the predictive power of the model.</p>
<figure>
<img src="ch5_fig31.png" id="fig:ch5_fig31" data-short-caption="Protein noise of the regulated promoter with multiplicative factor" alt="Figure 31: Protein noise of the regulated promoter with multiplicative factor. Comparison of the experimental noise for different operators ((A) O1, \Delta\varepsilon_r = -15.3 \; k_BT, (B) O2, \Delta\varepsilon_r = -13.9 \; k_BT, (C) O3, \Delta\varepsilon_r = -9.7 \; k_BT) with the theoretical predictions for the multi-promoter model. Linear regression revealed that multiplying the theoretical noise prediction by a factor of \approx 1.5 would improve agreement between theory and data. Points represent the experimental noise as computed from single-cell fluorescence measurements of different E. coli strains under 12 different inducer concentrations. Dotted line indicates the plot in linear rather than logarithmic scale. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. White-filled dots are plot at a different scale for better visualization. The Python code (ch5_fig31.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 31: <strong>Protein noise of the regulated promoter with multiplicative factor.</strong> Comparison of the experimental noise for different operators ((A) O1, <span class="math inline">\(\Delta\varepsilon_r = -15.3 \; k_BT\)</span>, (B) O2, <span class="math inline">\(\Delta\varepsilon_r = -13.9 \; k_BT\)</span>, (C) O3, <span class="math inline">\(\Delta\varepsilon_r = -9.7 \; k_BT\)</span>) with the theoretical predictions for the multi-promoter model. Linear regression revealed that multiplying the theoretical noise prediction by a factor of <span class="math inline">\(\approx 1.5\)</span> would improve agreement between theory and data. Points represent the experimental noise as computed from single-cell fluorescence measurements of different <em>E. coli</em> strains under 12 different inducer concentrations. Dotted line indicates the plot in linear rather than logarithmic scale. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. White-filled dots are plot at a different scale for better visualization. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS31.py">(<code>ch5_fig31.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h3 id="additive-factor-for-the-noise">Additive factor for the noise</h3>
<p>As an alternative way to empirically improve our model’s predictions, we will now test the idea of an additive constant. What this means is that our minimal model underestimates the noise in gene expression as <span class="math display">\[
\text{noise}_{\text{exp}} = \beta + \text{noise}_{\text{theory}},
\]</span> where <span class="math inline">\(\beta\)</span> is an additive constant to be determined from the data. As with the multiplicative constant, we performed a regression to determine this empirical additive constant comparing experimental and theoretical gene expression noise values. We use the error in the 95% bootstrap confidence interval as a weight for the linear regression. Fig. 32 shows the resulting theoretical vs. experimental noise where <span class="math inline">\(\beta \approx 0.2\)</span>. We can see a great improvement in the agreement between theory and experiment with this additive constant.</p>
<figure>
<img src="ch5_fig32.png" id="fig:ch5_fig32" data-short-caption="Additive factor in improving theoretical vs. experimental comparison of noise in gene expression" alt="Figure 32: Additive factor in improving theoretical vs. experimental comparison of noise in gene expression. Theoretical vs. experimental noise both in linear (left) and log (right) scale. The dashed line shows the identity line of slope 1 and intercept zero. All data are colored by the experimental fold-change’s corresponding value in gene expression as indicated by the color bar. A value of \approx 0.2 was added to all values in the x-axis as determined by linear regression from the data in Fig. 11. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. The Python code (ch5_fig32.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 32: <strong>Additive factor in improving theoretical vs. experimental comparison of noise in gene expression.</strong> Theoretical vs. experimental noise both in linear (left) and log (right) scale. The dashed line shows the identity line of slope 1 and intercept zero. All data are colored by the experimental fold-change’s corresponding value in gene expression as indicated by the color bar. A value of <span class="math inline">\(\approx 0.2\)</span> was added to all values in the <span class="math inline">\(x\)</span>-axis as determined by linear regression from the data in Fig. 11. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS32.py">(<code>ch5_fig32.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<p>For completeness Fig. 33 shows the noise in gene expression as a function of the inducer concentration, including this additive factor of <span class="math inline">\(\beta \approx 0.2\)</span>. If anything, the additive factor seems to improve the agreement between theory and data even more than the multiplicative factor.</p>
<figure>
<img src="ch5_fig33.png" id="fig:ch5_fig33" data-short-caption="Protein noise of the regulated promoter with an additive factor" alt="Figure 33: Protein noise of the regulated promoter with an additive factor. Comparison of the experimental noise for different operators ((A) O1, \Delta\varepsilon_r = -15.3 \; k_BT, (B) O2, \Delta\varepsilon_r = -13.9 \; k_BT, (C) O3, \Delta\varepsilon_r = -9.7 \; k_BT) with the theoretical predictions for the multi-promoter model. Linear regression revealed that an additive factor of \approx 0.2 to the theoretical noise prediction would improve agreement between theory and data. Points represent the experimental noise as computed from single-cell fluorescence measurements of different E. coli strains under 12 different inducer concentrations. The dotted line indicates the plot in linear rather than logarithmic scale. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with \geq 300 cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. White-filled dots are plot at a different scale for better visualization. The Python code (ch5_fig33.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 33: <strong>Protein noise of the regulated promoter with an additive factor.</strong> Comparison of the experimental noise for different operators ((A) O1, <span class="math inline">\(\Delta\varepsilon_r = -15.3 \; k_BT\)</span>, (B) O2, <span class="math inline">\(\Delta\varepsilon_r = -13.9 \; k_BT\)</span>, (C) O3, <span class="math inline">\(\Delta\varepsilon_r = -9.7 \; k_BT\)</span>) with the theoretical predictions for the multi-promoter model. Linear regression revealed that an additive factor of <span class="math inline">\(\approx 0.2\)</span> to the theoretical noise prediction would improve agreement between theory and data. Points represent the experimental noise as computed from single-cell fluorescence measurements of different <em>E. coli</em> strains under 12 different inducer concentrations. The dotted line indicates the plot in linear rather than logarithmic scale. Each datum represents a single date measurement of the corresponding strain and IPTG concentration with <span class="math inline">\(\geq 300\)</span> cells. The points correspond to the median, and the error bars correspond to the 95% confidence interval as determined by 10,000 bootstrap samples. White-filled dots are plot at a different scale for better visualization. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS33.py">(<code>ch5_fig33.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h3 id="correction-factor-for-channel-capacity-with-a-multiplicative-factor">Correction factor for channel capacity with a multiplicative factor</h3>
<p>A constant multiplicative factor can reduce the discrepancy between the model predictions and the data concerning the noise (standard deviation/mean) in protein copy numbers. Finding the equivalent correction for the channel capacity requires gaining insights from the so-called small noise approximation <span class="citation" data-cites="Tkacik2008a"> [<a href="#ref-Tkacik2008a" role="doc-biblioref">34</a>]</span>. The small noise approximation assumes that the input-output function can be modeled as a Gaussian distribution in which the standard deviation is small. Using these assumptions, one can derive a closed-form for the channel capacity. Although our data and model predictions do not satisfy the small noise approximation requirements, we can gain some intuition for how the channel capacity would scale given a systematic deviation in the cell-to-cell variability predictions compared with the data.</p>
<p>Using the small noise approximation, one can derive the form of the input distribution at channel capacity <span class="math inline">\(P^*(c)\)</span>. To do this, we use the fact that there is a deterministic relationship between the input inducer concentration <span class="math inline">\(c\)</span> and the mean output protein value <span class="math inline">\(\left\langle p \right\rangle\)</span>, therefore we can work with <span class="math inline">\(P(\left\langle p \right\rangle)\)</span> rather than <span class="math inline">\(P(c)\)</span> since the deterministic relation allows us to write <span class="math display">\[
P(c) dc = P(\left\langle p \right\rangle) d\left\langle p \right\rangle.
\]</span> Optimizing over all possible distributions <span class="math inline">\(P(\left\langle p \right\rangle)\)</span> using calculus of variations results in a distribution of the form <span class="math display">\[
P^*(\left\langle p \right\rangle) = 
\frac{1}{\mathcal{Z}} \frac{1}{\sigma_p(\left\langle p \right\rangle)},
\]</span> where <span class="math inline">\(\sigma_p(\left\langle p \right\rangle)\)</span> is the standard deviation of the protein distribution as a function of the mean protein expression, and <span class="math inline">\(\mathcal{Z}\)</span> is a normalization constant defined as <span class="math display">\[
\mathcal{Z} \equiv 
\int_{\left\langle{p(c=0)}\right\rangle}
^{\left\langle{p(c\rightarrow \infty)}\right\rangle}
\frac{1}{\sigma_p(\left\langle p \right\rangle)} d\left\langle p \right\rangle.
\]</span> Under these assumptions, the small noise approximation tells us that the channel capacity is of the form <span class="citation" data-cites="Tkacik2008a"> [<a href="#ref-Tkacik2008a" role="doc-biblioref">34</a>]</span> <span class="math display">\[
I = \log_2 \left( \frac{\mathcal{Z}}{\sqrt{2 \pi e}} \right).
\]</span></p>
<p>From the theory-experiment comparison in, we know that the standard deviation predicted by our model is systematically off by a factor of two compared to the experimental data, i.e. <span class="math display">\[
\sigma_p^{\exp} = 2 \sigma_p^{\text{theory}}.
\]</span> This then implies that the normalization constant <span class="math inline">\(\mathcal{Z}\)</span> between theory and experiment must follow a relationship of the form <span class="math display">\[
\mathcal{Z}^{\exp} = \frac{1}{2} \mathcal{Z}^{\text{theory}}.
\]</span> With this relationship, the small noise approximation would predict that the difference between the experimental and theoretical channel capacity should be of the form <span class="math display">\[
I^{\exp} = \log_2 \left( \frac{\mathcal{Z}^{\exp}}{\sqrt{2 \pi e}} \right)
= \log_2 \left( \frac{\mathcal{Z}^{\text{theory}}}{\sqrt{2 \pi e}} \right)
- \log_2(2).
\]</span> Therefore under the small noise approximation, we would expect our predictions for the channel capacity to be off by a constant of 1 bit (<span class="math inline">\(\log_2(2)\)</span>) of information. Again, the conditions for the small noise approximation do not apply to our data given the intrinsic level of cell-to-cell variability in the system; nevertheless, what this analysis tells us is that we expect that an additive constant should be able to explain the discrepancy between our model predictions and the experimental channel capacity. To test this hypothesis, we performed a “linear regression” between the model predictions and the experimental channel capacity with a fixed slope of 1. The intercept of this regression, -0.56 bits, indicates the systematic deviation we expect should explain the difference between our model and the data. Fig. 34 shows the comparison between the original predictions shown in Fig. 5(A) and the resulting predictions with this shift. Other than the data with zero channel capacity, this shift can correct the systematic deviation for all data. We, therefore, conclude that our model ends up underestimating the experimentally determined channel capacity by a constant amount of 0.43 bits.</p>
<figure>
<img src="ch5_fig34.png" id="fig:ch5_fig34" data-short-caption="Additive correction factor for channel capacity" alt="Figure 34: Additive correction factor for channel capacity. Solid lines represent the theoretical predictions of the channel capacity shown in (A). The dashed lines show the resulting predictions with a constant shift of -0.43 bits. Points represent single biological replicas of the inferred channel capacity. The Python code (ch5_fig34.py) used to generate this figure can be found on the original paper GitHub repository.." /><figcaption aria-hidden="true">Figure 34: <strong>Additive correction factor for channel capacity.</strong> Solid lines represent the theoretical predictions of the channel capacity shown in (A). The dashed lines show the resulting predictions with a constant shift of -0.43 bits. Points represent single biological replicas of the inferred channel capacity. The Python code <a href="https://github.com/RPGroup-PBoC/chann_cap/blob/master/src/figs/figS34.py">(<code>ch5_fig34.py</code>)</a> used to generate this figure can be found on the original paper <a href="https://github.com/RPGroup-PBoC/chann_cap">GitHub repository.</a>.</figcaption>
</figure>
<h2 id="derivation-of-the-steady-state-mrna-distribution">Derivation of the steady-state mRNA distribution</h2>
<p>In this section, we will derive the two-state promoter mRNA distribution we quote in XXX. For this method, we will make use of the so-called generating functions. Generating functions are mathematical objects on which we can encode a series of infinite numbers as coefficients of a power series. The power of generating functions comes from the fact that we can convert an infinite-dimensional system of coupled ordinary differential equations–in our case the system of differential equations defining all probabilities <span class="math inline">\(P(m, t)\)</span> for <span class="math inline">\(m \in \mathbb{Z}\)</span>–into a single partial differential equation that we can then solve to extract back the probability distributions.</p>
<p>To motivate the use of generating functions, we will begin with the simplest case: the one-state Poisson promoter.</p>
<h3 id="one-state-poisson-promoter">One-state Poisson promoter</h3>
<p>We begin by defining the reaction scheme that defines the one-state promoter. Fig. 35 shows the schematic representation of the Poisson promoter as a simple cartoon (part (A)) and as the Markov chain that defines the state space of the system (part (B)).</p>
<figure>
<img src="ch5_fig35.png" id="fig:ch5_fig35" data-short-caption="One-state Poisson promoter" alt="Figure 35: One-state Poisson promoter. (A) Schematic of the kinetics of the one state-promoter. mRNA is produced and degrade stochastically with a rate r_m and \gamma_m, respectively. (B) Representation of the Markov-chain for the state space that the promoter can be. The distribution P(m, t) represents the probability of having a certain discrete number of mRNA m at time t. The transition between states depends on the previously mentioned rates." /><figcaption aria-hidden="true">Figure 35: <strong>One-state Poisson promoter.</strong> (A) Schematic of the kinetics of the one state-promoter. mRNA is produced and degrade stochastically with a rate <span class="math inline">\(r_m\)</span> and <span class="math inline">\(\gamma_m\)</span>, respectively. (B) Representation of the Markov-chain for the state space that the promoter can be. The distribution <span class="math inline">\(P(m, t)\)</span> represents the probability of having a certain discrete number of mRNA <span class="math inline">\(m\)</span> at time <span class="math inline">\(t\)</span>. The transition between states depends on the previously mentioned rates.</figcaption>
</figure>
<p>The dynamics of the probability distribution <span class="math inline">\(P(m, t)\)</span> are governed by the chemical master equation <span class="math display">\[
\frac{d P(m, t)}{dt} = 
\overbrace{r_m P(m-1, t)}^{m-1 \rightarrow m}
- \overbrace{r_m P(m, t)}^{m \rightarrow m+1}
+ \overbrace{\gamma_m (m+1) P(m+1, t)}^{m+1 \rightarrow m}
- \overbrace{\gamma_m m P(m, t)}^{m \rightarrow m-1}.
\label{eq:one_state_master}
\]</span></p>
<p>When solving for the distribution, our objective is to obtain the equation that defines <span class="math inline">\(P(m, t)\)</span> for all possible values of <span class="math inline">\(m \in \mathbb{Z}\)</span>. The power of the generating functions is that these probability distribution values are used as a power series’s coefficients. To make this clear, let us define the generating function <span class="math inline">\(G(z, t)\)</span> as <span class="math display">\[
G(z, t) \equiv \sum_{m=0}^\infty z^m P(m, t),
\label{eq:gen_fn_def}
\]</span> where <span class="math inline">\(z\)</span> is a “dummy” variable that we don’t care about. The reason this is useful is that if we find the closed-form solution for this generating function, and we are able to split the factor <span class="math inline">\(z^m\)</span> from its coefficient <span class="math inline">\(P(m, t)\)</span>, then we will have find the solution for the distribution. Furthermore, the generating function allows us to compute the moments of the distribution. For example, for the zeroth moment <span class="math inline">\(\langle m^0 \rangle\)</span> we know that <span class="math display">\[
\langle m^0 \rangle = \sum_{m=0}^\infty m^0 P(m, t) = 1,
\]</span> i.e., this is the normalization constraint of the distribution. From the definition of the generating function we can then see that <span class="math display">\[
G(1, t) = \sum_{m=0}^\infty 1^m P(m, t) = 1.
\label{eq:generating_norm}
\]</span> Furthermore, the first moment of the distribution is defined as <span class="math display">\[
\langle m \rangle = \sum_{m=0}^\infty m P(m, t).
\]</span> From the definition of the generating function, we can construct this quantity by computing <span class="math display">\[
\left. \frac{\partial G(z, t)}{\partial z} \right\vert_{z=1} =
\frac{\partial}{\partial z} \left[ 
\sum_{m=0}^\infty z^m P(m, t)
\right]_{z=1} =
\sum_{m=0}^\infty m P(m, t).
\]</span> Therefore we have that <span class="math display">\[
\langle m \rangle = 
\left. \frac{\partial G(z, t)}{\partial z} \right\vert_{z=1}.
\label{eq:first_mom_gen}
\]</span> Similar constructions can be built for higher moments of the distribution.</p>
<p>Let us then apply the definition of the generating function to Eq. <span class="math inline">\(\ref{eq:one_state_master}\)</span>. For this, we multiply both sides by <span class="math inline">\(z^m\)</span> and sum over all values of <span class="math inline">\(m\)</span>, obtaining <span class="math display">\[
\begin{split}
\sum_{m=0}^\infty z^m
\frac{d P(m, t)}{dt} &amp;=
\sum_{m=0}^\infty z^m \left[
r_m P(m-1, t)
- r_m P(m, t) \right. \\
&amp;+ \left. \gamma_m (m+1) P(m+1, t)
- \gamma_m m P(m, t).
\right]
\end{split}
\]</span> Distributing the sum, we find <span class="math display">\[
\begin{split}
\frac{d}{dt} \sum_{m=0}^\infty z^m P(m, t) &amp;=
\sum_{m=0}^\infty z^m r_m P(m-1, t)
- \sum_{m=0}^\infty z^m r_m P(m, t) \\
&amp;+ \sum_{m=0}^\infty z^m \gamma_m (m+1) P(m+1, t)
- \sum_{m=0}^\infty z^m \gamma_m m P(m, t).
\end{split}
\label{eq:one_state_master_sum}
\]</span> We see that the terms involving <span class="math inline">\(z^m P(m, t)\)</span> can be directly substituted with Eq. <span class="math inline">\(\ref{eq:gen_fn_def}\)</span>. For the other terms, we have to be slightly more clever. The first trick will allow us to rewrite the term involving <span class="math inline">\(z^m m P(m, t)\)</span> as <span class="math display">\[
\begin{aligned}
\sum_{m} z^{m} \cdot m \cdot P(m, t) &amp;=
\sum_{m} z \frac{\partial z^{m}}{\partial z} P(m, t), \\
&amp;=\sum_{m} z \frac{\partial}{\partial z}\left(z^{m} P(m, t)\right), \\
&amp;=z \frac{\partial}{\partial z}\left(\sum_{m} z^{m} P(m, t)\right), \\
&amp;=z \frac{\partial G(z, t)}{\partial z}.
\end{aligned}
\]</span> Next, let us deal with the term involving <span class="math inline">\((m+1)\)</span>. We first define <span class="math inline">\(k = m + 1\)</span>. With this, we can write <span class="math display">\[
\begin{aligned}
\sum_{m=0}^{\infty} z^{m} \cdot(m+1) \cdot P(m+1, t) &amp;=
\sum_{k=1}^{\infty} z^{k-l} \cdot k \cdot P(k, t), \\
&amp;=z^{-1} \sum_{k=1}^{\infty} z^{k} \cdot k \cdot P(k, t), \\
&amp;=z^{-1} \sum_{k=0}^{\infty} z^{k} \cdot k \cdot P(k, t), \\
&amp;=z^{-1}\left(z \frac{ \partial G(z)}{\partial z}\right), \\
&amp;=\frac{\partial G(z)}{\partial z},
\end{aligned}
\]</span> where for the third step, we reindexed the sum to include <span class="math inline">\(k=0\)</span> since it does not contribute to the total sum. Finally, for the term involving <span class="math inline">\(P(m-1, t)\)</span>. For this we define <span class="math inline">\(k = m-1\)</span>. This allows us to rewrite the term as <span class="math display">\[
\begin{aligned}
\sum_{m=0}^{\infty} z^{m} P(m-1, t) &amp;=\sum_{k=-1}^{\infty} z^{k+1} P(k, t), \\
&amp;=\sum_{k=0}^{\infty} z^{k+1} P(k, t), \\
&amp;=z \sum_{k=0}^{\infty} z^{k} P(k, t), \\
&amp;=z G(z, t)
\end{aligned}
\]</span> For the second step we reindexed the sum from <span class="math inline">\(-1\)</span> to <span class="math inline">\(0\)</span> since <span class="math inline">\(P(-1, t) = 0\)</span>.</p>
<p>All of these clever reindexing allows us to rewrite Eq. <span class="math inline">\(\ref{eq:one_state_master_sum}\)</span> as <span class="math display">\[
\frac{\partial G(z, t)}{\partial t} =
r z G(z, t)-r G(z, t)
+ \gamma \frac{\partial G(z, t)}{\partial z}
- \gamma z \frac{\partial G(z, t)}{\partial z}.
\]</span> Factorizing terms we have <span class="math display">\[
\frac{\partial G(z, t)}{\partial t} 
=-r G(z, t)(1-z)
+\gamma \frac{\partial G(z, t)}{\partial z}(1-z).
\]</span> Let us appreciate how beautiful this is: we took an infinite-dimensional system of ordinary differential equations–the master equation–and turn it into a single partial differential equation (PDE). All we have to do now is solve this PDE, and then transform the solution into a power series to extract the distribution.</p>
<p>Let us focus on the steady-state case. For this, we set the time derivative to zero. Doing this cancels the <span class="math inline">\((1-z)\)</span> term, leaving a straightforward ordinary differential equation for <span class="math inline">\(G(z)\)</span> <span class="math display">\[
\frac{dG(z)}{dz} = \frac{r}{\gamma} G(z).
\]</span> Solving this equation by separation of variables results in <span class="math display">\[
G(z) = C e^{\frac{r}{\gamma}z}.
\]</span> To obtain the integration constant, we use the normalization condition of the probability distribution (Eq. <span class="math inline">\(\ref{eq:generating_norm}\)</span>), obtaining <span class="math display">\[
1 = C e^{\frac{r}{\gamma}} \Rightarrow
C = e^{-\frac{r}{\gamma}}.
\]</span> This means that the generating function takes the form <span class="math display">\[
G(z) = e^{-\frac{r}{\gamma}} e^{\frac{r}{\gamma}z}.
\]</span> All we have left is trying to rewrite the generating function as a power series on <span class="math inline">\(z\)</span>. If we succeed in doing so, we will have recovered the probability distribution <span class="math inline">\(P(m, t)\)</span>. For this, we simply use the Taylor expansion of <span class="math inline">\(e^x\)</span>, obtaining <span class="math display">\[
G(z) = e^{-\frac{r}{\gamma}} 
\sum_{m=0}^\infty \frac{\left( \frac{r}{\gamma}z \right)^m}{m!}.
\]</span> From this form, it becomes clear how to split the <span class="math inline">\(z^m\)</span> term from the coefficient that, by the definition of the generating function, is the probability distribution we are looking for. The separation takes the form <span class="math display">\[
G(z)=\sum_{m=0}^{\infty} z^{m}
\left[\frac{e^{-r / \gamma}\left(\frac{r}{\gamma}\right)^{m}}{m !}\right],
\]</span> where we can see that we recover the expected Poisson distribution for this one-state promoter <span class="math display">\[
P(m) = e^{-r / \gamma}\frac{\left(\frac{r}{\gamma}\right)^{m}}{m !}.
\]</span></p>
<h3 id="two-state-promoter">Two-state promoter</h3>
<p>Having shown the generating function’s power, let us now turn our attention to the relevant equation we are after: the two-state mRNA distribution. This model assumes that the promoter can exist in two discrete states (See Fig. 36(A)): a transcriptionally active state <span class="math inline">\(A\)</span> from which transcription can take place at a constant rate <span class="math inline">\(r_m\)</span>, and an inactive state <span class="math inline">\(I\)</span> where no transcription takes place. The mRNA is stochastically degraded with a rate <span class="math inline">\(\gamma_m\)</span> regardless of the state of the promoter. Fig. 36(B) shows the Markov chain that connects all of the possible states of the promoter. For this particular case, there are not only “horizontal” transitions where the mRNA copy number changes, but “vertical” transitions where only the promoter’s state changes. Because of this, we need to define two coupled master equations that take the form <span class="math display">\[
\begin{aligned}
\frac{d P_{A}(m, t)}{d t} &amp;=-k^{(p)}_{\text{off}} P_{A}(m, t) +
k^{(p)}_{\text{on}} P_{I}(m, t) \\
&amp;+\gamma_m (m+1) P_{A}(m+1, t) - \gamma_m m P_{n}(m, t) \\
&amp;+r_m P_{A}(m-1, t)-r_m P_{A}(m, t) \\
\end{aligned}
\]</span> for the active state, and <span class="math display">\[
\begin{aligned}
\frac{d P_{I}(m, t)}{dt} &amp;=k^{(p)}_{\text{off}} P_{A}(m, t)-
k^{(p)}_{\text{on}} P_{I}(m, t) \\
&amp;+\gamma_m (m+1) P_{I}(m+1, t)-\gamma_m m P_{I}(m, t),
\end{aligned}
\]</span> for the inactive state.</p>
<figure>
<img src="ch5_fig36.png" id="fig:ch5_fig36" data-short-caption="One-state Poisson promoter" alt="Figure 36: Two-state Poisson promoter. (A) Schematic of the kinetics of the two-state promoter. The promoter is imagined to exist in two-state–a transcriptionally active state A and an inactive state I. The transition between these states is governed by the rates k^{(p)}_{\text{on}} and k^{(p)}_{\text{off}} mRNA is produced and degrade stochastically with a rate r_m and \gamma_m, respectively. (B) Representation of the Markov-chain for the state space that the promoter can be in. The distribution P(m, t) represents the probability of having a certain discrete number of mRNA m at time t. The transition between states depends on the previously mentioned rates." /><figcaption aria-hidden="true">Figure 36: <strong>Two-state Poisson promoter.</strong> (A) Schematic of the kinetics of the two-state promoter. The promoter is imagined to exist in two-state–a transcriptionally active state <span class="math inline">\(A\)</span> and an inactive state <span class="math inline">\(I\)</span>. The transition between these states is governed by the rates <span class="math inline">\(k^{(p)}_{\text{on}}\)</span> and <span class="math inline">\(k^{(p)}_{\text{off}}\)</span> mRNA is produced and degrade stochastically with a rate <span class="math inline">\(r_m\)</span> and <span class="math inline">\(\gamma_m\)</span>, respectively. (B) Representation of the Markov-chain for the state space that the promoter can be in. The distribution <span class="math inline">\(P(m, t)\)</span> represents the probability of having a certain discrete number of mRNA <span class="math inline">\(m\)</span> at time <span class="math inline">\(t\)</span>. The transition between states depends on the previously mentioned rates.</figcaption>
</figure>
<h4 id="obtaining-the-partial-differential-equation-for-the-generating-function">Obtaining the partial differential equation for the generating function</h4>
<p>The first thing we must do is to transform this infinite-dimensional system of ordinary differential equations in <span class="math inline">\(m\)</span> to a single partial differential equation using the generating function. For this particular case, there are two generating functions of the form <span class="math display">\[
G_x(z, t) = \sum_{m=0}^\infty z^m P_x(m, t),
\]</span> where <span class="math inline">\(x \in \{A, I \}\)</span>. The probability of having <span class="math inline">\(m\)</span> mRNA at time <span class="math inline">\(t\)</span> regardless of the promoter state is given by <span class="math display">\[
P(m, t) = P_A(m, t) + P_I(m, t).
\]</span> Therefore, the corresponding generating function for the whole system is given by <span class="math display">\[
G(z, t) = G_A(z, t) + G_I(z, t).
\]</span></p>
<p>As with the one-state promoter case, let us transform our master equations by multiplying both sides by <span class="math inline">\(z^m\)</span> and sum over all <span class="math inline">\(m\)</span>. For the active state <span class="math inline">\(A\)</span> we have <span class="math display">\[
\begin{aligned}
\sum_{m} z^{m} \frac{d P_{A}(m, t)}{d t} =
\sum_{m} z^{m} &amp;\left[-k^{(p)}_{\text{off}} P_{A}(m, t) 
+ k^{(p)}_{\text{on}} P_{I}(m, t)\right.\\
&amp;+ \gamma_m (m+1) P_{A}(m+1, t)-\gamma_m m P_{m}(m, t) \\
&amp;\left. + r_m P_{A}(m-1, t) - r_m P_{A}(m, t)\right].
\end{aligned}
\]</span> After distributing the sum, we can use the tricks from the previous, allowing us to write this as a partial differential equation of the form <span class="math display">\[
\begin{aligned}
\frac{\partial G_{A}(z, t)}{\partial t} &amp;=
-k^{(p)}_{\text{off}} G_{A}(z, t)+k^{(p)}_{\text{on}} G_{I}(z, t) \\
&amp;-\gamma_m(z-1) \frac{\partial G_A(z, t)}{\partial z} 
+r_m(z-1) G_{A}(z, t).
\end{aligned}
\label{eq:gn_fn_act}
\]</span> An equivalent process can be done for the inactive state I, obtaining <span class="math display">\[
\begin{aligned}
\frac{\partial G_{I}(z, t)}{\partial t} &amp;=
k^{(p)}_{\text{off}} G_{A}(z, t) - k^{(p)}_{\text{on}} G_{I}(z, t) \\
&amp;-\gamma_m(z-1) \frac{\partial G A(z, t)}{\partial z} 
+r_m(z-1) G_{I}(z, t).
\end{aligned}
\label{eq:gn_fn_inact}
\]</span> We turned the infinite-dimensional system of ordinary differential equations into a system of two coupled partial differential equations. Let us transform the equations further. Since we have a common term <span class="math inline">\((z - 1)\)</span>, it will be convenient to define <span class="math inline">\(v \equiv (z -1)\)</span>. From the chain rule, it follows that <span class="math display">\[
d v=d(z-1)=d z \Rightarrow \frac{\partial G}{\partial v} = 
\frac{\partial G}{\partial z} \frac{d z}{d v}.
\]</span> Making this substitution in Eqs. <span class="math inline">\(\ref{eq:gn_fn_act}\)</span> and <span class="math inline">\(\ref{eq:gn_fn_inact}\)</span> results in <span class="math display">\[
\begin{aligned}
\frac{\partial G_{A}(v, t)}{\partial t} &amp;=-k^{(p)}_{\text{off}} G_{A}(v, t)
+ k^{(p)}_{\text{on}} G_{I}(v, t) \\
&amp;-\gamma_m v \frac{\partial G_{A}(v, t)}{\partial v} + r_m v G_{A}(v, t) \\
\end{aligned}
\]</span> for the actives state, and <span class="math display">\[
\begin{aligned}
\frac{\partial G I(v, t)}{\partial t}=&amp; k^{(p)}_{\text{off}} G_{A}(v, t)
- k^{(p)}_{\text{on}} G_{I}(v, t) \\
&amp;-r_m v \frac{\partial G_{I}(v, t)}{\partial v},
\end{aligned}
\]</span> for the active state.</p>
<p>Since we care about the steady-state distribution, it is at this point that we set the time derivative of both equations to zero. Doing this results in <span class="math display">\[
\gamma_{m} v \frac{d G_{A}(v)}{d v}= 
-k^{(p)}_{\text{off}} G_{A}(v)
+ k^{(p)}_{\text{on}} G_{I}(v).
+ r_m v G_{A}(v),
\label{eq:steady_act}
\]</span> and <span class="math display">\[
\gamma_{m} v \frac{d G_{I}(v)}{d v}= 
k^{(p)}_{\text{off}} G_{A}(v)
- k^{(p)}_{\text{on}} G_{I}(v).
\label{eq:steady_inact}
\]</span> Adding Eqs. <span class="math inline">\(\ref{eq:steady_act}\)</span> and <span class="math inline">\(\ref{eq:steady_inact}\)</span> gives a simple result <span class="math display">\[
\gamma_m \frac{d G(v)}{dv} = r_m G_A(v).
\label{eq:gen_fn_rel}
\]</span></p>
<p>Our objective is not to write Eqs. <span class="math inline">\(\ref{eq:steady_act}\)</span> and <span class="math inline">\(\ref{eq:steady_inact}\)</span> as a function of only one of the generating functions, i.e., we want two independent differential equations. These equations are both function of <span class="math inline">\(G_A(v)\)</span> and <span class="math inline">\(G_I(v)\)</span>, but Eq. <span class="math inline">\(\ref{eq:gen_fn_rel}\)</span> tells us how to relate both generating functions via the first derivative. This suggests that taking another derivative of Eqs. <span class="math inline">\(\ref{eq:steady_act}\)</span> and <span class="math inline">\(\ref{eq:steady_inact}\)</span> with respect to <span class="math inline">\(z\)</span> could be useful. Let us go ahead and compute these derivatives. For the active state, we find <span class="math display">\[
\small
\gamma_m \frac{d G_{A}(v)}{d v} 
+ \gamma_{m} v \frac{d^{2} G_{A}(v)}{d v^{2}} =
-k^{(p)}_{\text{off}} \frac{d G_{A}(v)}{d v}
+ k^{(p)}_{\text{on}} \frac{d \sigma I(v)}{d v}
+ r_m G_{A}(v, t) + r_m v \frac{d G_{A}(v)}{d v}
\]</span> Upon simplification, we can write this Eq. as <span class="math display">\[
\gamma_m v \frac{d^2 G_A}{d v^{2}}
+ \left(\gamma_m +k^{(p)}_{\text{off}}
-r_m v\right) \frac{d G_A}{d v}
-k^{(p)}_{\text{on}} \frac{d G_I}{d v}
-r_m G_{A}(v)=0.
\label{eq:gen_fn_2nd_act}
\]</span> From Eq. <span class="math inline">\(\ref{eq:gen_fn_rel}\)</span> we have that <span class="math display">\[
\frac{G_I}{dv} = \frac{r_m}{\gamma_m} G_A(v) 
- \frac{d G_A}{dv}.
\]</span> Substituting this into <span class="math inline">\(\ref{eq:gen_fn_2nd_act}\)</span> results in <span class="math display">\[
\gamma_m v \frac{d^{2} G_A}{d v^{2}}
+ \left(\gamma_m + k^{(p)}_{\text{off}} 
+ k^{(p)}_{\text{on}} 
- r_m v \right) \frac{d G_A}{d v}
- r_m \left(1+\frac{k^{(p)}_{\text{on}}}{\gamma_m}\right) G_A(v) = 0.
\label{eq:gen_2nd_act_final}
\]</span> For the inactive state, upon taking a derivative with respect to <span class="math inline">\(v\)</span>, we find <span class="math display">\[
\gamma_{m} v \frac{d^{2} G_{I}}{d v^{2}}
+ \left(\gamma_m + k^{(p)}_{\text{on}}\right) \frac{d G_I}{d v}
- k^{(p)}_{\text{off}} \frac{d G_A}{d v} = 0.
\label{eq:gen_fn_2nd_inact}
\]</span> Again from <span class="math inline">\(\ref{eq:gen_fn_rel}\)</span> we have that <span class="math display">\[
\frac{d G_A}{dv} = \frac{r_m}{\gamma_m} G_A - \frac{d G_I}{dv}.
\]</span> Substituting this result into Eq. <span class="math inline">\(\ref{eq:gen_fn_2nd_inact}\)</span> gives <span class="math display">\[
\gamma_{m} v \frac{d^{2} G_{I}}{d v^{2}}
+ \left(\gamma_m+k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}\right) 
\frac{d G_I}{d v} 
- \frac{k^{(p)}_{\text{off}} r_m}{\gamma_m} G_{A}(v)=0.
\label{eq:gen_fn_2nd_inact_2}
\]</span> So far, we have not removed the dependence on <span class="math inline">\(G_A(v)\)</span>. But we notice that from Eq. <span class="math inline">\(\ref{eq:steady_inact}\)</span> we have that <span class="math display">\[
G_A(v) = \frac{\gamma_m v}{k^{(p)}_{\text{off}}} \frac{d G_I}{d v}
+ \frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{off}}} G_I.
\]</span> Using this identity allows us to write Eq. <span class="math inline">\(\ref{eq:gen_fn_2nd_inact_2}\)</span> as <span class="math display">\[
\gamma_{m} v \frac{d^{2} G_{I}}{d v^{2}}
+ \left(\gamma_m + k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}} -r_m v\right) 
\frac{d G_I}{d v} - \frac{k^{(p)}_{\text{on}} r_m}{\gamma_m} G_I = 0.
\label{eq:gen_2nd_inact_final}
\]</span></p>
<p>To obtain a single partial differential equation we add Eqs. <span class="math inline">\(\ref{eq:gen_2nd_act_final}\)</span> and <span class="math inline">\(\ref{eq:gen_2nd_inact_final}\)</span>, obtaining <span class="math display">\[
\gamma_m v \frac{d^{2} G}{d v^{2}}
+ \left(\gamma_m + k^{(p)}_{\text{off}} + k^{(p)}_{\text{on}} -r_m v\right) 
\frac{d G}{d v} 
- \frac{r_m k^{(p)}_{\text{on}}}{\gamma_m} G(v)
- r_m G_A(v) = 0,
\]</span> where we substituted <span class="math inline">\(G_A(v) + G_I(v) = G(v)\)</span>. To remove the last <span class="math inline">\(G_A(v)\)</span> we utilize again Eq. <span class="math inline">\(\ref{eq:gen_fn_rel}\)</span>, obtaining <span class="math display">\[
\gamma_m v \frac{d^2 G}{dv^2} 
+ \left( k^{(p)}_{\text{off}} + k^{(p)}_{\text{on}} - r_m v \right)
\frac{dG}{dv} 
- \frac{r_m k^{(p)}_{\text{on}}}{\gamma_m} G(v) = 0.
\label{eq:gen_2nd}
\]</span></p>
<h4 id="solving-the-partial-differential-equation">Solving the partial differential equation</h4>
<p>Eq. <span class="math inline">\(\ref{eq:gen_2nd}\)</span> looks almost like the so-called Kummer’s equation also known as the confluent hypergeometric differential equation–a second order differential equation of the form <span class="math display">\[
z \frac{d^2w}{dz^2} + (b - z) \frac{dw}{dz} - aw = 0.
\label{eq:kummer}
\]</span> The solution fo the Kummer equation can be expressed as the sum of two functions: 1. The confluent hypergeometric function of the first kind, 2. The Tricomi function. This is written as <span class="math display">\[
w(z) = A {}_1F_1(a, b, z) + B z^{1-b} {}_1 F_1(a+1-b, 2-b, z),
\label{eq:kummer_sol}
\]</span> where <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are constants, and <span class="math inline">\({}_1F_1\)</span> is the confluent hypergeometric function of the first kind defined as <span class="math display">\[
{}_1F_1(a, b, z) = \sum_{m=0}^{\infty} 
\frac{a^{(m)}z^n}{b^{(m)} m!},
\]</span> where <span class="math inline">\(a^{(n})\)</span> and <span class="math inline">\(b^{(n)}\)</span> are the rising factorials, i.e., <span class="math display">\[
a^{(0)} = 1,
\]</span> and <span class="math display">\[
a^{(n)} = a (a + 1) (a + 2) \cdots (a + n - 1).
\]</span></p>
<p>To write Eq. <span class="math inline">\(\ref{eq:gen_2nd}\)</span> in the form of Eq. <span class="math inline">\(\ref{eq:kummer}\)</span> we can define <span class="math inline">\(s \equiv r_m v / \gamma_m\)</span>. The chain rule tells us that <span class="math display">\[
ds = \frac{r_m}{\gamma_m} dv \Rightarrow 
\frac{dG}{ds} = \frac{dG}{dv}\frac{dv}{ds} = 
\frac{\gamma_m}{r_m} \frac{dG}{dv}.
\]</span> From the chain rule, we also conclude that <span class="math display">\[
\frac{d^2G}{ds^2} = 
\frac{d}{dv} \left( \frac{dG}{dv} \frac{dv}{ds} \right) \frac{dv}{ds} =
\frac{\gamma_m ^2}{r_m^2} \frac{d^2G}{d v^2}.
\]</span> So the three relationships of <span class="math inline">\(v\)</span> with <span class="math inline">\(s\)</span> that we have take the form <span class="math display">\[
v = \frac{\gamma_m}{r_m} s, \;
\frac{dG}{dv} = \frac{r_m}{\gamma_m} \frac{dG}{ds}, \; \text{and }
\frac{d^2 G}{dv^2} = \frac{r_m^2}{\gamma_m^2} \frac{d^2G}{dv^2}.
\]</span> Substituting these definitions results in <span class="math display">\[
\gamma_m \left( \frac{\gamma_m}{r_m} s \right) 
\frac{r_m^2}{\gamma_m^2} \frac{d^2 G}{ds^2}
+ \left[ k^{(p)}_{\text{off}} + k^{(p)}_{\text{on}} 
- r_m \left( \frac{\gamma_m}{r_m} s \right) \right] 
\frac{r_m}{\gamma_m} \frac{dG}{ds}
- \frac{r_m k^{(p)}_{\text{on}}}{\gamma_m} G(s) = 0.
\]</span> Upon simplifying terms, we find an equation that is now in the form of Eq. <span class="math inline">\(\ref{eq:kummer}\)</span> <span class="math display">\[
s \frac{d^2 G}{ds^2}
+ \left(\frac{k^{(p)}_{\text{off}} + k^{(p)}_{\text{off}}}{\gamma_m} - s \right)
\frac{dG}{ds}
- \frac{k^{(p)}_{\text{on}}}{\gamma_m} G(s) = 0.
\label{eq:gen_kummer}
\]</span></p>
<p>Having put this in the form of the Kummer Eq., we can use Eq. <span class="math inline">\(\ref{eq:kummer_sol}\)</span> to write <span class="math inline">\(G(s)\)</span> as <span class="math display">\[
\begin{aligned}
G(s) &amp;= A {}_1F_1 
\left(
    \frac{k^{(p)}_{\text{on}}}{\gamma_m}, 
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m}, 
    s
\right) \\
&amp;+ B s^{1 - \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m}}
{}_1 F_1
\left(
    \frac{k^{(p)}_{\text{on}}}{\gamma_m} + 1 -
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m},
    2 - 
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m},
    s
\right).
\end{aligned}
\]</span> We can write down this solution in terms of the original variable of the generating function. We have that <span class="math inline">\(s = r_m/\gamma_m v\)</span>, and <span class="math inline">\(v = z - 1\)</span>. With this we then write <span class="math display">\[
\begin{aligned}
G(z) &amp;= A {}_1F_1 
\left(
    \frac{k^{(p)}_{\text{on}}}{\gamma_m}, 
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m}, 
    \frac{r_m}{\gamma_m}(z - 1)
\right) \\
&amp;+ B \left[\frac{r_m}{\gamma_m}(z - 1)\right]
^{1 - \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m}}
{}_1 F_1
\left(
    \frac{k^{(p)}_{\text{on}}}{\gamma_m} + 1 -
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m},
    2 - 
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m},
    \frac{r_m}{\gamma_m}(z - 1)
\right).
\end{aligned}
\label{eq:gen_sol}
\]</span></p>
<h4 id="finding-the-coefficients-for-the-solution">Finding the coefficients for the solution</h4>
<p>We can now use the normalization condition for the generating function, this is, <span class="math display">\[
G(1) = \sum_{m=0}^\infty 1^m P(m) = 1.
\]</span> Evaluating <span class="math inline">\(z=1\)</span> in Eq. <span class="math inline">\(\ref{eq:gen_sol}\)</span> results in <span class="math display">\[
G(1) = A {}_1F_1 
\left(
    \frac{k^{(p)}_{\text{on}}}{\gamma_m}, 
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m}, 
    0
\right).
\label{eq:gen_sol_1}
\]</span> Let’s look at the hypergeometric function evaluated of the form <span class="math inline">\({}_1F_1(a, b , 0)\)</span>. This takes the form <span class="math display">\[
{}_1 F_{1}(a, b, 0)=\sum_{m=0}^{\infty} \frac{a^{(m)} 0^{n}}{b^{(m)} m!} 
\]</span> All of the terms but one (<span class="math inline">\(n = 0\)</span>) are zero. The first term involving <span class="math inline">\(0^0\)</span> is undefined. Taking the limit as <span class="math inline">\(z \rightarrow 0\)</span> from the positive side, we find <span class="math display">\[
{}_1F_{1}(a, b, 0) = 
\lim _{z \rightarrow 0^{+}} {}_1 F_{1}(a, b, z) = 
\lim _{z \rightarrow 0^{+}} z^{0} = 1.
\]</span> Using this property in Eq. <span class="math inline">\(\ref{eq:gen_sol_1}\)</span> tells us that <span class="math inline">\(A = 1\)</span>.</p>
<p>We do not have another constraint for <span class="math inline">\(B\)</span>. Nevertheless, recall that Eq. <span class="math inline">\(\ref{eq:first_mom_gen}\)</span> tells us how to compute the first moment of the distribution from the generating function. For this, we need to compute the derivative of the confluent hypergeometric function. Let us derive this identity. Rather than computing the derivative directly, we will compute <span class="math display">\[
z \frac{d}{dz}{}_1F_1 = 
z \frac{d}{dz} 
\left[ \sum_{m=0}^{\infty} \frac{a^{(m)} z^{m}}{b^{(m)} m!}\right].
\]</span> Taking the derivative inside the sum gives <span class="math display">\[
z \frac{d}{dz}{}_1F_1 = 
z 
\left[ 
    \sum_{m=0}^{\infty} \frac{a^{(m)} }{b^{(m)} m!}
    \frac{d}{dz} z^{m}
\right] 
=
\left[ 
    \sum_{m=0}^{\infty} \frac{a^{(m)} }{b^{(m)}}
    \frac{m z^m}{m!}
\right] .
\]</span> Simplifying the term <span class="math inline">\(m/m!\)</span> gives <span class="math display">\[
z \frac{d}{dz}{}_1F_1 = 
\left[ 
    \sum_{m=0}^{\infty} \frac{a^{(m)} }{b^{(m)}}
    \frac{z^m}{(m-1)!}
\right] .
\label{eq:confluent_1}
\]</span> Note that the rising factorials can be rewritten as <span class="math display">\[
\begin{aligned}
    a^{(m)} &amp;=a(a+1)(a+2) \cdots(a+m-1) \\
    &amp;=a \cdot(a+1)[(a+1)+1][(a+1)+2] \cdots[(a+1)+m - 2] \\
    &amp;=a \cdot(a+1)^{(m-1)} .
\end{aligned}
\]</span> Therefore we can rewrite Eq. <span class="math inline">\(\ref{eq:confluent_1}\)</span> as <span class="math display">\[
\begin{aligned}
\sum_{m=0}^{\infty} \frac{a^{(m)}}{b^{(m)}} \frac{z^{m}}{(m-1) !} &amp;=
\sum_{m=0}^{\infty} \frac{a \cdot(a+1)^{(m-1)}}{b \cdot(b+1)^{(m-1)}} 
\frac{z \cdot z^{(m-1)}}{(m-1) !} \\
&amp;=\frac{a z}{b} 
\sum_{m=0}^{\infty} \frac{(a+1)^{(m-1)}}{(b+1)^{(m-1)}} \frac{z^{m-1}}{(m-1) !}
\end{aligned}
\]</span> If we define <span class="math inline">\(m&#39; = m - 1\)</span> we have <span class="math display">\[
\frac{a z}{b} \sum_{m=0}^{\infty} \frac{(a+1)^{(m-1)}}{(b+1)^{(m-1)}} 
\frac{z^{m-1}}{(m-1) !} = 
\frac{a z}{b} \sum_{m&#39;=-1}^{\infty}
\frac{(a+1)^{m&#39;}}{(b+1)^{m&#39;}} 
\frac{z^{m&#39;}}{m&#39;!}
\]</span> The term on the left is almost of the form of the confluent hypergeometric function again. The only difference is that the sum starts at <span class="math inline">\(m&#39; = -1\)</span>. This first term of the sum would then involve a term of the form <span class="math inline">\(1 / (-1)!\)</span>. But what does this even mean? To find this out, we can generalize the factorial function using the Gamma function such that <span class="math display">\[
(x - 1)! = \Gamma(x).
\]</span> The Gamma function diverges as <span class="math inline">\(x \rightarrow 0\)</span>, therefore <span class="math inline">\(1/\Gamma(x) \rightarrow 0\)</span> as <span class="math inline">\(x \rightarrow 0\)</span>. This means that the first term of the sum is zero, so we can begin the sum at <span class="math inline">\(m&#39; = 0\)</span>, recovering a confluent hypergeometric function. With this, we find that <span class="math display">\[
z \frac{d}{d z} {}_1F_1 = 
\frac{a z}{b} \sum_{m=0}^{\infty} \frac{(a+1)^{m}}{(b+1)^{m}} 
\frac{z^{m}}{m !} = 
\frac{a}{b} z_{1} F_{1}(a+1, b+1, z),
\]</span> therefore <span class="math display">\[
\frac{d}{dz}{}_1F_1 = \frac{a}{b} {}_1F_1(a + 1, b + 1, z).
\label{eq:gen_deriv}
\]</span></p>
<p>After this small but necessary detour, we can come back to computing the first moment of our distribution from the generating function. to evaluate Eq. <span class="math inline">\(\ref{eq:first_mom_gen}\)</span> on Eq. <span class="math inline">\(\ref{eq:gen_sol}\)</span> we first compute the derivative of the generating function. This can be easily evaluated using the relationship we derived for derivatives of <span class="math inline">\({}_1F_1\)</span>. The only thing to be aware of is that of the chain rule. In particular for our third entry of the third entry of the function we have <span class="math inline">\(r_m / \gamma_m (z - 1)\)</span> rather than simply <span class="math inline">\(z\)</span> as we had in Eq. <span class="math inline">\(\ref{eq:gen_deriv}\)</span>. This means that by the chain rule we have that if we define <span class="math inline">\(u = r_m / \gamma_m (z - 1)\)</span>, we have <span class="math display">\[
du = \frac{r_m}{\gamma_m} dz \Rightarrow
\frac{dG}{dz} = 
\frac{dG}{du} \frac{du}{dz} = 
\frac{dG}{du} \frac{r_m}{\gamma_m}.
\]</span> So there is an extra factor of <span class="math inline">\(r_m / \gamma_m\)</span> that will come along when we compute the derivative of our generating functions. Computing the derivative of Eq. <span class="math inline">\(\ref{eq:gen_sol}\)</span> results in <span class="math display">\[
\small
\begin{aligned}
\frac{dG}{dz} &amp;= 
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}
\frac{r_m}{\gamma_m}
{}_1F_1 
\left(
    \frac{k^{(p)}_{\text{on}}}{\gamma_m} + 1, 
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m} + 1, 
    \frac{r_m}{\gamma_m}(z - 1)
\right) \\
&amp; + B 
\left( 1 - \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m} \right)
\left[\frac{r_m}{\gamma_m}(z - 1)\right]
^{\frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m}}
{}_1 F_1
\left(
    \frac{k^{(p)}_{\text{on}}}{\gamma_m} + 1 -
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m},
    2 - 
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m},
    \frac{r_m}{\gamma_m}(z - 1)
\right) \\
&amp;+ B \left[\frac{r_m}{\gamma_m}(z - 1)\right]
^{1 - \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m}}
\left( 
    \frac{k^{(p)}_{\text{on}} + \gamma_m}
    {k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}} + \gamma_m}
\right)
\frac{r_m}{\gamma_m}
{}_1 F_1
\left(
    \frac{k^{(p)}_{\text{on}}}{\gamma_m} + 2 -
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m},
    1 - 
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m},
    \frac{r_m}{\gamma_m}(z - 1)
\right).
\end{aligned}
\]</span> This rather convoluted result is enormously simplified upon evaluating the derivative at <span class="math inline">\(z = 1\)</span> (See Eq. <span class="math inline">\(\ref{eq:first_mom_gen}\)</span>). This results in <span class="math display">\[
\left. \frac{dG}{dz} \right\vert_{z = 1} = 
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}
\frac{r_m}{\gamma_m}
{}_1F_1 
\left(
    \frac{k^{(p)}_{\text{on}}}{\gamma_m} + 1, 
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m} + 1, 
    0
\right) = 
\frac{r_m}{\gamma_m}
\frac{k^{(p)}_{\text{on}}}{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}},
\]</span> which is exactly the mean mRNA copy number we derived before. Since <span class="math inline">\(B\)</span> does not contribute to the mean, we can safely assume that <span class="math inline">\(B = 0\)</span>. This means that the final result for the generating function takes the much more compact form <span class="math display">\[
G(z) = 
{}_1F_1 
\left(
    \frac{k^{(p)}_{\text{on}}}{\gamma_m}, 
    \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m}, 
    \frac{r_m}{\gamma_m}(z - 1)
\right).
\label{eq:gen_final}
\]</span></p>
<h4 id="extracting-the-steady-state-mrna-distribution">Extracting the steady-state mRNA distribution</h4>
<p>Let us quickly recapitulate where we are. We started with a system of infinite many ordinary differential equations, one for each promoter state and mRNA copy number that defined the master equation for our two-state promoter. We then used the generating function to transform this system into a single partial differential equation. The resulting differential equation for the generating function took the form of the so-called Kummer differential equation, which has as a solution the confluent hypergeometric function and the Tricomi function. After imposing the normalization condition on the generating function, we found that the confluent hypergeometric function’s coefficient was <span class="math inline">\(A=1\)</span>. We then used the fact that the mean mRNA copy number <span class="math inline">\(\langle m \rangle\)</span> exists to show that the Tricomi function’s coefficient is <span class="math inline">\(B=0\)</span>. All that effort lead us to Eq. <span class="math inline">\(\ref{eq:gen_final}\)</span>, the generating function for the two-state promoter mRNA steady-state distribution. All we have left is trying to beat Eq. <span class="math inline">\(\ref{eq:gen_final}\)</span> into the form of a standard generating function to extract the probability distribution from it.</p>
<p>Let us begin this task by writing down Eq. <span class="math inline">\(\ref{eq:gen_final}\)</span> with the full definition of the confluent hypergeometric function. This gives us <span class="math display">\[
G(z) = \sum_{m=0}^\infty 
\frac{
    \left(\frac{k^{(p)}_{\text{on}}}{\gamma_m}\right)^{(m)}
}{
    \left(\frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}
    {\gamma_m}\right)^{(m)}
}
\frac{
    \left[\frac{r_m}{\gamma_m} (z-1) \right]^m
}{
    m!
}
\]</span> Let us now split apart the term <span class="math inline">\((z-1)\)</span>, obtaining <span class="math display">\[
G(z) = \sum_{m=0}^\infty 
\frac{
    \left(\frac{k^{(p)}_{\text{on}}}{\gamma_m}\right)^{(m)}
}{
    \left(\frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}
    {\gamma_m}\right)^{(m)}
}
\frac{
    \left(\frac{r_m}{\gamma_m} \right)^m
}{
    m!
}
(z - 1)^m.
\]</span> We now rewrite this last term <span class="math inline">\((z-2)^m\)</span> using the binomial expansion. This results in <span class="math display">\[
G(z) = \sum_{m=0}^\infty 
\frac{
    \left(\frac{k^{(p)}_{\text{on}}}{\gamma_m}\right)^{(m)}
}{
    \left(\frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}
    {\gamma_m}\right)^{(m)}
}
\frac{
    \left(\frac{r_m}{\gamma_m} \right)^m
}{
    m!
}
\left[ 
    \sum_{n=0}^m {m \choose n} z^n (-1)^{m -  n}.
\right]
\]</span> We can take out the sum over the index <span class="math inline">\(n\)</span> to the front, obtaining <span class="math display">\[
G(z) = \sum_{m=0}^\infty \sum_{n=0}^n
\frac{
    \left(\frac{k^{(p)}_{\text{on}}}{\gamma_m}\right)^{(m)}
}{
    \left(\frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}
    {\gamma_m}\right)^{(m)}
}
\frac{
    \left(\frac{r_m}{\gamma_m} \right)^m
}{
    m!
}
\left[ 
    {m \choose n} z^n (-1)^{m -  n}
\right].
\]</span> To make further progress, we must reindex the sum. The trick is to reverse the default order of the sums as <span class="math display">\[
\sum_{m=0}^{\infty} \sum_{n=0}^{m} = \sum_{n=0}^{\infty} \sum_{m=n}^{\infty}.
\]</span> To see the logic of the sum, we point the reader to Fig. 37. The key is to notice that the double sum <span class="math inline">\(\sum_{m=0}^\infty \sum_{n=0}^m\)</span> is adding all possible pairs <span class="math inline">\((m, n)\)</span> in the lower triangle, so we can add the terms vertically as the original sum indexing suggests, i.e. <span class="math display">\[
\sum_{m=0}^{\infty} \sum_{n=0}^{m} x_{(m, n)}= 
x_{(0, 0)} + x_{(1, 0)} + x_{(1, 1)} + x_{(2, 0)} + x_{(2, 1)} + x_{(2, 2)} + 
\ldots,
\]</span> where the variable <span class="math inline">\(x\)</span> is just a placeholder to indicate the order in which the sum is taking place. But we can also add the terms horizontally as <span class="math display">\[
\sum_{n=0}^{\infty} \sum_{m=n}^{\infty} x_{(m, n)} =
x_{(0, 0)} + x_{(1, 0)} + x_{(2, 0)} + \ldots + x_{(1,1)} + x_{(2, 1)} + \ldots,
\]</span> which still adds all of the lower triangle terms.</p>
<figure>
<img src="ch5_fig37.png" id="fig:ch5_fig37" data-short-caption="Reindexing double sum" alt="Figure 37: Reindexing double sum. Schematic for reindexing the sum \sum_{m=0}^\infty \sum_{n=0}^m. Blue circles depict the 2D grid of nonnegative integers restricted to the lower triangular part of the m, n plane. The trick is that this double sum runs over all (m, n) pairs with n\le m. Summing m first instead of n requires determining the boundary: the upper boundary of the n-first double sum becomes the lower boundary of the m-first double sum." /><figcaption aria-hidden="true">Figure 37: <strong>Reindexing double sum.</strong> Schematic for reindexing the sum <span class="math inline">\(\sum_{m=0}^\infty \sum_{n=0}^m\)</span>. Blue circles depict the 2D grid of nonnegative integers restricted to the lower triangular part of the <span class="math inline">\(m, n\)</span> plane. The trick is that this double sum runs over all <span class="math inline">\((m, n)\)</span> pairs with <span class="math inline">\(n\le m\)</span>. Summing <span class="math inline">\(m\)</span> first instead of <span class="math inline">\(n\)</span> requires determining the boundary: the upper boundary of the <span class="math inline">\(n\)</span>-first double sum becomes the lower boundary of the <span class="math inline">\(m\)</span>-first double sum.</figcaption>
</figure>
<p>Rewriting the sum in this way results in <span class="math display">\[
G(z) = \sum_{n=0}^\infty \sum_{m=n}^\infty
\frac{
    \left(\frac{k^{(p)}_{\text{on}}}{\gamma_m}\right)^{(m)}
}{
    \left(\frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}
    {\gamma_m}\right)^{(m)}
}
\frac{
    \left(\frac{r_m}{\gamma_m} \right)^m
}{
    m!
}
\left[ 
    {m \choose n} z^n (-1)^{m -  n}
\right].
\]</span> This allows us to separate the variable <span class="math inline">\(z^n\)</span> from the rest of the equation, leaving the standard format generating function to read the probability distribution <span class="math inline">\(P(m)\)</span>. This looks as <span class="math display">\[
G(z) = \sum_{n=0}^\infty z^n 
\left[
\sum_{m=n}^\infty
\frac{
    \left(\frac{k^{(p)}_{\text{on}}}{\gamma_m}\right)^{(m)}
}{
    \left(\frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}
    {\gamma_m}\right)^{(m)}
}
\frac{
    \left(\frac{r_m}{\gamma_m} \right)^m
}{
    m!
}
    {m \choose n} (-1)^{m -  n}
\right].
\]</span> Given the “dummy” nature of <span class="math inline">\(z\)</span>, it does not matter what the sum variable name is. We can simply rename <span class="math inline">\(m = n\)</span> and <span class="math inline">\(n = m\)</span> and conclude that our distribution takes the form <span class="math display">\[
P(m) = 
\sum_{n=m}^\infty
\frac{
    \left(\frac{k^{(p)}_{\text{on}}}{\gamma_m}\right)^{(n)}
}{
    \left(\frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}
    {\gamma_m}\right)^{(n)}
}
\frac{
    \left(\frac{r_m}{\gamma_m} \right)^n
}{
    n!
}
    \frac{n!}{m! (n - m)!} (-1)^{n -  m}.
\label{eq:prob_ss_1}
\]</span></p>
<p>We can simplify Eq. <span class="math inline">\(\ref{eq:prob_ss_1}\)</span> further. First we split the term <span class="math inline">\((-1)^{n-m} = (-1)^{-m} (-1)^{n}\)</span>. Furthermore we absorbed the <span class="math inline">\((-1)^{n}\)</span> term on the <span class="math inline">\((r_m / \gamma_m)^n\)</span> term. We also cancel the obvious <span class="math inline">\(n!/n!\)</span> term, obtaining <span class="math display">\[
P(m) = \sum_{n = m}^\infty
\frac{(-1)^{-m}}{m!}
\frac{
    \left( \frac{k^{(p)}_{\text{on}}}{\gamma_m}\right)^{(n)}
}{
    \left( \frac{k^{(p)}_{\text{on}}+ k^{(p)}_{\text{off}}}
    {\gamma_m}\right)^{(n)}
}
\frac{\left( - \frac{r_m}{\gamma_m}\right)^n}{(n - m)!}.
\label{eq:prob_ss_2}
\]</span> We recognize in Eq. <span class="math inline">\(\ref{eq:prob_ss_2}\)</span> that we have almost all the terms for a confluent hypergeometric function <span class="math inline">\({}_1F_1\)</span>. The problem is that the sum starts at <span class="math inline">\(n = m\)</span> rather than <span class="math inline">\(n = 0\)</span>. Since the upper limit of the sum is <span class="math inline">\(\infty\)</span>, we can simply define <span class="math inline">\(u = n - m \Rightarrow n = m + u\)</span>. We can then use the following property of raising factorials <span class="math display">\[
\begin{split}
a^{(n)} &amp;=a(a+1)(a+2) \cdots(a+n-1), \\
&amp;=a(a+1)(a+2) \cdots(a+(u+m)-1), \\
&amp;=a(a+1) \cdots(a+m-1)(a+m)(a+m+1) \cdots(a+m+u-1), \\
&amp;=a^{(m)}(a+m)^{(u)}.
\end{split}
\]</span> Making these substitutions results in <span class="math display">\[
P(m) = \sum_{u=0}^{\infty} 
\frac{(-1)^{-m}}{m !} 
\frac{
    \left(\frac{k^{(p)}_{\text{on}}}{\gamma_m} \right)^{(m)}
    \left(\frac{k^{(p)}_{\text{on}}}{\gamma_m} + m \right)^{(u)}
    \left(-\frac{r_m}{\gamma_m}\right)^{u}
    \left(-\frac{r_m}{\gamma_m}\right)^{m}
}{
    \left(
        \frac{
            k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}
        }{
            \gamma_m
        }
    \right)^{(m)}
    \left(
        \frac{
            k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}
            }{
                \gamma_m
            } + m
    \right)^{(n)}}
    \frac{1}{u!}.
\]</span> Taking out of the sum the terms that do not depend on <span class="math inline">\(u\)</span> gives <span class="math display">\[
P(m) = 
\frac{(-1)^{-m}}{m!}
\frac{
    \left(
        \frac{
            k^{(p)}_{\text{on}}
        }{
            \gamma_m
        }
    \right)^{(m)}
}{
    \left(
        \frac{
            k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}
        }{
            \gamma_m
        }
    \right)^{(m)}
}
\left(- \frac{r_m}{\gamma_m}\right)^m
\left[
    \sum_{u=0}^{\infty}
    \frac{
        \left(
            \frac{
                k^{(p)}_{\text{on}} 
            }{
                \gamma_m
            }
            + m
        \right)^{(u)}
    }{
        \left(
            \frac{
                k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}} 
            }{
                \gamma_m
            }
            + m
        \right)^{(u)}
    }
    \frac{
        \left(- \frac{r_m}{\gamma_m}\right)^u
    }{u!}
\right].
\]</span> We recognize the term in the square brackets to be the necessary components for a confluent hypergeometric function. We can therefore write the mRNA steady-state distribution as <span class="math display">\[
P(m) = 
\frac{1}{m!}
\frac{
    \left(
        \frac{
            k^{(p)}_{\text{on}}
        }{
            \gamma_m
        }
    \right)^{(m)}
}{
    \left(
        \frac{
            k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}
        }{
            \gamma_m
        }
    \right)^{(m)}
}
\left(\frac{r_m}{\gamma_m}\right)^m
{}_1F_1 
\left(
    \frac{
            k^{(p)}_{\text{on}} 
        }{
            \gamma_m
        }
    + m,
    \frac{
            k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}} 
        }{
            \gamma_m
        }
    + m,
    - \frac{r_m}{\gamma_m}
\right).
\]</span> For the last ingredient, we remove the rising factorials using the identity <span class="math display">\[
\begin{split}
a^{(m)} &amp;=(a)(a+1)(a+2) \cdots(a+m-1), \\
&amp;=\frac{(a+m-1) \cdots(a)(a-1) \cdots (1)}{(a+1) \cdots(1)}, \\
&amp;=\frac{(a+m-1) !}{(a-1) !}.
\end{split}
\]</span> This allows us to write <span class="math display">\[
\begin{split}
P(m) &amp;= 
\frac{1}{m!}
\frac{
    \left(
        \frac{k^{(p)}_{\text{on}}}{\gamma_m} + m - 1
    \right) !
}{
    \left(
        \frac{k^{(p)}_{\text{on}}}{\gamma_m} - 1
    \right) !
}
\frac{
    \left(
        \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m} - 1
    \right) !
}{
    \left(
        \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m} + m - 1
    \right) !
}
\left( \frac{r_m}{\gamma_m} \right)^m \\
&amp;\times {}_1F_1 
\left(
    \frac{
            k^{(p)}_{\text{on}} 
        }{
            \gamma_m
        }
    + m,
    \frac{
            k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}} 
        }{
            \gamma_m
        }
    + m,
    - \frac{r_m}{\gamma_m}
\right).
\end{split}
\]</span> Or in terms of Gamma functions, we obtain the final form of the steady-state mRNA distribution <span class="math display">\[
\begin{aligned}
P(m) &amp;= 
\frac{1}{\Gamma(m + 1)}
\frac{
    \Gamma
    \left(
        \frac{k^{(p)}_{\text{on}}}{\gamma_m} + m
    \right)
}{
    \Gamma
    \left(
        \frac{k^{(p)}_{\text{on}}}{\gamma_m}
    \right)
}
\frac{
    \Gamma
    \left(
        \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m}
    \right)
}{
    \Gamma
    \left(
        \frac{k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}}}{\gamma_m} + m 
    \right)
}
\left( \frac{r_m}{\gamma_m} \right)^m \\
&amp;\times {}_1F_1 
\left(
    \frac{
            k^{(p)}_{\text{on}} 
        }{
            \gamma_m
        }
    + m,
    \frac{
            k^{(p)}_{\text{on}} + k^{(p)}_{\text{off}} 
        }{
            \gamma_m
        }
    + m,
    - \frac{r_m}{\gamma_m}
\right),
\end{aligned}
\]</span> The equation used to fit the kinetic parameters for the unregulated promoter.</p>
<h2 id="derivation-of-the-cell-age-distribution">Derivation of the cell age distribution</h2>
<p>E. O. Powell first derived in 1956 the distribution of cell age for a cell population growing steadily in the exponential phase <span class="citation" data-cites="Powell1956"> [<a href="#ref-Powell1956" role="doc-biblioref">15</a>]</span>. This distribution is of the form <span class="math display">\[
P(a) = \ln(2) \cdot 2^{1 - a},
\]</span> where <span class="math inline">\(a \in [0, 1]\)</span> is the fraction of the cell cycle, 0 being the moment right after the mother cell divides, and 1 being the end of the cell cycle just before cell division. In this section, we will reproduce and expand the details on each of the steps of the derivation.</p>
<p>For an exponentially growing bacterial culture, the cells satisfy the growth law <span class="math display">\[
{\frac{dn}{dt}} = \mu n,
\]</span> where <span class="math inline">\(n\)</span> is the number of cells, and <span class="math inline">\(\mu\)</span> is the growth rate in units of time<span class="math inline">\(^{-1}\)</span>. We begin by defining <span class="math inline">\(P(a)\)</span> to be the probability density function of a cell having age <span class="math inline">\(a\)</span>. At time zero of a culture in exponential growth, i.e., the time when we start considering the growth, not the initial condition of the culture, there are <span class="math inline">\(NP(a)da\)</span> cells with an age range between <span class="math inline">\([a, a + da]\)</span>. In other words, for <span class="math inline">\(N \gg 1\)</span> and <span class="math inline">\(da \ll a\)</span> <span class="math display">\[
N P(a \leq x \leq a + da) \approx N P(a)da.
\]</span> We now define <span class="math display">\[
F(\tau) = \int_\tau^\infty f(\xi) d\xi,
\]</span> as the fraction of cells whose division time is greater than <span class="math inline">\(\tau\)</span>. This is because in principle, not all cells divide exactly after <span class="math inline">\(\tau\)</span> minutes, but there is a distribution function <span class="math inline">\(f(\tau)\)</span> for the division time after birth. Empirically it has been observed that a generalized Gamma distribution fits well to experimental data on cell division time, but we will worry about this specific point later on.</p>
<p>From the definition of <span class="math inline">\(F(\tau)\)</span> we can see that if a cell reaches an age <span class="math inline">\(a\)</span>, the probability of surviving to an age <span class="math inline">\(a + t\)</span> without dividing is given by <span class="math display">\[
P(\text{age} = (a + t) \mid \text{age} = a) = F(a + t \mid a) =
\frac{F(a + t)}{F(a)}.
\]</span> This result comes simply from the definition of conditional probability. Since <span class="math inline">\(F(a)\)</span> is the probability of surviving <span class="math inline">\(a\)</span> or more minutes without dividing, by the definition of conditional probability we have that <span class="math display">\[
F(a + t \mid a) = \frac{F(a, a + t)}{F(a)},
\]</span> where <span class="math inline">\(F(a, a + t)\)</span> is the joint probability of surviving <span class="math inline">\(a\)</span> minutes and <span class="math inline">\(a + t\)</span> minutes. But the probability of surviving <span class="math inline">\(a + t\)</span> minutes or more implies that the cell already survived <span class="math inline">\(a\)</span> minutes, therefore the information is redundant and we have <span class="math display">\[
F(a, a + t) = F(a + t).
\]</span> This explains XXX. From this equation, we can find that out of the <span class="math inline">\(N P(a)da\)</span> cells with age <span class="math inline">\(a\)</span> only a fraction <span class="math display">\[
\left[ NP(a)da \right] F(a + t \mid a) = NP(a) \frac{F(a + t)}{F(a)} da
\]</span> will survive without dividing until time <span class="math inline">\(a + t\)</span>. During that time interval <span class="math inline">\(t\)</span> the culture has passed from <span class="math inline">\(N\)</span> cells to <span class="math inline">\(N e^{\mu t}\)</span> cells, given the assumption that they are growing exponentially. The survivors <span class="math inline">\(NP(a)F(a + t \mid a)da\)</span> then represent a fraction of the total number of cells <span class="math display">\[
\frac{\text{\# survivors}}{\text{\# total cells}} =
\frac{\left[ NP(a)da \right] F(a + t \mid a)}{Ne^{\mu t}} =
  P(a)\frac{F(a + t)}{F(a)}da \frac{1}{e^{\mu t}},
\]</span> and their ages lie in the range <span class="math inline">\([a+t, a+t+da]\)</span>. Since we assume that the culture is in steady state then it follows that the fraction of cells that transitioned from age <span class="math inline">\(a\)</span> to age <span class="math inline">\(a + t\)</span> must be <span class="math inline">\(P(a + t)da\)</span>. Therefore we have a difference equation - the discrete analogous of a differential equation - of the form <span class="math display">\[
P(a + t) da = P(a) \frac{F(a + t)}{F(a)}e^{-\mu t} da.
\]</span> What this equation shows is a relationship that connects the probability of having a lifetime of <span class="math inline">\(a + t\)</span> with a probability of having a shorter lifetime <span class="math inline">\(a\)</span> and the growth of the population. If we take <span class="math inline">\(t\)</span> to be very small, specifically if we assume <span class="math inline">\(t \ll \mu^{-1}\)</span> we can Taylor expand around <span class="math inline">\(a\)</span> the following terms: <span class="math display">\[
F(a + t) \approx F(a) + \frac{dF}{da} t,
\]</span> <span class="math display">\[
P(a + t) \approx P(a) + \frac{dP}{da} t,
\]</span> and <span class="math display">\[
e^{-\mu t} \approx 1 - \mu t.
\]</span> Substituting these equations gives <span class="math display">\[
P(a) + \frac{dP}{da} t = P(a) \left( \frac{F(a) + \frac{dF}{da}t}{
  F(a)} \right) (1 - \mu t).
\]</span> This can be rewritten as <span class="math display">\[
\frac{1}{P(a)} \frac{dP}{da} =
\frac{1}{F(a)} \frac{dF}{da} - \mu - \frac{\mu t}{F(a)} \frac{dF}{da}.
\]</span> Since we assumed <span class="math inline">\(t \ll \mu^{-1}\)</span>, we approximate the last term to be close to zero. We can then simplify this result into <span class="math display">\[
\frac{1}{P(a)} \frac{dP}{da} = \frac{1}{F(a)} \frac{dF}{da} - \mu.
\]</span> Integrating both sides of the equation with respect to <span class="math inline">\(a\)</span> gives <span class="math display">\[
\ln P(a) = \ln F(a) - \mu a + C,
\]</span> where <span class="math inline">\(C\)</span> is the integration constant. Exponentiating both sides gives <span class="math display">\[
P(a) = C&#39; F(a)e^{-\mu a}.
\]</span> Where <span class="math inline">\(C&#39; \equiv e^C\)</span>. To obtain the value of the unknown constant, we recall that <span class="math inline">\(F(0) = 1\)</span> since the probability of having a life equal to or longer than zero must add up to one. Therefore we have that <span class="math inline">\(P(0) = C&#39;\)</span>. This gives then <span class="math display">\[
P(a) = P(0) e^{-\mu a} F(a).
\]</span> Substituting the definition of <span class="math inline">\(F(a)\)</span> gives <span class="math display">\[
P(a) = P(0) e^{-\mu a} \int_a^\infty f(\xi) d\xi.
\]</span> The last step of the derivation involves writing <span class="math inline">\(P(0)\)</span> and the growth rate <span class="math inline">\(\mu\)</span> in terms of the cell cycle length distribution <span class="math inline">\(f(\tau)\)</span>.</p>
<p>The growth rate of the population cell number (not the growth of cell mass) is defined as the number of cell doublings per unit of time divided by the number of cells. This is more clear to see if we write as a finite difference <span class="math display">\[
\frac{N(t + \Delta t) - N(t)}{\Delta t} = \mu N(t).
\]</span> If the time <span class="math inline">\(\Delta t\)</span> is the time interval it takes to go from <span class="math inline">\(N\)</span> to <span class="math inline">\(2N\)</span> cells we have <span class="math display">\[
\frac{2N - N}{\Delta t} = \mu N.
\]</span> Solving for <span class="math inline">\(\mu\)</span> gives <span class="math display">\[
\mu = \overbrace{\frac{2N - N}{\Delta t}}
^{\text{\# doubling events per unit time}}
\overbrace{\frac{1}{N}}^{\frac{1}{\text{population size}}}.
\]</span> We defined <span class="math inline">\(F(a)\)</span> to be the probability of a cell reaching an age <span class="math inline">\(a\)</span> or greater. For a cell to reach an age <span class="math inline">\(a + da\)</span>, we can then write <span class="math display">\[
F(a + da) = \int_{a + da}^{\infty} f(\xi) d\xi
= \int_a^{\infty} f(\xi) d\xi - \int_a^{a + da} f(\xi) d\xi.
\]</span> We can approximate the second term on the right-hand side to be <span class="math display">\[
\int_a^{a + da} f(\xi) d\xi \approx f(a) da,
\]</span> for <span class="math inline">\(da \ll a\)</span>, obtaining <span class="math display">\[
F(a + da) \approx F(a) - f(a)da.
\]</span> What this means is that from the original fraction of cells <span class="math inline">\(F(a)\)</span> with age <span class="math inline">\(a\)</span> or greater a fraction <span class="math inline">\(f(a)da / F(a)\)</span> will not reach age <span class="math inline">\((a + da)\)</span> because they will divide. So out of the <span class="math inline">\(NP(a)\)</span> cells that reached exactly age <span class="math inline">\(a\)</span>, the number of doubling events on a time interval <span class="math inline">\(da\)</span> is given by <span class="math display">\[
{\text{\# doublings of cells of age } a {\text{ on interval } da}} =
  \overbrace{NP(a)}^{\text{\# cells of age }a}
  \overbrace{\frac{f(a) da}{F(a)}}^{\text{fraction of doublings per unit time}}.
\]</span> The growth rate then is just the sum (integral) of each age contribution to the total number of doublings. This is <span class="math display">\[
\mu = \frac{1}{N} \int_0^\infty NP(a) \frac{f(a)da}{F(a)}.
\]</span> Substituting gives <span class="math display">\[
\mu = \int_0^\infty [P(0) e^{-\mu a} F(a)] \frac{f(a)da}{F(a)}
  = \int_0^\infty P(0) e^{-\mu a} f(a)da.
\]</span> We now have the growth rate <span class="math inline">\(\mu\)</span> written in terms of the cell cycle length probability distribution <span class="math inline">\(f(a)\)</span> and the probability <span class="math inline">\(P(0)\)</span>. Since <span class="math inline">\(P(a)\)</span> is a probability distribution, it must be normalized, i.e., <span class="math display">\[
\int_0^\infty P(a) da = 1.
\]</span> Substituting into this normalization constraint gives <span class="math display">\[
\int_0^\infty P(0) e^{-\mu a} F(a) da = 1.
\]</span> From here, we can integrate the left-hand side by parts. We note that given the definition of <span class="math inline">\(F(a)\)</span>, the derivative with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(-f(a)\)</span> rather than <span class="math inline">\(f(a)\)</span>. This is because if we write the derivative of <span class="math inline">\(F(a)\)</span>, we have <span class="math display">\[
\frac{dF(a)}{da} \equiv \lim_{da \rightarrow 0}
  \frac{F(a + da) - F(a)}{da}.
\]</span> Substituting the definition of <span class="math inline">\(F(a)\)</span> gives <span class="math display">\[
\frac{dF(a)}{da} = \lim_{da \rightarrow 0} \frac{1}{da}
\left[\int_{a + da}^\infty f(\xi) d\xi - \int_a^\infty f(\xi) d\xi \right].
\]</span> This difference in the integrals can be simplified to <span class="math display">\[
\lim_{da \rightarrow 0} \frac{1}{da} \left[ \int_{a + da}^\infty f(\xi) d\xi -
  \int_a^\infty f(\xi) d\xi \right]\approx \frac{-f(a)da}{da} = -f(a).
\]</span> Taking this into account, we now perform the integration by parts obtaining <span class="math display">\[
P(0) \left[ \frac{e^{-\mu t}}{-\mu} F(a) \right]^\infty_0
 - P(0) \int_0^\infty \frac{e^{-\mu a}}{-\mu} (-f(a)) da = 1.
\]</span> On the first term on the left hand side we have that as <span class="math inline">\(a \rightarrow \infty\)</span>, both terms <span class="math inline">\(e^{-\mu a}\)</span> and <span class="math inline">\(F(a)\)</span> go to zero. We also have that <span class="math inline">\(e^{\mu 0} = 1\)</span> and <span class="math inline">\(F(0) = 1\)</span>. This results in <span class="math display">\[
\frac{P(0)}{\mu} - P(0) \int_0^\infty \frac{e^{-\mu a}}{\mu} f(a) da = 1.
\]</span> The second term on the left-hand side is equal to since <span class="math display">\[
\mu = \int_0^\infty P(0) e^{-\mu a} f(a)da \Rightarrow
  1 = \int_0^\infty P(0) \frac{e^{-\mu a}}{\mu} f(a)da.
\]</span> This implies that we have <span class="math display">\[
\frac{P(0)}{\mu} - 1 = 1 \Rightarrow P(0) = 2 \mu.
\]</span> With this result in hand, we can rewrite it as <span class="math display">\[
P(a) = 2\mu e^{-\mu a} \int_a^\infty f(\xi) d\xi.
\]</span> Also, we can rewrite the result for the growth rate <span class="math inline">\(\mu\)</span> on as <span class="math display">\[
\mu = 2 \mu \int_0^\infty e^{-\mu a} f(a) da \Rightarrow
  2 \int_0^\infty e^{-\mu a} f(a) da = 1.
\]</span></p>
<p>As mentioned before, the distribution <span class="math inline">\(f(a)\)</span> has been empirically fit to a generalized Gamma distribution. But if we assume that our distribution has almost negligible dispersion around the mean average doubling time <span class="math inline">\(a = \tau_d\)</span>, we can approximate <span class="math inline">\(f(a)\)</span> as <span class="math display">\[
f(a) = \delta(a - \tau_d),
\]</span> a Dirac delta function. Applying this to results in <span class="math display">\[
2 \int_0^\infty e^{-\mu a} \delta(a - \tau_a) da = 1
  \Rightarrow 2 e^{-\mu \tau_d} = 1.
\]</span> Solving for <span class="math inline">\(\mu\)</span> gives <span class="math display">\[
\mu = \frac{\ln 2}{\tau_d}.
\]</span> This delta function approximation for <span class="math inline">\(f(a)\)</span> has as a consequence that <span class="math display">\[
F(a) =
  \begin{cases}
    1 \text{ for } a \in [0, \tau_d],\\
    0 \text{ for } a &gt; \tau_d.
  \end{cases}
\]</span> Finally, we can rewrite it as <span class="math display">\[
P(a) = 2 \left( \frac{\ln 2}{\tau_d} \right)
e^{- \frac{\ln 2}{\tau_d} a} \int_a^\infty \delta(\xi - \tau_d) d\xi
\Rightarrow = 2 \ln 2 \cdot 2^\frac{-a}{\tau_d}.
\]</span> Simplifying this, we obtain <span class="math display">\[
P(a) =
  \begin{cases}
    \ln 2 \cdot 2^{1 - \frac{a}{\tau_d}} \text{ for } a \in [0, \tau_d],\\
    0 \text{ otherwise}.
  \end{cases}
\]</span> This is the equation we aimed to derive. The distribution of cell ages over the cell cycle.</p>
<h2 class="unnumbered" id="references">References</h2>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-Sanchez2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">A. Sanchez, S. Choubey, and J. Kondev, <em><span class="nocase">Stochastic models of transcription: From single molecules to single cells</span></em>, Methods <strong>62</strong>, 13 (2013).</div>
</div>
<div id="ref-Peccoud1995" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">J. Peccoud and B. Ycart, <em><span class="nocase">Markovian Modeling of Gene-Product Synthesis</span></em>, Theoretical Population Biology <strong>48</strong>, 222 (1995).</div>
</div>
<div id="ref-Shahrezaei2008" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">V. Shahrezaei and P. S. Swain, <em><span class="nocase">Analytical distributions for stochastic gene expression.</span></em>, PNAS <strong>105</strong>, 17256 (2008).</div>
</div>
<div id="ref-Jones2014a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">D. L. Jones, R. C. Brewster, and R. Phillips, <em><span class="nocase">Promoter architecture dictates cell-to-cell variability in gene expression</span></em>, Science <strong>346</strong>, 1533 (2014).</div>
</div>
<div id="ref-Garcia2011c" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">H. G. Garcia and R. Phillips, <em><span class="nocase">Quantitative dissection of the simple repression input-output function.</span></em>, Proceedings of the National Academy of Sciences <strong>108</strong>, 12173 (2011a).</div>
</div>
<div id="ref-Brewster2014" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">R. C. Brewster, F. M. Weinert, H. G. Garcia, D. Song, M. Rydenfelt, and R. Phillips, <em><span class="nocase">The Transcription Factor Titration Effect Dictates Level of Gene Expression</span></em>, Cell <strong>156</strong>, 1312 (2014).</div>
</div>
<div id="ref-Razo-Mejia2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">M. Razo-Mejia, S. L. Barnes, N. M. Belliveau, G. Chure, T. Einav, M. Lewis, and R. Phillips, <em><span class="nocase">Tuning Transcriptional Regulation through Signaling: A Predictive Theory of Allosteric Induction</span></em>, Cell Systems <strong>6</strong>, 456 (2018).</div>
</div>
<div id="ref-MacKay2003" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">D. J. MacKay, <em><span class="nocase">Information theory, inference and learning algorithms</span></em> (Cambridge university press, 2003).</div>
</div>
<div id="ref-Brewster2012" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">R. C. Brewster, D. L. Jones, and R. Phillips, <em><span class="nocase">Tuning Promoter Strength through RNA Polymerase Binding Site Design in <em>Escherichia coli</em></span></em>, PLoS Computational Biology <strong>8</strong>, e1002811 (2012).</div>
</div>
<div id="ref-Bintu2005a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">L. Bintu, N. E. Buchler, H. G. Garcia, U. Gerland, T. Hwa, J. Kondev, and R. Phillips, <em><span class="nocase">Transcriptional regulation by the numbers: models</span></em>, Current Opinion in Genetics <span>&amp;</span> Development <strong>15</strong>, 116 (2005).</div>
</div>
<div id="ref-Klumpp2008" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">S. Klumpp and T. Hwa, <em><span class="nocase">Growth-rate-dependent partitioning of RNA polymerases in bacteria.</span></em>, PNAS <strong>105</strong>, 20245 (2008).</div>
</div>
<div id="ref-Transtrum2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">M. K. Transtrum, B. B. Machta, K. S. Brown, B. C. Daniels, C. R. Myers, and J. P. Sethna, <em><span class="nocase">Perspective: Sloppiness and emergent theories in physics, biology, and beyond.</span></em>, The Journal of Chemical Physics <strong>143</strong>, 010901 (2015).</div>
</div>
<div id="ref-Peterson2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">J. R. Peterson, J. A. Cole, J. Fei, T. Ha, and Z. A. Luthey-Schulten, <em><span class="nocase">Effects of DNA replication on mRNA noise</span></em>, PNAS <strong>112</strong>, 15886 (2015).</div>
</div>
<div id="ref-Bremer1996" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">H. Bremer and P. P. Dennis, <em><span class="nocase">Modulation of Chemical Composition and Other Parameters of the Cell by Growth Rate</span></em>, (n.d.).</div>
</div>
<div id="ref-Powell1956" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">E. O. Powell, <em><span class="nocase">Growth Rate and Generation Time of Bacteria, with Special Reference to Continuous Culture</span></em>, Journal of General Microbiology <strong>15</strong>, 492 (1956).</div>
</div>
<div id="ref-Browning2004" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">D. F. Browning and S. J. W. Busby, <em><span class="nocase">The regulation of bacterial transcription initiation</span></em>, Nature Reviews Microbiology <strong>2</strong>, 57 (2004).</div>
</div>
<div id="ref-Yu2006" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">J. Yu, <em><span class="nocase">Probing Gene Expression in Live Cells, One Protein Molecule at a Time</span></em>, Science <strong>311</strong>, 1600 (2006).</div>
</div>
<div id="ref-Phillips2015a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">R. Phillips, <em><span class="nocase">Napoleon Is in Equilibrium</span></em>, Annual Review of Condensed Matter Physics <strong>6</strong>, 85 (2015).</div>
</div>
<div id="ref-Buchler2003" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">N. E. Buchler, U. Gerland, and T. Hwa, <em><span class="nocase">On schemes of combinatorial transcription logic.</span></em>, PNAS <strong>100</strong>, 5136 (2003).</div>
</div>
<div id="ref-Vilar2011" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">J. M. G. Vilar and L. Saiz, <em><span class="nocase">Control of gene expression by modulated self-assembly</span></em>, Nucleic Acids Research <strong>39</strong>, 6854 (2011).</div>
</div>
<div id="ref-Radzikowski2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">J. L. Radzikowski, S. Vedelaar, D. Siegel, Á. D. Ortega, A. Schmidt, and M. Heinemann, <em><span class="nocase">Bacterial persistence is an active <span class="math inline">\(\sigma\)</span> S stress response to metabolic flux limitation</span></em>, Molecular Systems Biology <strong>12</strong>, 882 (2016).</div>
</div>
<div id="ref-Hammar2014" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">P. Hammar, M. Walldén, D. Fange, F. Persson, Ö. Baltekin, G. Ullman, P. Leroy, and J. Elf, <em><span class="nocase">Direct measurement of transcription factor dissociation excludes a simple operator occupancy model for gene regulation</span></em>, Nature Genetics <strong>46</strong>, 405 (2014).</div>
</div>
<div id="ref-Moran2010" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">U. Moran, R. Phillips, and R. Milo, <em><span class="nocase">SnapShot: Key Numbers in Biology</span></em>, Cell <strong>141</strong>, 1262 (2010).</div>
</div>
<div id="ref-sympy" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">A. Meurer, C. P. Smith, M. Paprocki, O. Čertík, S. B. Kirpichev, M. Rocklin, A. Kumar, S. Ivanov, J. K. Moore, S. Singh, T. Rathnayake, S. Vig, B. E. Granger, R. P. Muller, F. Bonazzi, H. Gupta, S. Vats, F. Johansson, F. Pedregosa, M. J. Curry, A. R. Terrel, Š. Roučka, A. Saboo, I. Fernando, S. Kulal, R. Cimrman, and A. Scopatz, <em>SymPy: Symbolic Computing in Python</em>, PeerJ Computer Science <strong>3</strong>, e103 (2017).</div>
</div>
<div id="ref-Phillips2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">R. Phillips, N. M. Belliveau, G. Chure, H. G. Garcia, M. Razo-Mejia, and C. Scholes, <em><span class="nocase">Figure 1 Theory Meets Figure 2 Experiments in the Study of Gene Expression</span></em>, Annual Review of Biophysics <strong>48</strong>, 121 (2019).</div>
</div>
<div id="ref-Barnes2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">S. L. Barnes, N. M. Belliveau, W. T. Ireland, J. B. Kinney, and R. Phillips, <em><span class="nocase">Mapping DNA sequence to transcription factor binding energy in vivo</span></em>, PLoS Computational Biology <strong>15</strong>, e1006226 (2019).</div>
</div>
<div id="ref-Ale2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">A. Ale, P. Kirk, and M. P. H. Stumpf, <em><span class="nocase">A general moment expansion method for stochastic kinetic models</span></em>, The Journal of Chemical Physics <strong>138</strong>, 174101 (2013).</div>
</div>
<div id="ref-Andreychenko2017" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">A. Andreychenko, L. Bortolussi, R. Grima, P. Thomas, and V. Wolf, <em><span>Modeling Cellular Systems</span></em>, Vol. 11 (Springer International Publishing, Cham, 2017).</div>
</div>
<div id="ref-Frohlich2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">F. Fröhlich, P. Thomas, A. Kazeroonian, F. J. Theis, R. Grima, and J. Hasenauer, <em><span class="nocase">Inference for Stochastic Chemical Kinetics Using Moment Equations and System Size Expansion</span></em>, PLoS Computational Biology <strong>12</strong>, e1005030 (2016).</div>
</div>
<div id="ref-Schnoerr2017" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">D. Schnoerr, G. Sanguinetti, and R. Grima, <em><span class="nocase">Approximation and inference methods for stochastic biochemical kinetics—a tutorial review</span></em>, Journal of Physics A: Mathematical and Theoretical <strong>50</strong>, 093001 (2017).</div>
</div>
<div id="ref-Smadbeck2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">P. Smadbeck and Y. N. Kaznessis, <em><span class="nocase">A closure scheme for chemical master equations</span></em>, PNAS <strong>110</strong>, 14261 (2013).</div>
</div>
<div id="ref-Jaynes1957" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">E. T. Jaynes, <em><span class="nocase">Information Theory and Statistical Mechanics</span></em>, Physical Review <strong>106</strong>, 620 (1957).</div>
</div>
<div id="ref-Shannon1948" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">C. E. Shannon, <em><span class="nocase">A Mathematical Theory of Communication</span></em>, Bell System Technical Journal <strong>27</strong>, 379 (1948).</div>
</div>
<div id="ref-Tkacik2008a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[34] </div><div class="csl-right-inline">G. Tkačik, C. G. Callan, and W. Bialek, <em><span class="nocase">Information capacity of genetic regulatory elements</span></em>, Physical Review E <strong>78</strong>, 011910 (2008).</div>
</div>
<div id="ref-Blahut1972" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[35] </div><div class="csl-right-inline">R. Blahut, <em><span class="nocase">Computation of channel capacity and rate-distortion functions</span></em>, IEEE Transactions on Information Theory <strong>18</strong>, 460 (1972).</div>
</div>
<div id="ref-Cheong2011a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[36] </div><div class="csl-right-inline">R. Cheong, A. Rhee, C. J. Wang, I. Nemenman, and A. Levchenko, <em><span class="nocase">Information Transduction Capacity of Noisy Biochemical Signaling Networks</span></em>, Science <strong>334</strong>, 354 (2011).</div>
</div>
</div>
                    </div><!-- /.content -->
                </div><!-- /.col -->
                <div class="col-md-4 col-md-offset-1">
                    <div class="sections-list-wrapper">
                        <div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"><span
                                style="font-size: 14pt">Table of Contents</span>
                            <hr>
                        </div><!-- /.sections-list -->
                    </div>
                </div><!-- /.col -->
            </div><!-- /.row -->
        </div><!-- /.container -->
    </div><!-- /.section -->
    
    <div class="section section--grey">
        <div class="container">
            <div class="row">
                <div class="col-md-7">
                    <div id="disqus_thread"></div>
<script>
	/**
	 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
	 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
	 */
	 /*
	var disqus_config = function () {
	this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
		this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
	};
	*/
	(function() {
		var d = document, s = d.createElement('script');

		s.src = 'https://mrazomej-phd.disqus.com/embed.js';

		s.setAttribute('data-timestamp', +new Date());
		(d.head || d.body).appendChild(s);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

                </div><!-- /.col -->
            </div><!-- /.row -->
        </div><!-- /.container -->
    </div><!-- /.section -->
    
    <div class="js-footer-area">
        
        <nav class="page-nav">
            <div class="container">
                <div class="row">
                    <div class="col-xs-12">
                        
                        <a href="https://mrazomej.github.io/phd/chapter_04"
                            class="page-nav__item page-nav__item--prev">
                            <i class="icon icon--arrow-left"></i>
                            Chapter 4
                        </a><!-- /.page-nav__item -->
                        
                        
                        <a href="https://mrazomej.github.io/phd/chapter_06"
                            class="page-nav__item page-nav__item--next">
                            Chapter 6
                            <i class="icon icon--arrow-right"></i>
                        </a><!-- /.page-nav__item -->
                        
                    </div><!-- /.col -->
                </div><!-- /.row -->
            </div><!-- /.container -->
        </nav><!-- /.page-nav -->
        
        
        
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
						<a href="https://mrazomej.github.io/phd//" class="site-footer__logo">Manuel Razo-Mejia | PhD Thesis</a>
					
					
						<hr>
						<p class="site-footer__copyright">Copyright &copy; 2021. - Manuel Razo-Mejia <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
					<div class="col-sm-6 align-right">
						<ul class="social-list">
							
								<li>
									<a href="https://twitter.com/mrazomej" target="_blank" class="social-list__item social-list__item--twitter">
										<i class="icon icon--twitter"></i>
									</a>
								</li>
							
						</ul><!-- /.social-list -->
					</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="https://mrazomej.github.io/phd//doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="https://mrazomej.github.io/phd//doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="https://mrazomej.github.io/phd//doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="https://mrazomej.github.io/phd//doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="https://mrazomej.github.io/phd//doks-theme/assets/js/scripts.min.js"></script>


<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-109526846-4', 'auto');
	ga('send', 'pageview');
</script>



    </div><!-- /.js-footer-area -->
</body>

</html>